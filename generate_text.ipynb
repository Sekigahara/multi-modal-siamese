{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3189608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d7723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992e79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.load_data import load_npz\n",
    "\n",
    "# Load data\n",
    "dataset_main_path = 'dataset/cifar10_randomed/'\n",
    "\n",
    "pair_1_fname_train, pair_2_fname_train, label_pairing_train, label_anchor_train, label_non_anchor_train = load_npz(dataset_main_path+\"train.npz\")\n",
    "pair_1_fname_test, pair_2_fname_test, label_pairing_test, label_anchor_test, label_non_anchor_test = load_npz(dataset_main_path+\"test.npz\")\n",
    "pair_1_fname_val, pair_2_fname_val, label_pairing_val, label_anchor_val, label_non_anchor_val = load_npz(dataset_main_path+\"val.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b0a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# idx = 0\n",
    "# print(pair_1_fname_train[idx])\n",
    "# print(label_pairing_train[idx])\n",
    "# print(label_anchor_train[idx])\n",
    "# print(pair_2_fname_train[idx])\n",
    "# print(label_non_anchor_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdb5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# plt.imshow(cv2.imread(\"dataset/cifar10_randomed/images/\" + pair_1_fname_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6489f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.imread(\"dataset/cifar10_randomed/images/\" + pair_2_fname_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f033c721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-01 09:52:51.867794: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-01 09:52:51.881977: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-01 09:52:51.886178: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-01 09:52:51.897223: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-01 09:52:54,963\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "from module.util_llm import *\n",
    "from module.inference_llm import *\n",
    "\n",
    "def get_url_string(fname_list_1:str, fname_list_2:str, image_base_dir:str='dataset/cifar10_randomed/images'):\n",
    "    url_1_list, url_2_list = [], []\n",
    "    for idx, fname in tqdm.tqdm(enumerate(fname_list_1)):\n",
    "        url_1 = pixtral_preprocessing(\n",
    "            os.path.join(image_base_dir, fname_list_1[idx])\n",
    "        )\n",
    "        \n",
    "        url_2 = pixtral_preprocessing(\n",
    "            os.path.join(image_base_dir, fname_list_2[idx])\n",
    "        )\n",
    "        \n",
    "        url_1_list.append(url_1)\n",
    "        url_2_list.append(url_2)\n",
    "        \n",
    "    return url_1_list, url_2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09abb3ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90000it [00:03, 24513.61it/s]\n",
      "10000it [00:00, 26482.28it/s]\n",
      "20000it [00:00, 26227.61it/s]\n"
     ]
    }
   ],
   "source": [
    "pair_1_url_train, pair_2_url_train = get_url_string(\n",
    "    fname_list_1=pair_1_fname_train,\n",
    "    fname_list_2=pair_2_fname_train\n",
    ")\n",
    "\n",
    "pair_1_url_val, pair_2_url_val = get_url_string(\n",
    "    fname_list_1=pair_1_fname_val,\n",
    "    fname_list_2=pair_2_fname_val\n",
    ")\n",
    "\n",
    "pair_1_url_test, pair_2_url_test = get_url_string(\n",
    "    fname_list_1=pair_1_fname_test,\n",
    "    fname_list_2=pair_2_fname_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92d7be94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-01 09:53:01 config.py:1664] Downcasting torch.float32 to torch.float16.\n",
      "INFO 11-01 09:53:10 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='mistralai/Pixtral-12B-2409', speculative_config=None, tokenizer='mistralai/Pixtral-12B-2409', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Pixtral-12B-2409, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 11-01 09:53:13 model_runner.py:1056] Starting to load model mistralai/Pixtral-12B-2409...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab-ai/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/islab-ai/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-01 09:53:14 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-01 09:53:14 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.75s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.75s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-01 09:53:20 model_runner.py:1067] Loading model weights took 23.6552 GB\n",
      "INFO 11-01 09:53:26 gpu_executor.py:122] # GPU blocks: 17690, # CPU blocks: 1638\n",
      "INFO 11-01 09:53:26 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 8.64x\n",
      "INFO 11-01 09:53:28 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-01 09:53:28 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-01 09:53:48 model_runner.py:1523] Graph capturing finished in 21 secs.\n"
     ]
    }
   ],
   "source": [
    "model, sampling_params = load_model(model_name='mistralai/Pixtral-12B-2409')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd612ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'This image is sample of cifar dataset, Describe this two images what object is inside and keep the explanation as concise as possible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c243bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-01 09:53:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-01 09:53:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s, est. speed input: 90.74 toks/s, output: 47.64 toks/s]\u001b[A\n",
      "1it [00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-01 09:53:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-01 09:53:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 38.95 toks/s, output: 53.55 toks/s]\u001b[A\n",
      "2it [00:01,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-01 09:53:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-01 09:53:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s, est. speed input: 103.15 toks/s, output: 48.99 toks/s]\u001b[A\n",
      "2it [00:01,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "pair_text = pixtral_inference(\n",
    "    llm=model,\n",
    "    sampling_params=sampling_params,\n",
    "    image_url_1=pair_1_url_train,\n",
    "    image_url_2=pair_2_url_train,\n",
    "    prompt=prompt,\n",
    "    test=pair_1_fname_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b9c5a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. The first image contains a **cat**.\\n2. The second image contains a **dog**.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_text[0][0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df9a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
