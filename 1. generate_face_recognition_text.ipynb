{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5bbc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd55f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e99fd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.load_data import load_npz\n",
    "\n",
    "# Load data\n",
    "dataset_main_path = 'formatted_dataset/face_recognition/'\n",
    "\n",
    "pair_1_fname_train, pair_2_fname_train, label_pairing_train, label_anchor_train, label_non_anchor_train = load_npz(dataset_main_path+\"train.npz\")\n",
    "pair_1_fname_test, pair_2_fname_test, label_pairing_test, label_anchor_test, label_non_anchor_test = load_npz(dataset_main_path+\"test.npz\")\n",
    "#pair_1_fname_val, pair_2_fname_val, label_pairing_val, label_anchor_val, label_non_anchor_val = load_npz(dataset_main_path+\"val.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0108da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# idx = 0\n",
    "# print(pair_1_fname_train[idx])\n",
    "# print(label_pairing_train[idx])\n",
    "# print(label_anchor_train[idx])\n",
    "# print(pair_2_fname_train[idx])\n",
    "# print(label_non_anchor_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c22b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# plt.imshow(cv2.imread(\"dataset/cifar10_randomed/images/\" + pair_1_fname_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a146a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.imread(\"dataset/cifar10_randomed/images/\" + pair_2_fname_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ad9600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-05 05:44:42.904645: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-05 05:44:42.918121: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-05 05:44:42.922356: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-05 05:44:42.932810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-05 05:44:45,853\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "from module.util_llm import *\n",
    "from module.inference_llm import *\n",
    "\n",
    "def get_url_string(fname_list_1:str, fname_list_2:str, image_base_dir:str='dataset/cifar10_randomed/images'):\n",
    "    url_1_list, url_2_list = [], []\n",
    "    for idx, fname in tqdm.tqdm(enumerate(fname_list_1)):\n",
    "        url_1 = pixtral_preprocessing(\n",
    "            os.path.join(image_base_dir, fname_list_1[idx])\n",
    "        )\n",
    "        \n",
    "        url_2 = pixtral_preprocessing(\n",
    "            os.path.join(image_base_dir, fname_list_2[idx])\n",
    "        )\n",
    "        \n",
    "        url_1_list.append(url_1)\n",
    "        url_2_list.append(url_2)\n",
    "        \n",
    "    return url_1_list, url_2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d5e516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22032it [00:00, 26732.65it/s]\n",
      "8204it [00:00, 26993.52it/s]\n"
     ]
    }
   ],
   "source": [
    "pair_1_url_train, pair_2_url_train = get_url_string(\n",
    "    fname_list_1=pair_1_fname_train,\n",
    "    fname_list_2=pair_2_fname_train\n",
    ")\n",
    "\n",
    "pair_1_url_test, pair_2_url_test = get_url_string(\n",
    "    fname_list_1=pair_1_fname_test,\n",
    "    fname_list_2=pair_2_fname_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1b2b16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-05 05:44:48 config.py:1664] Downcasting torch.float32 to torch.float16.\n",
      "INFO 11-05 05:44:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='mistralai/Pixtral-12B-2409', speculative_config=None, tokenizer='mistralai/Pixtral-12B-2409', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Pixtral-12B-2409, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 11-05 05:44:59 model_runner.py:1056] Starting to load model mistralai/Pixtral-12B-2409...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab-ai/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/islab-ai/.local/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-05 05:45:00 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-05 05:45:00 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.22s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:05<00:00,  5.22s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-05 05:45:07 model_runner.py:1067] Loading model weights took 23.6552 GB\n",
      "INFO 11-05 05:45:13 gpu_executor.py:122] # GPU blocks: 17690, # CPU blocks: 1638\n",
      "INFO 11-05 05:45:13 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 8.64x\n",
      "INFO 11-05 05:45:15 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-05 05:45:15 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-05 05:45:36 model_runner.py:1523] Graph capturing finished in 21 secs.\n"
     ]
    }
   ],
   "source": [
    "model, sampling_params = load_model(model_name='mistralai/Pixtral-12B-2409')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ad19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Describe the person you saw from the given images especially from hair, face, and nose. Tell only the differences or similarity without any chat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d12bdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:45:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:45:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:45:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:45:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:06,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:45:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:45:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:08,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:45:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:45:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:10,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:45:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:45:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:11,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:45:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:45:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:14,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:45:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:45:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:17,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:45:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:45:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:19,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:45:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:45:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:22,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:45:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:45:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:23,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:27,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:30,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:32,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:34,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:36,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:39,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:41,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:43,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:47,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:50,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:52,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:56,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:58,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [01:01,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [01:03,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [01:05,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [01:07,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [01:09,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [01:12,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [01:14,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [01:16,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [01:17,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [01:20,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:46:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:46:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [01:24,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [01:26,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [01:28,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [01:31,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [01:34,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [01:36,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [01:38,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [01:41,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [01:45,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [01:47,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [01:50,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [01:54,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [01:56,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [01:58,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [02:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [02:03,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [02:06,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [02:07,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [02:11,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [02:13,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [02:18,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [02:22,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:47:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:47:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [02:26,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [02:28,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [02:29,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [02:32,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [02:35,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [02:36,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [02:38,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [02:40,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [02:42,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [02:44,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [02:46,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [02:48,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [02:50,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [02:52,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [02:53,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [02:55,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [02:58,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [02:59,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [03:03,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [03:05,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [03:07,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [03:10,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [03:11,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [03:14,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [03:17,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [03:19,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [03:20,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [03:22,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:48:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:48:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [03:24,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [03:25,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [03:28,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [03:32,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [03:33,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [03:35,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [03:38,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [03:42,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [03:45,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93it [03:49,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [03:50,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [03:52,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [03:55,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97it [03:59,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [04:01,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [04:04,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [04:06,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [04:08,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [04:09,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [04:14,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [04:16,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [04:17,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106it [04:19,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "107it [04:21,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [04:23,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:49:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:49:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109it [04:24,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [04:26,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [04:30,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [04:33,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [04:36,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114it [04:38,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [04:40,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [04:42,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117it [04:43,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [04:46,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [04:48,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [04:51,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [04:53,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "122it [04:55,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [04:58,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124it [05:01,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [05:04,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126it [05:06,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [05:09,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [05:13,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [05:17,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [05:20,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:50:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:50:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131it [05:23,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132it [05:25,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "133it [05:27,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [05:29,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "135it [05:31,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136it [05:34,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "137it [05:37,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138it [05:40,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "139it [05:42,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [05:44,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141it [05:47,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142it [05:48,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "143it [05:50,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [05:54,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [05:56,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146it [05:58,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "147it [05:59,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [06:03,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "149it [06:04,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [06:07,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "151it [06:11,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [06:14,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "153it [06:16,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "154it [06:18,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:51:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:51:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155it [06:23,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [06:25,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [06:29,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158it [06:33,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159it [06:37,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [06:40,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [06:42,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "162it [06:44,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "163it [06:46,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [06:48,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [06:49,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [06:52,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [06:54,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168it [06:55,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169it [06:58,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170it [07:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "171it [07:03,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172it [07:07,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [07:10,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174it [07:11,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "175it [07:14,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [07:16,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "177it [07:20,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178it [07:23,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:52:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:52:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "179it [07:24,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180it [07:27,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "181it [07:29,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182it [07:31,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "183it [07:34,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "184it [07:36,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "185it [07:38,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [07:40,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "187it [07:43,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188it [07:44,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "189it [07:47,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "190it [07:49,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [07:52,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192it [07:53,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "193it [07:57,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "194it [08:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "195it [08:03,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [08:05,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "197it [08:09,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198it [08:13,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [08:15,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [08:18,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [08:21,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202it [08:22,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:53:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:53:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "203it [08:27,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "204it [08:28,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "205it [08:32,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [08:35,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "207it [08:39,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "208it [08:42,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209it [08:46,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210it [08:47,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [08:49,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "212it [08:51,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "213it [08:54,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "214it [08:56,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "215it [08:58,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "216it [09:00,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "217it [09:01,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218it [09:03,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "219it [09:06,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "220it [09:07,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [09:10,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "222it [09:13,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [09:14,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "224it [09:18,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [09:19,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "226it [09:22,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:54:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:54:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [09:24,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "228it [09:27,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "229it [09:30,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "230it [09:33,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231it [09:36,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "232it [09:38,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "233it [09:41,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [09:44,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "235it [09:47,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "236it [09:49,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "237it [09:52,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "238it [09:54,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "239it [09:56,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [10:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [10:01,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "242it [10:04,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "243it [10:07,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [10:10,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [10:13,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "246it [10:15,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "247it [10:18,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "248it [10:20,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:55:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:55:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "249it [10:25,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [10:28,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [10:30,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "252it [10:32,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "253it [10:34,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "254it [10:37,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "255it [10:40,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [10:42,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [10:45,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "258it [10:47,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "259it [10:51,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "260it [10:53,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "261it [10:55,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "262it [10:57,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "263it [10:59,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "264it [11:01,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "265it [11:03,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "266it [11:07,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [11:09,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "268it [11:12,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "269it [11:16,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "270it [11:22,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:56:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:56:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [11:24,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "272it [11:28,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [11:30,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [11:32,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "275it [11:35,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "276it [11:36,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "277it [11:39,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "278it [11:43,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "279it [11:45,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [11:48,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "281it [11:50,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "282it [11:54,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "283it [11:55,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [11:58,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "285it [12:01,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "286it [12:04,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [12:06,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "288it [12:09,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "289it [12:14,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "290it [12:17,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "291it [12:19,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "292it [12:21,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "293it [12:23,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:57:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:57:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "294it [12:24,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "295it [12:27,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "296it [12:29,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "297it [12:30,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "298it [12:33,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "299it [12:36,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [12:39,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [12:43,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "302it [12:45,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "303it [12:48,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "304it [12:51,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "305it [12:54,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "306it [12:56,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "307it [12:58,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "308it [13:01,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "309it [13:02,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "310it [13:06,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "311it [13:08,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "312it [13:10,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [13:12,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "314it [13:13,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "315it [13:15,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "316it [13:17,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317it [13:19,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [13:20,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "319it [13:22,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:58:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:58:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "320it [13:24,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "321it [13:26,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "322it [13:28,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "323it [13:31,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "324it [13:34,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "325it [13:37,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "326it [13:39,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "327it [13:43,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "328it [13:44,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "329it [13:47,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "330it [13:49,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "331it [13:52,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "332it [13:54,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "333it [13:56,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "334it [13:58,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "335it [14:01,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "336it [14:03,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "337it [14:05,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [14:06,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "339it [14:10,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "340it [14:13,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "341it [14:15,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "342it [14:18,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343it [14:21,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "344it [14:23,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 05:59:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 05:59:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "345it [14:25,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "346it [14:26,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "347it [14:28,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "348it [14:33,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "349it [14:36,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "350it [14:39,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "351it [14:42,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "352it [14:44,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "353it [14:48,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "354it [14:50,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "355it [14:53,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "356it [14:55,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "357it [14:57,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "358it [14:59,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [15:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360it [15:04,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "361it [15:06,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "362it [15:08,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "363it [15:10,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "364it [15:12,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "365it [15:15,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "366it [15:17,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "367it [15:20,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "368it [15:22,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:00:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:00:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "369it [15:24,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "370it [15:26,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "371it [15:27,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "372it [15:29,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "373it [15:31,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "374it [15:35,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "375it [15:37,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "376it [15:39,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "377it [15:40,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "378it [15:42,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "379it [15:44,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "380it [15:47,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "381it [15:50,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "382it [15:52,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383it [15:55,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "384it [15:58,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "385it [16:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "386it [16:01,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "387it [16:03,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "388it [16:06,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "389it [16:08,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "390it [16:10,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391it [16:13,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "392it [16:15,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "393it [16:18,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "394it [16:20,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [16:22,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:01:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:01:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "396it [16:23,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "397it [16:25,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398it [16:29,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "399it [16:31,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [16:33,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [16:35,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "402it [16:37,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "403it [16:40,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404it [16:44,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "405it [16:45,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "406it [16:47,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "407it [16:51,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [16:52,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "409it [16:54,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "410it [16:57,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "411it [16:59,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "412it [17:02,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "413it [17:04,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "414it [17:07,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "415it [17:09,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "416it [17:10,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "417it [17:12,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "418it [17:14,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "419it [17:16,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "420it [17:18,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "421it [17:21,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:02:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:02:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "422it [17:24,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "423it [17:27,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "424it [17:29,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "425it [17:32,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "426it [17:34,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "427it [17:39,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [17:40,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "429it [17:42,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "430it [17:44,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "431it [17:48,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "432it [17:51,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "433it [17:55,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "434it [17:57,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "435it [17:58,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "436it [18:03,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "437it [18:05,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "438it [18:06,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [18:08,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "440it [18:11,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "441it [18:15,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "442it [18:19,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "443it [18:20,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:03:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:03:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "444it [18:23,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "445it [18:25,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "446it [18:27,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "447it [18:31,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "448it [18:36,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "449it [18:38,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450it [18:39,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "451it [18:42,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "452it [18:44,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "453it [18:46,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "454it [18:47,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "455it [18:49,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "456it [18:51,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "457it [18:54,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "458it [18:59,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "459it [19:01,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "460it [19:04,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "461it [19:08,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "462it [19:11,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "463it [19:14,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "464it [19:15,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "465it [19:17,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "466it [19:21,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:04:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:04:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "467it [19:24,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "468it [19:26,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [19:29,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "470it [19:31,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "471it [19:33,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "472it [19:34,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "473it [19:36,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "474it [19:40,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "475it [19:42,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "476it [19:43,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "477it [19:45,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "478it [19:49,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "479it [19:53,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480it [19:54,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "481it [19:57,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "482it [19:59,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "483it [20:01,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "484it [20:04,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "485it [20:08,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "486it [20:12,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "487it [20:14,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "488it [20:16,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "489it [20:20,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "490it [20:22,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:05:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:05:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "491it [20:26,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "492it [20:28,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "493it [20:30,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "494it [20:32,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "495it [20:35,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "496it [20:37,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "497it [20:41,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "498it [20:42,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "499it [20:46,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [20:48,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [20:51,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "502it [20:53,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "503it [20:57,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "504it [21:00,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "505it [21:02,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "506it [21:05,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "507it [21:07,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "508it [21:09,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "509it [21:11,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "510it [21:13,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "511it [21:15,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "512it [21:16,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "513it [21:19,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "514it [21:20,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515it [21:22,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:06:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:06:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "516it [21:24,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "517it [21:26,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "518it [21:29,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "519it [21:32,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [21:34,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "521it [21:37,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "522it [21:40,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "523it [21:42,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "524it [21:44,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "525it [21:48,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "526it [21:50,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "527it [21:52,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "528it [21:53,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "529it [21:56,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "530it [22:00,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "531it [22:02,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "532it [22:04,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "533it [22:06,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "534it [22:09,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "535it [22:12,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "536it [22:14,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "537it [22:15,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "538it [22:17,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "539it [22:20,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:07:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:07:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "540it [22:23,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [22:26,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "542it [22:28,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [22:30,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "544it [22:32,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "545it [22:35,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [22:37,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [22:40,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "548it [22:43,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "549it [22:45,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550it [22:49,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [22:51,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "552it [22:52,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "553it [22:56,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "554it [22:58,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "555it [23:01,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "556it [23:03,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "557it [23:05,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "558it [23:09,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "559it [23:13,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "560it [23:15,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "561it [23:17,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "562it [23:20,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "563it [23:21,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:08:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:08:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "564it [23:23,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "565it [23:27,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "566it [23:29,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "567it [23:33,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "568it [23:35,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "569it [23:38,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "570it [23:42,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "571it [23:43,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "572it [23:47,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "573it [23:50,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "574it [23:52,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "575it [23:56,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "576it [23:59,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "577it [24:01,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "578it [24:02,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "579it [24:06,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "580it [24:10,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "581it [24:11,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "582it [24:14,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "583it [24:16,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "584it [24:18,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "585it [24:20,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "586it [24:22,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:09:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:09:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "587it [24:24,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "588it [24:26,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "589it [24:28,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "590it [24:29,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "591it [24:32,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "592it [24:35,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "593it [24:37,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "594it [24:40,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "595it [24:43,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "596it [24:45,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "597it [24:47,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "598it [24:49,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "599it [24:53,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [24:55,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [24:56,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "602it [24:59,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "603it [25:02,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "604it [25:04,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "605it [25:07,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "606it [25:08,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "607it [25:12,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "608it [25:16,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "609it [25:18,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "610it [25:20,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "611it [25:22,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:10:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:10:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "612it [25:24,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "613it [25:27,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "614it [25:29,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "615it [25:33,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "616it [25:34,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "617it [25:39,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "618it [25:40,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "619it [25:43,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "620it [25:47,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "621it [25:48,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "622it [25:50,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "623it [25:53,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "624it [25:54,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [25:56,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "626it [26:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "627it [26:02,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "628it [26:04,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "629it [26:07,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "630it [26:09,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "631it [26:11,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "632it [26:14,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "633it [26:17,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "634it [26:19,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "635it [26:22,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:11:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:11:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "636it [26:24,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "637it [26:26,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "638it [26:28,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "639it [26:32,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "640it [26:35,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "641it [26:36,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "642it [26:39,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "643it [26:42,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "644it [26:46,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "645it [26:48,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "646it [26:51,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "647it [26:56,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "648it [26:59,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "649it [27:03,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "650it [27:04,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "651it [27:06,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "652it [27:08,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "653it [27:10,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "654it [27:13,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "655it [27:14,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "656it [27:19,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "657it [27:21,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:12:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:12:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "658it [27:25,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "659it [27:26,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "660it [27:29,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "661it [27:33,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "662it [27:37,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "663it [27:40,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "664it [27:44,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "665it [27:46,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "666it [27:49,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "667it [27:51,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "668it [27:54,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "669it [27:58,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "670it [28:01,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "671it [28:04,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "672it [28:06,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "673it [28:08,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "674it [28:11,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "675it [28:14,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "676it [28:16,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "677it [28:19,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "678it [28:21,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "679it [28:23,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:13:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:13:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "680it [28:26,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "681it [28:28,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "682it [28:30,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "683it [28:34,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "684it [28:35,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "685it [28:38,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "686it [28:40,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "687it [28:42,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "688it [28:44,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "689it [28:46,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "690it [28:49,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "691it [28:52,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "692it [28:56,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "693it [28:58,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "694it [29:01,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "695it [29:04,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "696it [29:07,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "697it [29:09,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "698it [29:11,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "699it [29:13,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "700it [29:16,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "701it [29:19,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "702it [29:22,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:14:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:14:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "703it [29:23,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "704it [29:25,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "705it [29:27,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "706it [29:29,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "707it [29:32,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "708it [29:34,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "709it [29:36,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "710it [29:38,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "711it [29:40,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "712it [29:42,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "713it [29:45,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "714it [29:47,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "715it [29:49,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "716it [29:52,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "717it [29:54,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "718it [29:56,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "719it [29:58,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "720it [30:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "721it [30:02,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "722it [30:04,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "723it [30:07,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "724it [30:11,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "725it [30:13,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "726it [30:15,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "727it [30:18,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "728it [30:22,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:15:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:15:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "729it [30:25,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "730it [30:28,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "731it [30:31,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "732it [30:34,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "733it [30:35,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "734it [30:37,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "735it [30:41,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "736it [30:44,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "737it [30:46,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "738it [30:50,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "739it [30:52,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "740it [30:54,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "741it [30:57,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "742it [31:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "743it [31:01,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "744it [31:05,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "745it [31:07,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "746it [31:09,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "747it [31:13,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "748it [31:18,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "749it [31:22,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:16:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:16:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "750it [31:26,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "751it [31:28,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "752it [31:30,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "753it [31:31,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "754it [31:33,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "755it [31:36,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "756it [31:38,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "757it [31:41,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "758it [31:44,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "759it [31:46,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "760it [31:48,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "761it [31:52,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "762it [31:57,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "763it [32:00,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "764it [32:02,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "765it [32:03,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "766it [32:05,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "767it [32:07,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "768it [32:09,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "769it [32:12,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "770it [32:14,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "771it [32:15,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "772it [32:18,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "773it [32:20,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "774it [32:22,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:17:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:17:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "775it [32:24,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "776it [32:28,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "777it [32:29,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "778it [32:32,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "779it [32:34,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780it [32:36,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "781it [32:40,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "782it [32:42,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "783it [32:45,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "784it [32:47,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "785it [32:50,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "786it [32:52,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "787it [32:55,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "788it [32:56,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "789it [32:58,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "790it [33:01,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "791it [33:04,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "792it [33:07,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "793it [33:09,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "794it [33:11,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "795it [33:13,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "796it [33:15,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "797it [33:17,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "798it [33:19,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "799it [33:21,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:18:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:18:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [33:25,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [33:28,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "802it [33:33,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "803it [33:35,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "804it [33:37,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "805it [33:39,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "806it [33:41,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807it [33:44,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "808it [33:45,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "809it [33:48,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "810it [33:50,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "811it [33:52,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "812it [33:54,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "813it [33:57,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "814it [34:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "815it [34:03,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "816it [34:08,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "817it [34:10,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "818it [34:13,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "819it [34:15,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "820it [34:17,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "821it [34:19,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:19:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:19:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "822it [34:23,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "823it [34:26,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "824it [34:28,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "825it [34:30,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "826it [34:31,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "827it [34:33,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "828it [34:35,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "829it [34:38,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "830it [34:41,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "831it [34:43,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "832it [34:45,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "833it [34:47,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "834it [34:52,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "835it [34:55,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "836it [34:57,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "837it [34:59,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "838it [35:01,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "839it [35:03,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "840it [35:05,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "841it [35:07,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "842it [35:09,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "843it [35:11,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "844it [35:13,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "845it [35:17,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "846it [35:20,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "847it [35:21,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:20:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:20:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "848it [35:24,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "849it [35:27,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "850it [35:31,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "851it [35:32,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "852it [35:35,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "853it [35:36,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "854it [35:40,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "855it [35:41,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "856it [35:43,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "857it [35:46,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "858it [35:47,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "859it [35:51,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "860it [35:53,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "861it [35:56,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "862it [35:57,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "863it [35:59,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "864it [36:01,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "865it [36:03,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "866it [36:06,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "867it [36:08,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "868it [36:10,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "869it [36:13,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "870it [36:15,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "871it [36:18,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "872it [36:21,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:21:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:21:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "873it [36:23,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "874it [36:25,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [36:26,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "876it [36:30,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "877it [36:32,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "878it [36:33,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "879it [36:37,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "880it [36:39,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "881it [36:42,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "882it [36:45,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "883it [36:47,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "884it [36:50,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "885it [36:52,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "886it [36:55,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "887it [36:57,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "888it [36:59,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "889it [37:02,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "890it [37:03,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "891it [37:05,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "892it [37:08,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "893it [37:10,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "894it [37:12,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "895it [37:14,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "896it [37:17,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "897it [37:20,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "898it [37:22,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:22:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:22:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "899it [37:25,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "900it [37:28,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "901it [37:32,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "902it [37:34,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "903it [37:38,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "904it [37:40,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "905it [37:43,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "906it [37:46,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "907it [37:48,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "908it [37:50,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "909it [37:52,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "910it [37:54,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "911it [37:57,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "912it [37:58,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "913it [38:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "914it [38:02,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "915it [38:04,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "916it [38:06,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "917it [38:07,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "918it [38:09,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "919it [38:11,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "920it [38:12,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "921it [38:15,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "922it [38:16,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "923it [38:19,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "924it [38:22,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:23:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:23:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "925it [38:26,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "926it [38:28,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "927it [38:30,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "928it [38:32,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "929it [38:34,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "930it [38:37,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "931it [38:39,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "932it [38:41,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "933it [38:42,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "934it [38:46,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "935it [38:50,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "936it [38:52,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "937it [38:54,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "938it [38:56,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "939it [38:59,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "940it [39:01,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "941it [39:05,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "942it [39:08,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "943it [39:10,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "944it [39:14,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "945it [39:16,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "946it [39:17,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "947it [39:20,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "948it [39:23,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:24:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:24:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "949it [39:25,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "950it [39:28,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "951it [39:30,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "952it [39:32,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "953it [39:34,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "954it [39:36,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "955it [39:38,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "956it [39:41,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "957it [39:42,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "958it [39:44,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "959it [39:46,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "960it [39:48,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "961it [39:50,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "962it [39:51,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "963it [39:53,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "964it [39:55,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "965it [39:57,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "966it [39:59,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "967it [40:03,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "968it [40:05,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "969it [40:08,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "970it [40:11,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "971it [40:14,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "972it [40:15,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "973it [40:17,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "974it [40:20,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "975it [40:22,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:25:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:25:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "976it [40:25,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "977it [40:27,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "978it [40:29,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "979it [40:31,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "980it [40:34,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "981it [40:37,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "982it [40:41,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "983it [40:43,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "984it [40:44,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "985it [40:46,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "986it [40:47,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "987it [40:49,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "988it [40:52,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "989it [40:54,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "990it [40:57,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "991it [40:59,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "992it [41:03,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "993it [41:05,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "994it [41:09,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "995it [41:13,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "996it [41:15,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "997it [41:17,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "998it [41:19,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999it [41:22,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:26:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:26:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [41:23,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [41:26,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1002it [41:28,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1003it [41:30,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1004it [41:31,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1005it [41:34,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1006it [41:37,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1007it [41:39,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1008it [41:40,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1009it [41:42,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1010it [41:45,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1011it [41:48,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1012it [41:50,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1013it [41:54,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1014it [41:58,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1015it [42:00,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1016it [42:02,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1017it [42:04,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1018it [42:06,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1019it [42:08,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1020it [42:11,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1021it [42:13,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1022it [42:15,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1023it [42:18,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1024it [42:20,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1025it [42:22,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:27:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:27:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1026it [42:27,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1027it [42:29,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1028it [42:31,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1029it [42:34,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1030it [42:37,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1031it [42:40,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1032it [42:41,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1033it [42:43,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1034it [42:45,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1035it [42:47,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1036it [42:51,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1037it [42:52,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1038it [42:54,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1039it [42:56,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1040it [42:57,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1041it [43:02,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1042it [43:04,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1043it [43:08,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1044it [43:09,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1045it [43:11,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1046it [43:13,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1047it [43:15,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1048it [43:17,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1049it [43:19,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1050it [43:21,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:28:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:28:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1051it [43:23,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1052it [43:25,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1053it [43:27,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1054it [43:29,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1055it [43:30,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1056it [43:32,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1057it [43:35,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1058it [43:37,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1059it [43:40,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1060it [43:42,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1061it [43:43,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1062it [43:45,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1063it [43:48,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1064it [43:49,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1065it [43:53,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1066it [43:55,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1067it [43:58,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1068it [44:01,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1069it [44:03,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1070it [44:05,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1071it [44:07,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1072it [44:10,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1073it [44:11,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1074it [44:14,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1075it [44:16,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1076it [44:17,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1077it [44:20,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1078it [44:23,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:29:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:29:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1079it [44:25,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1080it [44:27,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [44:30,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1082it [44:33,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1083it [44:35,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1084it [44:38,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1085it [44:42,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [44:44,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1087it [44:46,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1088it [44:50,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1089it [44:52,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1090it [44:55,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1091it [44:56,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1092it [44:58,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1093it [45:00,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1094it [45:01,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1095it [45:02,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1096it [45:04,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1097it [45:08,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1098it [45:09,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1099it [45:12,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1100it [45:16,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1101it [45:18,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1102it [45:21,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:30:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:30:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1103it [45:24,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1104it [45:27,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1105it [45:29,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1106it [45:30,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1107it [45:32,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1108it [45:35,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1109it [45:37,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1110it [45:39,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1111it [45:40,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1112it [45:42,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1113it [45:44,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1114it [45:46,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1115it [45:48,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1116it [45:51,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1117it [45:55,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1118it [45:58,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1119it [46:01,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1120it [46:03,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1121it [46:04,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1122it [46:06,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1123it [46:09,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1124it [46:11,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1125it [46:13,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1126it [46:15,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1127it [46:17,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1128it [46:18,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1129it [46:21,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1130it [46:23,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:31:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:31:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1131it [46:25,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1132it [46:27,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1133it [46:31,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1134it [46:32,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1135it [46:36,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1136it [46:38,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1137it [46:41,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1138it [46:43,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1139it [46:44,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1140it [46:46,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1141it [46:49,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1142it [46:54,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1143it [46:57,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1144it [46:59,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1145it [47:01,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1146it [47:03,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147it [47:06,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1148it [47:08,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1149it [47:11,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1150it [47:14,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1151it [47:17,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1152it [47:19,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1153it [47:21,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:32:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:32:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1154it [47:24,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1155it [47:26,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1156it [47:30,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1157it [47:31,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1158it [47:33,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1159it [47:36,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1160it [47:38,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1161it [47:40,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1162it [47:41,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1163it [47:43,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1164it [47:46,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1165it [47:50,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1166it [47:53,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1167it [47:55,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1168it [47:57,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1169it [47:59,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1170it [48:04,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1171it [48:08,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1172it [48:12,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1173it [48:16,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1174it [48:19,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1175it [48:22,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:33:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:33:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1176it [48:25,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1177it [48:28,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1178it [48:30,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1179it [48:32,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1180it [48:35,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1181it [48:36,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1182it [48:38,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1183it [48:40,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1184it [48:42,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1185it [48:45,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1186it [48:48,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1187it [48:52,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1188it [48:53,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1189it [48:55,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1190it [48:57,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1191it [49:00,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1192it [49:01,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193it [49:03,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1194it [49:06,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1195it [49:08,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1196it [49:11,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1197it [49:13,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1198it [49:16,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1199it [49:18,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [49:19,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1201it [49:21,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1202it [49:23,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:34:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:34:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1203it [49:24,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1204it [49:26,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1205it [49:29,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1206it [49:31,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1207it [49:33,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1208it [49:34,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1209it [49:36,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1210it [49:39,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1211it [49:42,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1212it [49:44,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1213it [49:45,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1214it [49:48,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1215it [49:50,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1216it [49:52,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1217it [49:53,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1218it [49:55,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1219it [49:57,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1220it [49:58,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1221it [50:01,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1222it [50:04,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1223it [50:06,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1224it [50:08,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1225it [50:11,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1226it [50:13,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1227it [50:14,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1228it [50:16,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1229it [50:20,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1230it [50:22,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:35:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:35:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1231it [50:25,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1232it [50:27,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1233it [50:30,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1234it [50:33,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1235it [50:35,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1236it [50:37,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1237it [50:38,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1238it [50:40,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1239it [50:43,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1240it [50:46,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1241it [50:49,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1242it [50:52,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1243it [50:53,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1244it [50:57,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1245it [50:59,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1246it [51:02,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1247it [51:06,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1248it [51:08,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1249it [51:10,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1250it [51:13,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [51:16,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1252it [51:18,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1253it [51:22,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:36:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:36:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1254it [51:24,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1255it [51:27,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1256it [51:30,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1257it [51:33,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1258it [51:36,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1259it [51:39,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1260it [51:41,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1261it [51:43,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1262it [51:45,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1263it [51:46,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1264it [51:48,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1265it [51:49,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1266it [51:51,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1267it [51:53,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1268it [51:55,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1269it [51:57,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1270it [51:59,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1271it [52:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1272it [52:03,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1273it [52:06,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1274it [52:07,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1275it [52:09,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1276it [52:11,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1277it [52:13,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1278it [52:15,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1279it [52:17,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1280it [52:18,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1281it [52:21,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1282it [52:23,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:37:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:37:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1283it [52:24,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1284it [52:27,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1285it [52:31,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1286it [52:35,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1287it [52:39,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1288it [52:41,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1289it [52:44,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1290it [52:48,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1291it [52:51,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1292it [52:52,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1293it [52:55,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1294it [52:57,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1295it [52:59,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1296it [53:01,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1297it [53:03,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1298it [53:05,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1299it [53:07,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1300it [53:10,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1301it [53:12,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1302it [53:13,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1303it [53:16,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1304it [53:18,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1305it [53:20,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1306it [53:22,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:38:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:38:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1307it [53:24,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1308it [53:27,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1309it [53:30,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1310it [53:32,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1311it [53:33,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1312it [53:36,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1313it [53:39,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1314it [53:42,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1315it [53:44,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1316it [53:47,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1317it [53:50,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1318it [53:51,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1319it [53:53,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1320it [53:55,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1321it [53:56,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1322it [53:58,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1323it [54:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1324it [54:02,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1325it [54:04,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1326it [54:06,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1327it [54:08,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1328it [54:10,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1329it [54:12,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1330it [54:13,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1331it [54:16,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1332it [54:19,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1333it [54:21,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:39:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:39:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1334it [54:24,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1335it [54:27,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1336it [54:29,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1337it [54:30,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1338it [54:32,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1339it [54:34,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1340it [54:36,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1341it [54:38,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1342it [54:41,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1343it [54:44,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1344it [54:46,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1345it [54:49,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1346it [54:53,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1347it [54:55,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1348it [54:57,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1349it [54:59,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1350it [55:03,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1351it [55:06,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1352it [55:08,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1353it [55:10,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1354it [55:11,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1355it [55:13,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1356it [55:14,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1357it [55:18,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1358it [55:20,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:40:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:40:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1359it [55:23,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1360it [55:25,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1361it [55:28,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1362it [55:31,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1363it [55:34,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1364it [55:37,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1365it [55:40,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1366it [55:43,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1367it [55:44,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1368it [55:47,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1369it [55:49,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1370it [55:51,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1371it [55:54,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1372it [55:56,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1373it [55:58,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1374it [56:02,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1375it [56:04,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1376it [56:07,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1377it [56:11,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1378it [56:14,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1379it [56:16,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1380it [56:18,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1381it [56:20,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:41:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:41:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1382it [56:23,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1383it [56:26,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1384it [56:29,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1385it [56:30,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [56:31,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1387it [56:34,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1388it [56:38,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1389it [56:41,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [56:44,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1391it [56:46,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1392it [56:48,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1393it [56:50,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1394it [56:52,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1395it [56:54,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1396it [56:56,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1397it [56:58,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1398it [57:02,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1399it [57:05,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1400it [57:09,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1401it [57:11,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1402it [57:12,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1403it [57:15,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1404it [57:18,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1405it [57:20,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1406it [57:22,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:42:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:42:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1407it [57:24,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1408it [57:25,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1409it [57:28,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1410it [57:30,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1411it [57:33,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1412it [57:36,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1413it [57:37,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1414it [57:39,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1415it [57:42,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1416it [57:44,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1417it [57:48,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1418it [57:52,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1419it [57:54,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1420it [57:55,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1421it [57:58,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1422it [58:01,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1423it [58:02,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1424it [58:05,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1425it [58:08,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1426it [58:10,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1427it [58:12,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1428it [58:14,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1429it [58:17,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1430it [58:20,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1431it [58:23,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:43:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:43:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1432it [58:25,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1433it [58:28,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1434it [58:31,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1435it [58:34,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1436it [58:35,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1437it [58:37,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1438it [58:40,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1439it [58:43,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1440it [58:44,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1441it [58:46,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1442it [58:48,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1443it [58:51,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1444it [58:53,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1445it [58:57,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1446it [59:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1447it [59:03,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1448it [59:06,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1449it [59:08,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1450it [59:12,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1451it [59:15,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1452it [59:17,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1453it [59:19,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1454it [59:21,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1455it [59:23,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:44:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:44:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1456it [59:25,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1457it [59:27,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1458it [59:31,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1459it [59:33,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1460it [59:35,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1461it [59:39,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1462it [59:42,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1463it [59:45,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1464it [59:48,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1465it [59:49,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1466it [59:53,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1467it [59:55,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1468it [59:57,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1469it [59:59,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1470it [1:00:01,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1471it [1:00:03,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1472it [1:00:06,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1473it [1:00:08,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1474it [1:00:11,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1475it [1:00:14,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1476it [1:00:15,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1477it [1:00:18,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1478it [1:00:21,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:45:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:45:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1479it [1:00:26,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1480it [1:00:28,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1481it [1:00:32,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1482it [1:00:33,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1483it [1:00:36,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1484it [1:00:39,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1485it [1:00:41,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1486it [1:00:42,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1487it [1:00:45,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1488it [1:00:48,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1489it [1:00:51,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1490it [1:00:52,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1491it [1:00:55,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1492it [1:00:58,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1493it [1:01:00,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1494it [1:01:04,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1495it [1:01:06,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1496it [1:01:07,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1497it [1:01:11,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1498it [1:01:12,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1499it [1:01:15,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1500it [1:01:19,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [1:01:22,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:46:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:46:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1502it [1:01:23,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1503it [1:01:25,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1504it [1:01:27,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1505it [1:01:29,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1506it [1:01:31,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1507it [1:01:35,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1508it [1:01:38,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1509it [1:01:40,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1510it [1:01:42,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1511it [1:01:43,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1512it [1:01:46,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1513it [1:01:47,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1514it [1:01:49,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1515it [1:01:52,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1516it [1:01:54,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1517it [1:01:56,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1518it [1:01:59,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1519it [1:02:01,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1520it [1:02:03,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1521it [1:02:07,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1522it [1:02:10,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1523it [1:02:14,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1524it [1:02:16,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1525it [1:02:17,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1526it [1:02:20,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:47:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:47:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1527it [1:02:23,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1528it [1:02:25,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1529it [1:02:28,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1530it [1:02:30,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1531it [1:02:32,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1532it [1:02:34,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1533it [1:02:36,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1534it [1:02:37,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1535it [1:02:40,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1536it [1:02:43,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1537it [1:02:46,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1538it [1:02:49,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1539it [1:02:51,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1540it [1:02:52,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1541it [1:02:54,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1542it [1:02:56,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1543it [1:02:59,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1544it [1:03:03,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1545it [1:03:07,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1546it [1:03:09,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1547it [1:03:10,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1548it [1:03:13,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1549it [1:03:15,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1550it [1:03:17,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1551it [1:03:19,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1552it [1:03:22,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:48:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:48:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1553it [1:03:24,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1554it [1:03:26,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1555it [1:03:28,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1556it [1:03:30,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1557it [1:03:33,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1558it [1:03:36,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1559it [1:03:38,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1560it [1:03:40,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1561it [1:03:42,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [1:03:44,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1563it [1:03:47,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1564it [1:03:51,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1565it [1:03:52,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1566it [1:03:54,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1567it [1:03:56,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1568it [1:03:58,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1569it [1:04:01,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1570it [1:04:03,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1571it [1:04:04,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1572it [1:04:06,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1573it [1:04:08,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1574it [1:04:11,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1575it [1:04:12,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1576it [1:04:14,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1577it [1:04:16,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1578it [1:04:18,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1579it [1:04:20,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1580it [1:04:22,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:49:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:49:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1581it [1:04:24,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1582it [1:04:26,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1583it [1:04:29,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1584it [1:04:31,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1585it [1:04:35,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1586it [1:04:38,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1587it [1:04:41,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1588it [1:04:44,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1589it [1:04:46,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1590it [1:04:50,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1591it [1:04:52,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1592it [1:04:54,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1593it [1:04:56,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1594it [1:04:58,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1595it [1:04:59,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1596it [1:05:01,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1597it [1:05:04,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1598it [1:05:06,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1599it [1:05:08,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [1:05:12,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1601it [1:05:14,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1602it [1:05:17,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1603it [1:05:20,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1604it [1:05:23,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:50:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:50:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1605it [1:05:25,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1606it [1:05:28,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1607it [1:05:31,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1608it [1:05:33,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1609it [1:05:36,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1610it [1:05:38,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1611it [1:05:41,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1612it [1:05:43,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1613it [1:05:45,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1614it [1:05:46,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1615it [1:05:48,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1616it [1:05:51,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1617it [1:05:53,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1618it [1:05:55,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1619it [1:05:58,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1620it [1:05:59,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1621it [1:06:01,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1622it [1:06:03,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1623it [1:06:06,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1624it [1:06:10,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1625it [1:06:12,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1626it [1:06:15,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1627it [1:06:16,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1628it [1:06:18,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1629it [1:06:20,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1630it [1:06:22,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:51:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:51:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1631it [1:06:24,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1632it [1:06:26,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1633it [1:06:29,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1634it [1:06:31,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1635it [1:06:33,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1636it [1:06:35,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1637it [1:06:37,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1638it [1:06:39,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1639it [1:06:41,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1640it [1:06:45,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1641it [1:06:47,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1642it [1:06:49,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1643it [1:06:52,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1644it [1:06:53,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1645it [1:06:55,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1646it [1:06:59,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1647it [1:07:02,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1648it [1:07:04,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649it [1:07:06,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1650it [1:07:09,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1651it [1:07:10,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1652it [1:07:11,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1653it [1:07:13,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654it [1:07:16,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1655it [1:07:18,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1656it [1:07:20,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1657it [1:07:23,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:52:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:52:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1658it [1:07:24,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1659it [1:07:26,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1660it [1:07:29,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1661it [1:07:34,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1662it [1:07:37,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1663it [1:07:39,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1664it [1:07:42,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1665it [1:07:44,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1666it [1:07:46,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1667it [1:07:49,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1668it [1:07:51,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1669it [1:07:53,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1670it [1:07:55,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1671it [1:07:59,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1672it [1:08:01,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1673it [1:08:04,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1674it [1:08:06,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1675it [1:08:09,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1676it [1:08:12,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1677it [1:08:13,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1678it [1:08:15,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1679it [1:08:18,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1680it [1:08:20,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:53:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:53:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1681it [1:08:23,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1682it [1:08:26,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1683it [1:08:29,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1684it [1:08:30,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1685it [1:08:33,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1686it [1:08:34,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1687it [1:08:38,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1688it [1:08:41,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1689it [1:08:44,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1690it [1:08:46,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1691it [1:08:50,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1692it [1:08:53,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1693it [1:08:57,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1694it [1:08:59,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1695it [1:09:02,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1696it [1:09:04,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1697it [1:09:06,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1698it [1:09:07,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1699it [1:09:09,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1700it [1:09:11,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1701it [1:09:14,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1702it [1:09:17,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1703it [1:09:19,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1704it [1:09:22,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:54:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:54:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1705it [1:09:23,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1706it [1:09:26,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1707it [1:09:30,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1708it [1:09:32,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1709it [1:09:35,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1710it [1:09:38,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1711it [1:09:42,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1712it [1:09:43,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1713it [1:09:45,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1714it [1:09:47,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1715it [1:09:49,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1716it [1:09:53,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1717it [1:09:55,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1718it [1:09:57,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1719it [1:09:59,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1720it [1:10:03,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1721it [1:10:07,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1722it [1:10:08,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1723it [1:10:13,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1724it [1:10:15,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1725it [1:10:18,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1726it [1:10:20,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1727it [1:10:22,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:55:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:55:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1728it [1:10:24,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1729it [1:10:26,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1730it [1:10:28,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1731it [1:10:35,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1732it [1:10:37,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1733it [1:10:37,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1734it [1:10:39,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1735it [1:10:41,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1736it [1:10:43,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1737it [1:10:46,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1738it [1:10:47,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1739it [1:10:49,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1740it [1:10:51,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1741it [1:10:55,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1742it [1:10:58,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1743it [1:10:59,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1744it [1:11:02,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1745it [1:11:04,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1746it [1:11:06,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1747it [1:11:07,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1748it [1:11:10,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1749it [1:11:12,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1750it [1:11:13,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1751it [1:11:16,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1752it [1:11:19,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1753it [1:11:22,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:56:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:56:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1754it [1:11:25,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1755it [1:11:28,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1756it [1:11:30,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1757it [1:11:32,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1758it [1:11:34,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1759it [1:11:36,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1760it [1:11:37,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1761it [1:11:39,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1762it [1:11:43,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1763it [1:11:47,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1764it [1:11:49,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1765it [1:11:52,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1766it [1:11:54,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1767it [1:11:55,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1768it [1:11:57,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1769it [1:12:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1770it [1:12:04,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1771it [1:12:05,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1772it [1:12:08,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1773it [1:12:10,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1774it [1:12:12,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1775it [1:12:14,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1776it [1:12:17,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1777it [1:12:20,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1778it [1:12:21,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:57:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:57:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1779it [1:12:23,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1780it [1:12:25,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1781it [1:12:27,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1782it [1:12:29,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1783it [1:12:31,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1784it [1:12:35,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1785it [1:12:37,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1786it [1:12:39,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1787it [1:12:41,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1788it [1:12:43,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1789it [1:12:46,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1790it [1:12:48,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1791it [1:12:50,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1792it [1:12:52,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1793it [1:12:54,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1794it [1:12:56,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1795it [1:12:58,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1796it [1:13:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1797it [1:13:02,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1798it [1:13:05,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1799it [1:13:10,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1800it [1:13:12,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1801it [1:13:16,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1802it [1:13:19,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1803it [1:13:20,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:58:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:58:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1804it [1:13:24,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1805it [1:13:27,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1806it [1:13:29,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1807it [1:13:31,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1808it [1:13:34,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1809it [1:13:37,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1810it [1:13:39,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1811it [1:13:41,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1812it [1:13:43,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1813it [1:13:45,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1814it [1:13:47,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1815it [1:13:49,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1816it [1:13:51,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1817it [1:13:54,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1818it [1:13:56,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1819it [1:13:58,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1820it [1:14:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1821it [1:14:02,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1822it [1:14:05,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1823it [1:14:08,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1824it [1:14:09,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1825it [1:14:11,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1826it [1:14:13,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1827it [1:14:15,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1828it [1:14:20,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 06:59:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 06:59:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1829it [1:14:24,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1830it [1:14:28,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1831it [1:14:30,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1832it [1:14:31,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1833it [1:14:33,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1834it [1:14:34,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1835it [1:14:36,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1836it [1:14:40,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1837it [1:14:42,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1838it [1:14:44,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1839it [1:14:45,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1840it [1:14:47,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1841it [1:14:52,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1842it [1:14:55,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1843it [1:14:57,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1844it [1:14:59,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1845it [1:15:02,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1846it [1:15:04,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1847it [1:15:06,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1848it [1:15:09,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1849it [1:15:13,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1850it [1:15:15,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1851it [1:15:17,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1852it [1:15:20,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:00:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:00:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1853it [1:15:23,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1854it [1:15:26,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1855it [1:15:28,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1856it [1:15:29,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1857it [1:15:32,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1858it [1:15:33,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1859it [1:15:36,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1860it [1:15:38,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1861it [1:15:41,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1862it [1:15:45,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1863it [1:15:47,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1864it [1:15:49,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1865it [1:15:51,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1866it [1:15:53,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1867it [1:15:56,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1868it [1:15:59,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1869it [1:16:00,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1870it [1:16:02,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1871it [1:16:05,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1872it [1:16:07,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1873it [1:16:08,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1874it [1:16:10,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [1:16:12,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1876it [1:16:14,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1877it [1:16:16,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1878it [1:16:18,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1879it [1:16:21,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:01:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:01:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1880it [1:16:23,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1881it [1:16:27,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1882it [1:16:30,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1883it [1:16:32,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1884it [1:16:34,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1885it [1:16:36,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1886it [1:16:38,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1887it [1:16:42,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888it [1:16:44,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1889it [1:16:46,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1890it [1:16:48,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1891it [1:16:51,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1892it [1:16:54,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1893it [1:16:56,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1894it [1:16:58,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1895it [1:17:01,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1896it [1:17:02,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1897it [1:17:04,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1898it [1:17:06,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1899it [1:17:09,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900it [1:17:10,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1901it [1:17:12,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1902it [1:17:13,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1903it [1:17:16,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1904it [1:17:19,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1905it [1:17:22,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:02:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:02:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1906it [1:17:25,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1907it [1:17:28,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1908it [1:17:30,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1909it [1:17:32,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1910it [1:17:35,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1911it [1:17:36,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1912it [1:17:39,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1913it [1:17:40,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1914it [1:17:42,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1915it [1:17:44,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1916it [1:17:46,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917it [1:17:50,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1918it [1:17:54,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1919it [1:17:55,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1920it [1:17:57,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1921it [1:17:59,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1922it [1:18:01,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1923it [1:18:04,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1924it [1:18:07,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1925it [1:18:09,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1926it [1:18:12,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1927it [1:18:14,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1928it [1:18:16,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1929it [1:18:18,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1930it [1:18:21,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:03:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:03:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1931it [1:18:23,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1932it [1:18:26,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1933it [1:18:27,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1934it [1:18:29,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1935it [1:18:32,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1936it [1:18:35,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1937it [1:18:36,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1938it [1:18:40,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1939it [1:18:43,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1940it [1:18:46,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1941it [1:18:48,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1942it [1:18:50,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1943it [1:18:52,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1944it [1:18:55,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1945it [1:18:58,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1946it [1:19:01,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1947it [1:19:03,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1948it [1:19:07,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1949it [1:19:08,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1950it [1:19:10,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1951it [1:19:12,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1952it [1:19:14,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1953it [1:19:16,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1954it [1:19:18,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1955it [1:19:20,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1956it [1:19:22,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:04:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:04:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1957it [1:19:24,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1958it [1:19:26,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1959it [1:19:28,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1960it [1:19:30,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1961it [1:19:32,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1962it [1:19:34,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1963it [1:19:37,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1964it [1:19:39,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1965it [1:19:42,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1966it [1:19:44,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1967it [1:19:47,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1968it [1:19:50,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1969it [1:19:52,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1970it [1:19:54,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1971it [1:19:56,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1972it [1:20:00,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1973it [1:20:02,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1974it [1:20:04,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1975it [1:20:06,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1976it [1:20:08,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1977it [1:20:12,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1978it [1:20:14,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1979it [1:20:17,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1980it [1:20:20,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1981it [1:20:22,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:05:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:05:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1982it [1:20:23,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1983it [1:20:25,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1984it [1:20:27,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1985it [1:20:31,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1986it [1:20:34,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1987it [1:20:37,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1988it [1:20:39,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1989it [1:20:41,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1990it [1:20:44,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1991it [1:20:47,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1992it [1:20:50,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1993it [1:20:52,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1994it [1:20:55,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1995it [1:20:57,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1996it [1:21:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1997it [1:21:02,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1998it [1:21:05,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999it [1:21:07,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [1:21:11,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [1:21:14,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2002it [1:21:16,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2003it [1:21:19,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2004it [1:21:21,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:06:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:06:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2005it [1:21:23,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2006it [1:21:25,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2007it [1:21:27,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2008it [1:21:30,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2009it [1:21:34,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2010it [1:21:35,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2011it [1:21:38,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2012it [1:21:40,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2013it [1:21:42,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2014it [1:21:44,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015it [1:21:48,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016it [1:21:50,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017it [1:21:52,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018it [1:21:54,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019it [1:21:56,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020it [1:21:59,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021it [1:22:02,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022it [1:22:03,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023it [1:22:06,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024it [1:22:08,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025it [1:22:09,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026it [1:22:12,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2027it [1:22:15,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2028it [1:22:17,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2029it [1:22:18,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2030it [1:22:21,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2031it [1:22:22,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:07:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:07:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2032it [1:22:24,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2033it [1:22:27,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2034it [1:22:29,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2035it [1:22:31,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2036it [1:22:33,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2037it [1:22:35,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2038it [1:22:39,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2039it [1:22:41,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2040it [1:22:43,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2041it [1:22:44,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2042it [1:22:46,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2043it [1:22:49,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2044it [1:22:52,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2045it [1:22:54,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2046it [1:22:56,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2047it [1:22:59,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2048it [1:23:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2049it [1:23:03,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2050it [1:23:06,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2051it [1:23:09,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2052it [1:23:10,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2053it [1:23:14,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2054it [1:23:17,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2055it [1:23:18,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2056it [1:23:21,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2057it [1:23:23,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:08:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:08:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2058it [1:23:25,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2059it [1:23:27,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2060it [1:23:29,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2061it [1:23:31,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2062it [1:23:34,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2063it [1:23:36,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2064it [1:23:37,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2065it [1:23:40,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2066it [1:23:42,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2067it [1:23:45,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2068it [1:23:46,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2069it [1:23:48,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2070it [1:23:50,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071it [1:23:52,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2072it [1:23:53,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2073it [1:23:55,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2074it [1:23:59,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2075it [1:24:02,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2076it [1:24:04,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2077it [1:24:06,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2078it [1:24:08,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2079it [1:24:11,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2080it [1:24:12,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2081it [1:24:14,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2082it [1:24:16,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2083it [1:24:19,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:09:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:09:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2084it [1:24:23,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2085it [1:24:25,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2086it [1:24:28,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2087it [1:24:30,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2088it [1:24:31,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2089it [1:24:33,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2090it [1:24:35,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2091it [1:24:38,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2092it [1:24:40,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2093it [1:24:42,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2094it [1:24:44,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2095it [1:24:47,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2096it [1:24:49,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2097it [1:24:51,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2098it [1:24:55,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2099it [1:24:56,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2100it [1:24:58,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2101it [1:25:02,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2102it [1:25:04,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2103it [1:25:06,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2104it [1:25:08,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2105it [1:25:11,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2106it [1:25:12,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2107it [1:25:15,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2108it [1:25:19,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2109it [1:25:21,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2110it [1:25:22,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:10:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:10:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2111it [1:25:24,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2112it [1:25:26,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2113it [1:25:29,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2114it [1:25:31,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2115it [1:25:34,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2116it [1:25:37,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2117it [1:25:38,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2118it [1:25:40,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2119it [1:25:43,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2120it [1:25:45,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2121it [1:25:48,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2122it [1:25:50,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2123it [1:25:53,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2124it [1:25:55,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2125it [1:25:56,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2126it [1:26:00,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2127it [1:26:04,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2128it [1:26:08,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2129it [1:26:11,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2130it [1:26:13,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2131it [1:26:17,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2132it [1:26:20,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:11:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:11:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2133it [1:26:23,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2134it [1:26:25,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2135it [1:26:28,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2136it [1:26:30,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2137it [1:26:32,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2138it [1:26:34,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2139it [1:26:38,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2140it [1:26:39,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2141it [1:26:41,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2142it [1:26:44,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2143it [1:26:45,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2144it [1:26:48,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2145it [1:26:52,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2146it [1:26:54,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2147it [1:26:56,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2148it [1:26:58,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2149it [1:27:00,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2150it [1:27:03,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2151it [1:27:04,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2152it [1:27:07,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2153it [1:27:09,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2154it [1:27:13,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2155it [1:27:16,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2156it [1:27:18,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2157it [1:27:21,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:12:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:12:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2158it [1:27:23,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2159it [1:27:27,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2160it [1:27:29,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2161it [1:27:33,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2162it [1:27:37,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2163it [1:27:41,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2164it [1:27:44,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2165it [1:27:47,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2166it [1:27:51,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2167it [1:27:53,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2168it [1:27:55,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2169it [1:27:59,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2170it [1:28:02,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2171it [1:28:07,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2172it [1:28:11,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2173it [1:28:13,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2174it [1:28:15,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2175it [1:28:18,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2176it [1:28:21,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:13:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:13:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2177it [1:28:24,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2178it [1:28:26,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2179it [1:28:29,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2180it [1:28:32,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2181it [1:28:34,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2182it [1:28:36,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2183it [1:28:40,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2184it [1:28:41,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2185it [1:28:44,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2186it [1:28:45,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2187it [1:28:47,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2188it [1:28:51,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2189it [1:28:53,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2190it [1:28:54,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2191it [1:28:56,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2192it [1:28:58,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2193it [1:29:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2194it [1:29:04,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2195it [1:29:05,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196it [1:29:06,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2197it [1:29:10,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2198it [1:29:13,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2199it [1:29:16,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2200it [1:29:17,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2201it [1:29:19,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2202it [1:29:21,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2203it [1:29:22,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:14:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:14:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2204it [1:29:26,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2205it [1:29:27,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2206it [1:29:29,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2207it [1:29:31,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2208it [1:29:35,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2209it [1:29:37,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2210it [1:29:40,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2211it [1:29:43,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2212it [1:29:46,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2213it [1:29:48,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2214it [1:29:50,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2215it [1:29:54,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2216it [1:29:57,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2217it [1:29:59,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2218it [1:30:02,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2219it [1:30:04,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2220it [1:30:08,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2221it [1:30:11,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2222it [1:30:15,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2223it [1:30:17,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2224it [1:30:19,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2225it [1:30:21,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:15:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:15:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2226it [1:30:24,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2227it [1:30:26,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2228it [1:30:29,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2229it [1:30:33,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2230it [1:30:36,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2231it [1:30:37,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2232it [1:30:41,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2233it [1:30:42,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2234it [1:30:44,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2235it [1:30:48,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2236it [1:30:50,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2237it [1:30:53,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2238it [1:30:56,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2239it [1:30:58,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2240it [1:30:59,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2241it [1:31:02,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2242it [1:31:05,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2243it [1:31:06,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2244it [1:31:08,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2245it [1:31:09,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2246it [1:31:12,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2247it [1:31:15,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2248it [1:31:18,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2249it [1:31:21,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:16:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:16:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2250it [1:31:24,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2251it [1:31:26,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2252it [1:31:28,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2253it [1:31:30,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2254it [1:31:32,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2255it [1:31:35,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2256it [1:31:38,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2257it [1:31:40,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2258it [1:31:41,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2259it [1:31:44,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2260it [1:31:46,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2261it [1:31:48,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2262it [1:31:50,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2263it [1:31:53,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2264it [1:31:55,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2265it [1:31:58,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2266it [1:32:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2267it [1:32:02,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2268it [1:32:05,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2269it [1:32:07,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2270it [1:32:09,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2271it [1:32:11,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2272it [1:32:13,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2273it [1:32:15,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2274it [1:32:17,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2275it [1:32:20,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2276it [1:32:22,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:17:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:17:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2277it [1:32:24,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2278it [1:32:25,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2279it [1:32:27,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2280it [1:32:29,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2281it [1:32:31,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2282it [1:32:32,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2283it [1:32:36,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2284it [1:32:40,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2285it [1:32:42,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2286it [1:32:45,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2287it [1:32:47,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2288it [1:32:49,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2289it [1:32:51,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2290it [1:32:53,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2291it [1:32:55,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2292it [1:32:56,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2293it [1:32:59,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2294it [1:33:01,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2295it [1:33:03,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2296it [1:33:05,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2297it [1:33:07,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2298it [1:33:08,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2299it [1:33:10,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2300it [1:33:11,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2301it [1:33:14,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2302it [1:33:16,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2303it [1:33:18,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2304it [1:33:20,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2305it [1:33:23,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:18:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:18:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2306it [1:33:26,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2307it [1:33:29,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2308it [1:33:32,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2309it [1:33:34,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2310it [1:33:36,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2311it [1:33:37,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2312it [1:33:40,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2313it [1:33:43,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2314it [1:33:46,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2315it [1:33:50,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2316it [1:33:53,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2317it [1:33:57,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2318it [1:33:59,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2319it [1:34:03,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2320it [1:34:07,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2321it [1:34:10,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2322it [1:34:12,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2323it [1:34:14,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2324it [1:34:15,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2325it [1:34:17,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2326it [1:34:19,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2327it [1:34:22,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:19:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:19:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2328it [1:34:24,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2329it [1:34:26,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2330it [1:34:28,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2331it [1:34:29,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2332it [1:34:31,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2333it [1:34:35,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2334it [1:34:36,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2335it [1:34:39,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2336it [1:34:41,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2337it [1:34:42,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2338it [1:34:45,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2339it [1:34:46,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2340it [1:34:50,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2341it [1:34:52,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2342it [1:34:54,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2343it [1:34:57,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2344it [1:34:59,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2345it [1:35:02,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2346it [1:35:06,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2347it [1:35:07,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2348it [1:35:10,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2349it [1:35:13,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2350it [1:35:15,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2351it [1:35:17,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2352it [1:35:19,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2353it [1:35:21,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2354it [1:35:23,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:20:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:20:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2355it [1:35:26,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2356it [1:35:29,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2357it [1:35:33,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2358it [1:35:34,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2359it [1:35:36,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2360it [1:35:38,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2361it [1:35:40,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2362it [1:35:43,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2363it [1:35:47,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2364it [1:35:49,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2365it [1:35:51,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2366it [1:35:53,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2367it [1:35:57,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2368it [1:36:01,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2369it [1:36:03,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2370it [1:36:06,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2371it [1:36:08,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2372it [1:36:10,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2373it [1:36:14,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2374it [1:36:16,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2375it [1:36:18,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2376it [1:36:21,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:21:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:21:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2377it [1:36:24,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2378it [1:36:26,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2379it [1:36:28,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2380it [1:36:30,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2381it [1:36:32,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2382it [1:36:34,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2383it [1:36:36,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2384it [1:36:39,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2385it [1:36:41,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2386it [1:36:43,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2387it [1:36:45,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2388it [1:36:47,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2389it [1:36:48,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2390it [1:36:52,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2391it [1:36:54,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2392it [1:36:56,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2393it [1:37:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2394it [1:37:01,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2395it [1:37:04,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2396it [1:37:07,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2397it [1:37:10,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2398it [1:37:14,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2399it [1:37:17,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2400it [1:37:20,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:22:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:22:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2401it [1:37:23,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2402it [1:37:24,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2403it [1:37:27,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2404it [1:37:28,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2405it [1:37:31,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2406it [1:37:33,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2407it [1:37:35,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2408it [1:37:38,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2409it [1:37:42,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2410it [1:37:44,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2411it [1:37:46,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2412it [1:37:47,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2413it [1:37:50,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2414it [1:37:52,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2415it [1:37:55,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2416it [1:37:58,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2417it [1:38:01,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2418it [1:38:03,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2419it [1:38:06,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2420it [1:38:08,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2421it [1:38:10,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2422it [1:38:13,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2423it [1:38:16,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2424it [1:38:17,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2425it [1:38:19,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2426it [1:38:21,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2427it [1:38:23,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:23:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:23:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2428it [1:38:26,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2429it [1:38:29,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2430it [1:38:33,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2431it [1:38:37,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2432it [1:38:38,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2433it [1:38:42,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2434it [1:38:46,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2435it [1:38:48,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2436it [1:38:49,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2437it [1:38:52,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2438it [1:38:54,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2439it [1:38:56,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2440it [1:38:58,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2441it [1:39:02,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2442it [1:39:04,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2443it [1:39:08,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2444it [1:39:11,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2445it [1:39:15,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [1:39:17,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2447it [1:39:20,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:24:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:24:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2448it [1:39:24,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2449it [1:39:25,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2450it [1:39:29,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2451it [1:39:31,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2452it [1:39:33,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2453it [1:39:34,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2454it [1:39:36,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2455it [1:39:38,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2456it [1:39:40,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2457it [1:39:43,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2458it [1:39:45,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2459it [1:39:47,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2460it [1:39:49,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2461it [1:39:53,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2462it [1:39:57,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2463it [1:39:59,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2464it [1:40:01,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2465it [1:40:03,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2466it [1:40:05,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2467it [1:40:09,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2468it [1:40:13,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2469it [1:40:15,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2470it [1:40:18,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2471it [1:40:20,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:25:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:25:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2472it [1:40:23,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2473it [1:40:25,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2474it [1:40:28,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2475it [1:40:30,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2476it [1:40:32,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2477it [1:40:33,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2478it [1:40:37,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2479it [1:40:39,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2480it [1:40:41,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2481it [1:40:45,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2482it [1:40:47,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2483it [1:40:50,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2484it [1:40:52,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2485it [1:40:55,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2486it [1:40:58,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2487it [1:41:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2488it [1:41:02,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2489it [1:41:06,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2490it [1:41:09,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2491it [1:41:10,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2492it [1:41:14,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2493it [1:41:17,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2494it [1:41:19,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2495it [1:41:22,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:26:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:26:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2496it [1:41:23,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2497it [1:41:27,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2498it [1:41:31,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2499it [1:41:33,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2500it [1:41:36,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2501it [1:41:39,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2502it [1:41:42,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2503it [1:41:45,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2504it [1:41:47,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2505it [1:41:50,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2506it [1:41:51,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2507it [1:41:53,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2508it [1:41:55,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2509it [1:41:59,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2510it [1:42:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2511it [1:42:03,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2512it [1:42:05,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2513it [1:42:08,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2514it [1:42:09,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2515it [1:42:13,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2516it [1:42:16,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2517it [1:42:18,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2518it [1:42:20,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2519it [1:42:23,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:27:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:27:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2520it [1:42:25,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2521it [1:42:27,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2522it [1:42:29,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2523it [1:42:31,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2524it [1:42:33,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2525it [1:42:35,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2526it [1:42:36,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2527it [1:42:38,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2528it [1:42:40,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2529it [1:42:42,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2530it [1:42:45,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2531it [1:42:48,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2532it [1:42:50,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2533it [1:42:52,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2534it [1:42:55,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2535it [1:42:57,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2536it [1:43:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2537it [1:43:01,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2538it [1:43:04,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2539it [1:43:06,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2540it [1:43:08,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2541it [1:43:10,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2542it [1:43:12,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2543it [1:43:15,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2544it [1:43:19,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2545it [1:43:23,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:28:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:28:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2546it [1:43:24,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2547it [1:43:26,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2548it [1:43:29,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2549it [1:43:32,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2550it [1:43:34,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2551it [1:43:37,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2552it [1:43:39,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2553it [1:43:41,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2554it [1:43:43,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2555it [1:43:47,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2556it [1:43:50,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2557it [1:43:52,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2558it [1:43:53,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2559it [1:43:56,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2560it [1:43:58,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2561it [1:43:59,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2562it [1:44:01,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2563it [1:44:03,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2564it [1:44:06,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2565it [1:44:08,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2566it [1:44:10,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2567it [1:44:14,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2568it [1:44:17,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2569it [1:44:20,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:29:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:29:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2570it [1:44:24,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2571it [1:44:25,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2572it [1:44:27,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2573it [1:44:30,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2574it [1:44:32,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2575it [1:44:35,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2576it [1:44:38,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2577it [1:44:40,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2578it [1:44:42,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2579it [1:44:45,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2580it [1:44:47,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2581it [1:44:49,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2582it [1:44:52,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2583it [1:44:55,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2584it [1:45:00,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2585it [1:45:03,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2586it [1:45:06,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2587it [1:45:08,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2588it [1:45:10,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2589it [1:45:13,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2590it [1:45:15,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2591it [1:45:18,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2592it [1:45:20,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2593it [1:45:22,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:30:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:30:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2594it [1:45:24,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2595it [1:45:28,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2596it [1:45:30,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2597it [1:45:32,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2598it [1:45:34,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2599it [1:45:38,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2600it [1:45:41,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2601it [1:45:44,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2602it [1:45:46,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2603it [1:45:48,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2604it [1:45:51,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2605it [1:45:53,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2606it [1:45:55,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2607it [1:45:59,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2608it [1:46:01,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2609it [1:46:03,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2610it [1:46:06,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2611it [1:46:08,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2612it [1:46:10,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2613it [1:46:12,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2614it [1:46:13,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2615it [1:46:15,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2616it [1:46:17,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2617it [1:46:19,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2618it [1:46:21,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:31:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:31:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2619it [1:46:23,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2620it [1:46:27,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2621it [1:46:29,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2622it [1:46:31,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2623it [1:46:35,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2624it [1:46:36,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2625it [1:46:38,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2626it [1:46:41,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2627it [1:46:42,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2628it [1:46:44,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2629it [1:46:46,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2630it [1:46:47,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2631it [1:46:49,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2632it [1:46:51,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2633it [1:46:53,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2634it [1:46:54,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2635it [1:46:56,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2636it [1:46:58,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2637it [1:47:01,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2638it [1:47:04,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2639it [1:47:08,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2640it [1:47:10,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2641it [1:47:14,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2642it [1:47:15,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2643it [1:47:18,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2644it [1:47:21,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:32:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:32:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2645it [1:47:24,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2646it [1:47:26,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2647it [1:47:27,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2648it [1:47:29,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2649it [1:47:32,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2650it [1:47:34,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2651it [1:47:36,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2652it [1:47:38,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2653it [1:47:40,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2654it [1:47:42,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2655it [1:47:44,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2656it [1:47:46,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2657it [1:47:49,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2658it [1:47:52,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2659it [1:47:54,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2660it [1:47:57,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2661it [1:47:59,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2662it [1:48:01,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2663it [1:48:03,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2664it [1:48:07,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2665it [1:48:10,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2666it [1:48:12,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2667it [1:48:14,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2668it [1:48:16,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2669it [1:48:19,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2670it [1:48:22,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:33:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:33:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2671it [1:48:23,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2672it [1:48:27,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2673it [1:48:29,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2674it [1:48:32,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2675it [1:48:35,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2676it [1:48:38,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2677it [1:48:42,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2678it [1:48:45,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2679it [1:48:49,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2680it [1:48:51,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2681it [1:48:53,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2682it [1:48:54,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2683it [1:48:58,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2684it [1:49:01,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2685it [1:49:03,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2686it [1:49:04,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2687it [1:49:07,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2688it [1:49:10,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2689it [1:49:12,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2690it [1:49:14,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2691it [1:49:17,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2692it [1:49:19,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2693it [1:49:22,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:34:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:34:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2694it [1:49:25,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2695it [1:49:28,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2696it [1:49:31,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2697it [1:49:33,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2698it [1:49:34,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2699it [1:49:38,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2700it [1:49:40,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2701it [1:49:42,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2702it [1:49:45,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2703it [1:49:47,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2704it [1:49:50,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2705it [1:49:53,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2706it [1:49:55,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2707it [1:49:57,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2708it [1:50:01,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2709it [1:50:03,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2710it [1:50:06,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2711it [1:50:10,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2712it [1:50:12,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2713it [1:50:16,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2714it [1:50:18,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2715it [1:50:20,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:35:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:35:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2716it [1:50:24,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2717it [1:50:25,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2718it [1:50:27,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2719it [1:50:30,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2720it [1:50:32,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2721it [1:50:34,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2722it [1:50:36,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2723it [1:50:38,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2724it [1:50:40,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2725it [1:50:42,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2726it [1:50:45,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2727it [1:50:48,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2728it [1:50:50,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2729it [1:50:51,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2730it [1:50:55,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2731it [1:50:58,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2732it [1:51:01,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2733it [1:51:05,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2734it [1:51:07,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2735it [1:51:09,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2736it [1:51:10,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2737it [1:51:14,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2738it [1:51:16,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2739it [1:51:17,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2740it [1:51:22,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:36:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:36:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2741it [1:51:26,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2742it [1:51:28,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2743it [1:51:32,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2744it [1:51:35,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2745it [1:51:39,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2746it [1:51:40,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2747it [1:51:42,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2748it [1:51:46,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2749it [1:51:48,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2750it [1:51:51,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2751it [1:51:53,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2752it [1:51:55,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2753it [1:51:57,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2754it [1:52:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2755it [1:52:02,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2756it [1:52:04,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2757it [1:52:07,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2758it [1:52:11,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2759it [1:52:13,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2760it [1:52:15,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2761it [1:52:18,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2762it [1:52:21,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:37:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:37:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2763it [1:52:23,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2764it [1:52:25,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2765it [1:52:28,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2766it [1:52:31,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2767it [1:52:33,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2768it [1:52:35,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2769it [1:52:37,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2770it [1:52:39,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2771it [1:52:43,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2772it [1:52:44,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2773it [1:52:47,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2774it [1:52:49,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2775it [1:52:53,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2776it [1:52:54,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2777it [1:52:56,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2778it [1:52:58,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2779it [1:53:02,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2780it [1:53:04,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2781it [1:53:06,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2782it [1:53:08,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2783it [1:53:11,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2784it [1:53:13,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2785it [1:53:15,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2786it [1:53:17,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2787it [1:53:21,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2788it [1:53:22,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:38:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:38:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2789it [1:53:25,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2790it [1:53:26,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2791it [1:53:30,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2792it [1:53:32,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2793it [1:53:35,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2794it [1:53:37,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2795it [1:53:41,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2796it [1:53:45,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2797it [1:53:46,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2798it [1:53:48,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2799it [1:53:50,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2800it [1:53:53,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2801it [1:53:55,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2802it [1:53:57,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2803it [1:54:01,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2804it [1:54:03,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2805it [1:54:06,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2806it [1:54:09,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2807it [1:54:11,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2808it [1:54:14,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2809it [1:54:16,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2810it [1:54:17,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2811it [1:54:20,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:39:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:39:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2812it [1:54:24,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2813it [1:54:28,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2814it [1:54:31,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2815it [1:54:33,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2816it [1:54:34,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2817it [1:54:36,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2818it [1:54:38,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2819it [1:54:40,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2820it [1:54:43,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2821it [1:54:45,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2822it [1:54:47,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2823it [1:54:48,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2824it [1:54:50,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2825it [1:54:52,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2826it [1:54:54,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2827it [1:54:56,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2828it [1:54:58,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2829it [1:55:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2830it [1:55:02,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2831it [1:55:05,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2832it [1:55:09,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2833it [1:55:11,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2834it [1:55:13,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2835it [1:55:16,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2836it [1:55:18,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2837it [1:55:22,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:40:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:40:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2838it [1:55:24,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2839it [1:55:27,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2840it [1:55:31,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2841it [1:55:34,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2842it [1:55:37,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2843it [1:55:40,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2844it [1:55:41,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2845it [1:55:43,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2846it [1:55:46,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2847it [1:55:48,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2848it [1:55:50,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2849it [1:55:54,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2850it [1:55:56,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2851it [1:56:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2852it [1:56:01,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2853it [1:56:03,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2854it [1:56:06,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2855it [1:56:09,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2856it [1:56:12,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2857it [1:56:15,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2858it [1:56:17,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2859it [1:56:20,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2860it [1:56:22,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:41:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:41:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2861it [1:56:24,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2862it [1:56:28,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2863it [1:56:31,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2864it [1:56:33,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2865it [1:56:35,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2866it [1:56:37,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2867it [1:56:39,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2868it [1:56:42,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2869it [1:56:44,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2870it [1:56:48,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2871it [1:56:52,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2872it [1:56:54,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2873it [1:56:56,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2874it [1:57:00,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2875it [1:57:02,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2876it [1:57:04,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2877it [1:57:06,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2878it [1:57:08,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2879it [1:57:10,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2880it [1:57:14,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2881it [1:57:16,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2882it [1:57:20,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2883it [1:57:23,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:42:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:42:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2884it [1:57:25,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2885it [1:57:27,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2886it [1:57:30,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2887it [1:57:32,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2888it [1:57:34,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2889it [1:57:36,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2890it [1:57:37,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2891it [1:57:41,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2892it [1:57:45,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2893it [1:57:46,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2894it [1:57:49,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2895it [1:57:52,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2896it [1:57:56,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2897it [1:57:59,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2898it [1:58:01,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2899it [1:58:03,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2900it [1:58:04,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2901it [1:58:06,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2902it [1:58:08,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2903it [1:58:10,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2904it [1:58:12,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2905it [1:58:15,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2906it [1:58:18,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2907it [1:58:20,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2908it [1:58:22,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:43:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:43:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2909it [1:58:24,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2910it [1:58:25,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2911it [1:58:27,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2912it [1:58:29,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2913it [1:58:32,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2914it [1:58:34,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2915it [1:58:37,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2916it [1:58:40,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2917it [1:58:42,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2918it [1:58:45,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2919it [1:58:47,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2920it [1:58:49,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2921it [1:58:50,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2922it [1:58:53,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2923it [1:58:57,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2924it [1:59:02,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2925it [1:59:04,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2926it [1:59:06,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2927it [1:59:09,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2928it [1:59:12,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2929it [1:59:15,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2930it [1:59:17,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2931it [1:59:20,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2932it [1:59:21,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:44:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:44:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2933it [1:59:23,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2934it [1:59:24,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2935it [1:59:27,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2936it [1:59:30,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2937it [1:59:32,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2938it [1:59:35,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2939it [1:59:37,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2940it [1:59:40,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2941it [1:59:43,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2942it [1:59:45,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2943it [1:59:47,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2944it [1:59:49,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2945it [1:59:52,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2946it [1:59:54,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2947it [1:59:57,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2948it [1:59:59,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2949it [2:00:01,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2950it [2:00:03,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2951it [2:00:07,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2952it [2:00:10,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2953it [2:00:13,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2954it [2:00:17,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2955it [2:00:19,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2956it [2:00:21,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:45:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:45:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2957it [2:00:24,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2958it [2:00:26,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2959it [2:00:29,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2960it [2:00:32,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2961it [2:00:34,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2962it [2:00:36,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2963it [2:00:38,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2964it [2:00:40,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2965it [2:00:43,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2966it [2:00:46,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2967it [2:00:50,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2968it [2:00:52,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2969it [2:00:54,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2970it [2:00:56,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2971it [2:00:59,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2972it [2:01:03,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2973it [2:01:06,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2974it [2:01:08,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2975it [2:01:10,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2976it [2:01:11,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2977it [2:01:14,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2978it [2:01:16,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2979it [2:01:18,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2980it [2:01:20,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2981it [2:01:22,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:46:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:46:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2982it [2:01:24,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2983it [2:01:28,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2984it [2:01:32,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2985it [2:01:33,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2986it [2:01:36,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2987it [2:01:39,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2988it [2:01:43,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2989it [2:01:46,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2990it [2:01:48,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2991it [2:01:50,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2992it [2:01:54,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2993it [2:01:56,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2994it [2:01:58,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2995it [2:02:02,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2996it [2:02:03,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2997it [2:02:05,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2998it [2:02:06,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2999it [2:02:08,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [2:02:11,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [2:02:15,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3002it [2:02:19,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3003it [2:02:22,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:47:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:47:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3004it [2:02:25,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3005it [2:02:26,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3006it [2:02:28,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3007it [2:02:31,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3008it [2:02:33,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3009it [2:02:36,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3010it [2:02:39,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3011it [2:02:41,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3012it [2:02:44,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3013it [2:02:47,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3014it [2:02:49,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3015it [2:02:51,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3016it [2:02:53,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3017it [2:02:56,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3018it [2:03:00,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3019it [2:03:03,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3020it [2:03:05,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3021it [2:03:07,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3022it [2:03:08,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3023it [2:03:10,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3024it [2:03:14,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3025it [2:03:16,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3026it [2:03:17,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3027it [2:03:19,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3028it [2:03:21,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:48:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:48:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3029it [2:03:25,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3030it [2:03:28,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3031it [2:03:30,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3032it [2:03:31,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3033it [2:03:33,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3034it [2:03:35,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3035it [2:03:36,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3036it [2:03:39,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3037it [2:03:41,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3038it [2:03:43,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3039it [2:03:47,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3040it [2:03:50,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3041it [2:03:52,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3042it [2:03:54,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3043it [2:03:56,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3044it [2:03:58,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3045it [2:03:59,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3046it [2:04:02,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3047it [2:04:03,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3048it [2:04:05,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3049it [2:04:06,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3050it [2:04:08,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3051it [2:04:10,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3052it [2:04:11,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3053it [2:04:13,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3054it [2:04:15,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3055it [2:04:18,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3056it [2:04:20,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3057it [2:04:23,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:49:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:49:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3058it [2:04:24,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3059it [2:04:28,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3060it [2:04:31,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3061it [2:04:33,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3062it [2:04:35,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3063it [2:04:38,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3064it [2:04:40,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3065it [2:04:42,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3066it [2:04:45,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3067it [2:04:46,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3068it [2:04:48,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3069it [2:04:51,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3070it [2:04:54,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3071it [2:04:57,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3072it [2:05:01,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3073it [2:05:03,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3074it [2:05:05,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3075it [2:05:07,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3076it [2:05:10,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3077it [2:05:12,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3078it [2:05:14,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3079it [2:05:15,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3080it [2:05:19,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3081it [2:05:21,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:50:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:50:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3082it [2:05:24,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3083it [2:05:27,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3084it [2:05:30,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3085it [2:05:32,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3086it [2:05:34,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3087it [2:05:38,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3088it [2:05:43,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3089it [2:05:45,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3090it [2:05:47,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3091it [2:05:49,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3092it [2:05:52,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3093it [2:05:54,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3094it [2:05:57,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3095it [2:05:59,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3096it [2:06:01,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3097it [2:06:03,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3098it [2:06:05,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3099it [2:06:06,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3100it [2:06:09,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3101it [2:06:13,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3102it [2:06:16,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3103it [2:06:17,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3104it [2:06:20,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3105it [2:06:23,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:51:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:51:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3106it [2:06:25,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3107it [2:06:27,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3108it [2:06:29,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3109it [2:06:31,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3110it [2:06:32,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3111it [2:06:36,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3112it [2:06:38,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3113it [2:06:39,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3114it [2:06:42,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3115it [2:06:45,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3116it [2:06:48,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3117it [2:06:50,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3118it [2:06:51,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3119it [2:06:56,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3120it [2:06:58,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3121it [2:07:00,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3122it [2:07:02,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3123it [2:07:03,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3124it [2:07:05,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3125it [2:07:08,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3126it [2:07:09,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3127it [2:07:12,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3128it [2:07:14,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3129it [2:07:16,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3130it [2:07:18,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3131it [2:07:22,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:52:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:52:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3132it [2:07:25,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3133it [2:07:28,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3134it [2:07:30,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3135it [2:07:33,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3136it [2:07:34,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3137it [2:07:37,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3138it [2:07:39,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3139it [2:07:42,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3140it [2:07:46,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3141it [2:07:47,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3142it [2:07:49,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3143it [2:07:51,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3144it [2:07:52,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3145it [2:07:54,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3146it [2:07:57,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3147it [2:08:01,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3148it [2:08:04,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3149it [2:08:05,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3150it [2:08:07,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3151it [2:08:09,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3152it [2:08:11,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3153it [2:08:15,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3154it [2:08:18,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3155it [2:08:19,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3156it [2:08:22,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:53:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:53:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3157it [2:08:24,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3158it [2:08:26,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3159it [2:08:28,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3160it [2:08:33,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3161it [2:08:34,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3162it [2:08:38,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3163it [2:08:40,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3164it [2:08:42,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3165it [2:08:43,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3166it [2:08:45,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3167it [2:08:48,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3168it [2:08:51,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3169it [2:08:52,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3170it [2:08:55,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3171it [2:08:57,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3172it [2:08:59,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3173it [2:09:01,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3174it [2:09:02,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3175it [2:09:04,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3176it [2:09:08,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3177it [2:09:11,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3178it [2:09:13,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3179it [2:09:15,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3180it [2:09:18,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3181it [2:09:19,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:54:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:54:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3182it [2:09:24,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3183it [2:09:25,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3184it [2:09:27,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3185it [2:09:29,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3186it [2:09:31,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3187it [2:09:34,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3188it [2:09:36,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3189it [2:09:39,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3190it [2:09:41,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3191it [2:09:43,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3192it [2:09:46,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3193it [2:09:47,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3194it [2:09:51,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3195it [2:09:53,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3196it [2:09:55,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3197it [2:09:57,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3198it [2:10:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3199it [2:10:02,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3200it [2:10:03,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3201it [2:10:06,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3202it [2:10:08,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3203it [2:10:12,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3204it [2:10:14,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3205it [2:10:15,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3206it [2:10:18,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3207it [2:10:22,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:55:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:55:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3208it [2:10:24,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3209it [2:10:28,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3210it [2:10:30,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3211it [2:10:32,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3212it [2:10:34,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3213it [2:10:36,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3214it [2:10:38,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3215it [2:10:44,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3216it [2:10:46,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3217it [2:10:50,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3218it [2:10:53,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3219it [2:10:54,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3220it [2:10:56,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3221it [2:10:58,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3222it [2:11:01,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3223it [2:11:03,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3224it [2:11:07,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3225it [2:11:09,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3226it [2:11:13,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3227it [2:11:14,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3228it [2:11:16,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3229it [2:11:18,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3230it [2:11:21,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3231it [2:11:23,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:56:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:56:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3232it [2:11:25,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3233it [2:11:27,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3234it [2:11:31,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3235it [2:11:33,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3236it [2:11:36,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3237it [2:11:39,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3238it [2:11:40,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3239it [2:11:43,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3240it [2:11:46,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3241it [2:11:49,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3242it [2:11:50,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3243it [2:11:54,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3244it [2:11:58,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3245it [2:12:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3246it [2:12:04,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3247it [2:12:06,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3248it [2:12:08,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3249it [2:12:10,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3250it [2:12:12,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3251it [2:12:14,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3252it [2:12:16,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3253it [2:12:19,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3254it [2:12:21,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:57:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:57:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3255it [2:12:23,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3256it [2:12:25,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3257it [2:12:27,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3258it [2:12:29,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3259it [2:12:32,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3260it [2:12:34,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3261it [2:12:37,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3262it [2:12:39,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3263it [2:12:42,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3264it [2:12:44,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3265it [2:12:48,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3266it [2:12:49,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3267it [2:12:52,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3268it [2:12:53,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3269it [2:12:55,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3270it [2:12:57,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3271it [2:13:00,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3272it [2:13:02,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3273it [2:13:03,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3274it [2:13:07,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3275it [2:13:09,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3276it [2:13:12,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3277it [2:13:14,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3278it [2:13:16,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3279it [2:13:20,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3280it [2:13:21,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:58:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:58:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3281it [2:13:23,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3282it [2:13:26,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3283it [2:13:29,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3284it [2:13:30,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3285it [2:13:34,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3286it [2:13:37,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3287it [2:13:39,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3288it [2:13:42,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3289it [2:13:44,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3290it [2:13:47,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3291it [2:13:50,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3292it [2:13:54,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3293it [2:13:56,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3294it [2:13:58,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3295it [2:14:01,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3296it [2:14:03,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3297it [2:14:05,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3298it [2:14:07,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3299it [2:14:11,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3300it [2:14:15,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3301it [2:14:17,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3302it [2:14:18,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3303it [2:14:21,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3304it [2:14:22,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 07:59:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 07:59:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3305it [2:14:24,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3306it [2:14:27,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3307it [2:14:30,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3308it [2:14:33,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3309it [2:14:35,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3310it [2:14:36,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3311it [2:14:39,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3312it [2:14:40,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3313it [2:14:42,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3314it [2:14:46,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3315it [2:14:49,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3316it [2:14:50,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3317it [2:14:53,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3318it [2:14:55,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3319it [2:14:56,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3320it [2:14:58,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3321it [2:15:01,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3322it [2:15:03,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3323it [2:15:05,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3324it [2:15:08,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3325it [2:15:10,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3326it [2:15:11,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3327it [2:15:13,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3328it [2:15:15,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3329it [2:15:17,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3330it [2:15:18,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3331it [2:15:20,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3332it [2:15:22,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:00:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:00:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3333it [2:15:24,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3334it [2:15:26,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3335it [2:15:30,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3336it [2:15:31,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3337it [2:15:33,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3338it [2:15:35,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3339it [2:15:39,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3340it [2:15:42,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3341it [2:15:43,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3342it [2:15:45,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3343it [2:15:47,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3344it [2:15:49,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3345it [2:15:53,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3346it [2:15:55,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3347it [2:15:58,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3348it [2:16:01,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3349it [2:16:03,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3350it [2:16:05,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3351it [2:16:06,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3352it [2:16:08,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3353it [2:16:10,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3354it [2:16:12,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3355it [2:16:15,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3356it [2:16:18,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3357it [2:16:21,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3358it [2:16:23,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:01:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:01:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3359it [2:16:26,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3360it [2:16:28,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3361it [2:16:30,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3362it [2:16:35,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3363it [2:16:38,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3364it [2:16:41,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3365it [2:16:44,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3366it [2:16:46,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3367it [2:16:48,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3368it [2:16:50,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3369it [2:16:53,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3370it [2:16:55,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3371it [2:16:57,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3372it [2:16:59,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3373it [2:17:01,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3374it [2:17:03,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3375it [2:17:05,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3376it [2:17:07,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3377it [2:17:10,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3378it [2:17:12,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3379it [2:17:14,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3380it [2:17:15,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3381it [2:17:18,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3382it [2:17:21,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3383it [2:17:23,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:02:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:02:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3384it [2:17:25,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3385it [2:17:27,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3386it [2:17:29,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3387it [2:17:31,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3388it [2:17:34,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3389it [2:17:37,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3390it [2:17:41,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3391it [2:17:44,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3392it [2:17:46,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3393it [2:17:49,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3394it [2:17:51,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3395it [2:17:53,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3396it [2:17:56,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3397it [2:17:58,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3398it [2:18:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3399it [2:18:02,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3400it [2:18:05,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3401it [2:18:09,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3402it [2:18:11,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3403it [2:18:15,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3404it [2:18:17,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3405it [2:18:20,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:03:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:03:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3406it [2:18:24,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3407it [2:18:28,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3408it [2:18:30,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3409it [2:18:33,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3410it [2:18:35,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3411it [2:18:38,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3412it [2:18:40,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3413it [2:18:42,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3414it [2:18:44,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3415it [2:18:47,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3416it [2:18:50,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3417it [2:18:52,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3418it [2:18:54,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3419it [2:18:56,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3420it [2:18:57,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3421it [2:19:00,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3422it [2:19:01,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3423it [2:19:04,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3424it [2:19:07,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3425it [2:19:10,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3426it [2:19:11,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3427it [2:19:15,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3428it [2:19:17,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3429it [2:19:19,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3430it [2:19:20,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:04:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:04:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3431it [2:19:23,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3432it [2:19:25,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3433it [2:19:27,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3434it [2:19:29,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3435it [2:19:31,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3436it [2:19:33,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437it [2:19:36,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3438it [2:19:38,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3439it [2:19:40,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3440it [2:19:42,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3441it [2:19:44,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3442it [2:19:46,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3443it [2:19:47,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3444it [2:19:49,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3445it [2:19:50,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3446it [2:19:52,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3447it [2:19:54,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3448it [2:19:57,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3449it [2:19:59,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3450it [2:20:01,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3451it [2:20:04,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3452it [2:20:06,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3453it [2:20:10,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3454it [2:20:15,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3455it [2:20:17,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3456it [2:20:21,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:05:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:05:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3457it [2:20:24,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3458it [2:20:27,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3459it [2:20:28,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3460it [2:20:31,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3461it [2:20:34,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3462it [2:20:37,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3463it [2:20:39,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3464it [2:20:41,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3465it [2:20:43,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3466it [2:20:45,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3467it [2:20:46,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3468it [2:20:48,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3469it [2:20:50,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3470it [2:20:52,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3471it [2:20:55,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3472it [2:20:58,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3473it [2:21:02,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3474it [2:21:06,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3475it [2:21:08,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3476it [2:21:10,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3477it [2:21:12,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3478it [2:21:16,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3479it [2:21:19,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3480it [2:21:21,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:06:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:06:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3481it [2:21:23,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3482it [2:21:25,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3483it [2:21:29,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3484it [2:21:31,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3485it [2:21:33,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3486it [2:21:35,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3487it [2:21:37,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3488it [2:21:40,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3489it [2:21:42,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3490it [2:21:46,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3491it [2:21:49,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3492it [2:21:52,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3493it [2:21:54,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3494it [2:21:56,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3495it [2:21:58,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3496it [2:22:01,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3497it [2:22:05,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3498it [2:22:08,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3499it [2:22:13,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3500it [2:22:15,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3501it [2:22:18,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3502it [2:22:22,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:07:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:07:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3503it [2:22:26,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3504it [2:22:28,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3505it [2:22:31,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3506it [2:22:34,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3507it [2:22:36,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3508it [2:22:38,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3509it [2:22:41,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3510it [2:22:44,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3511it [2:22:48,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3512it [2:22:49,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3513it [2:22:51,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3514it [2:22:53,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3515it [2:22:56,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3516it [2:22:59,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3517it [2:23:01,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3518it [2:23:03,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3519it [2:23:06,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3520it [2:23:09,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3521it [2:23:11,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3522it [2:23:12,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3523it [2:23:15,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3524it [2:23:17,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3525it [2:23:19,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3526it [2:23:21,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:08:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:08:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3527it [2:23:24,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3528it [2:23:26,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3529it [2:23:29,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3530it [2:23:33,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3531it [2:23:35,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3532it [2:23:37,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3533it [2:23:39,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3534it [2:23:41,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3535it [2:23:45,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3536it [2:23:47,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3537it [2:23:49,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3538it [2:23:51,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3539it [2:23:53,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3540it [2:23:55,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3541it [2:23:57,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3542it [2:24:01,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3543it [2:24:03,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3544it [2:24:05,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3545it [2:24:08,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3546it [2:24:10,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3547it [2:24:14,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3548it [2:24:16,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3549it [2:24:19,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3550it [2:24:21,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:09:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:09:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3551it [2:24:23,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3552it [2:24:26,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3553it [2:24:28,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3554it [2:24:31,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3555it [2:24:34,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3556it [2:24:39,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3557it [2:24:42,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3558it [2:24:44,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3559it [2:24:45,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3560it [2:24:47,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3561it [2:24:48,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3562it [2:24:51,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3563it [2:24:53,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3564it [2:24:54,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3565it [2:24:57,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3566it [2:24:59,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3567it [2:25:03,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3568it [2:25:05,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3569it [2:25:08,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3570it [2:25:10,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3571it [2:25:14,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3572it [2:25:15,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3573it [2:25:17,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3574it [2:25:19,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3575it [2:25:22,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:10:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:10:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3576it [2:25:24,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3577it [2:25:25,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3578it [2:25:27,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3579it [2:25:30,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3580it [2:25:32,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3581it [2:25:34,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3582it [2:25:36,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3583it [2:25:38,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3584it [2:25:40,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3585it [2:25:42,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3586it [2:25:44,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3587it [2:25:44,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3588it [2:25:47,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3589it [2:25:48,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3590it [2:25:51,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3591it [2:25:53,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3592it [2:25:54,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3593it [2:25:56,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3594it [2:25:58,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3595it [2:26:01,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3596it [2:26:03,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3597it [2:26:05,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3598it [2:26:07,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3599it [2:26:10,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3600it [2:26:12,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3601it [2:26:14,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3602it [2:26:16,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3603it [2:26:19,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3604it [2:26:21,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:11:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:11:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3605it [2:26:24,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3606it [2:26:26,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3607it [2:26:27,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3608it [2:26:32,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3609it [2:26:33,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3610it [2:26:36,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3611it [2:26:39,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3612it [2:26:42,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3613it [2:26:44,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3614it [2:26:47,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3615it [2:26:49,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3616it [2:26:51,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3617it [2:26:52,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3618it [2:26:55,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3619it [2:26:59,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3620it [2:27:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3621it [2:27:01,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3622it [2:27:04,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3623it [2:27:05,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3624it [2:27:07,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3625it [2:27:11,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3626it [2:27:14,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3627it [2:27:16,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3628it [2:27:18,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3629it [2:27:20,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:12:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:12:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3630it [2:27:24,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3631it [2:27:25,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3632it [2:27:27,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3633it [2:27:31,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3634it [2:27:35,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3635it [2:27:37,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3636it [2:27:39,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3637it [2:27:41,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3638it [2:27:43,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3639it [2:27:47,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3640it [2:27:48,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3641it [2:27:51,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3642it [2:27:55,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3643it [2:27:57,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3644it [2:28:00,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3645it [2:28:01,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3646it [2:28:04,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3647it [2:28:06,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3648it [2:28:09,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3649it [2:28:12,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3650it [2:28:14,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3651it [2:28:16,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3652it [2:28:19,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3653it [2:28:20,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3654it [2:28:22,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:13:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:13:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3655it [2:28:25,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3656it [2:28:26,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3657it [2:28:28,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3658it [2:28:31,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3659it [2:28:32,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3660it [2:28:34,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3661it [2:28:36,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3662it [2:28:39,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3663it [2:28:41,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3664it [2:28:43,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3665it [2:28:45,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3666it [2:28:48,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3667it [2:28:50,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3668it [2:28:52,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3669it [2:28:54,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3670it [2:28:57,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3671it [2:28:58,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3672it [2:29:02,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3673it [2:29:04,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3674it [2:29:06,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3675it [2:29:09,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3676it [2:29:10,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3677it [2:29:12,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3678it [2:29:15,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3679it [2:29:18,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3680it [2:29:20,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3681it [2:29:22,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:14:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:14:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3682it [2:29:24,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3683it [2:29:26,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3684it [2:29:28,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3685it [2:29:29,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3686it [2:29:31,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3687it [2:29:35,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3688it [2:29:36,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3689it [2:29:40,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3690it [2:29:41,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3691it [2:29:44,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3692it [2:29:46,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3693it [2:29:48,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3694it [2:29:51,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3695it [2:29:54,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3696it [2:29:57,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3697it [2:29:59,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3698it [2:30:02,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3699it [2:30:06,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3700it [2:30:09,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3701it [2:30:11,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3702it [2:30:13,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3703it [2:30:16,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3704it [2:30:18,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3705it [2:30:20,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3706it [2:30:22,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:15:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:15:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3707it [2:30:25,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3708it [2:30:27,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3709it [2:30:29,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3710it [2:30:32,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3711it [2:30:33,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3712it [2:30:35,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3713it [2:30:39,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3714it [2:30:43,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3715it [2:30:47,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3716it [2:30:48,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3717it [2:30:50,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3718it [2:30:53,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3719it [2:30:56,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3720it [2:30:58,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3721it [2:31:01,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3722it [2:31:04,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3723it [2:31:06,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3724it [2:31:09,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3725it [2:31:11,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3726it [2:31:15,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3727it [2:31:19,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3728it [2:31:20,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:16:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:16:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3729it [2:31:23,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3730it [2:31:27,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3731it [2:31:29,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3732it [2:31:31,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3733it [2:31:33,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3734it [2:31:37,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3735it [2:31:39,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3736it [2:31:42,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3737it [2:31:46,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3738it [2:31:49,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3739it [2:31:51,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3740it [2:31:54,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3741it [2:31:55,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3742it [2:31:59,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3743it [2:32:03,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3744it [2:32:05,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3745it [2:32:07,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3746it [2:32:09,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3747it [2:32:11,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3748it [2:32:15,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3749it [2:32:17,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3750it [2:32:20,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3751it [2:32:21,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:17:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:17:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3752it [2:32:24,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3753it [2:32:27,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3754it [2:32:30,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3755it [2:32:32,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3756it [2:32:34,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3757it [2:32:35,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3758it [2:32:37,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3759it [2:32:41,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3760it [2:32:43,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3761it [2:32:44,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3762it [2:32:46,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3763it [2:32:48,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3764it [2:32:52,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3765it [2:32:54,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3766it [2:32:55,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3767it [2:32:57,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3768it [2:32:59,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3769it [2:33:01,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3770it [2:33:04,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3771it [2:33:06,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3772it [2:33:08,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3773it [2:33:10,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3774it [2:33:12,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3775it [2:33:15,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3776it [2:33:17,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3777it [2:33:19,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3778it [2:33:21,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3779it [2:33:23,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:18:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:18:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3780it [2:33:27,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3781it [2:33:30,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3782it [2:33:34,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3783it [2:33:36,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3784it [2:33:38,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3785it [2:33:39,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3786it [2:33:44,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3787it [2:33:47,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3788it [2:33:49,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3789it [2:33:51,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3790it [2:33:53,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3791it [2:33:54,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3792it [2:33:56,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3793it [2:33:59,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3794it [2:34:02,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3795it [2:34:04,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3796it [2:34:07,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3797it [2:34:09,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3798it [2:34:12,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3799it [2:34:14,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3800it [2:34:17,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3801it [2:34:19,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3802it [2:34:21,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:19:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:19:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3803it [2:34:24,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3804it [2:34:27,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3805it [2:34:30,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3806it [2:34:32,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3807it [2:34:35,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3808it [2:34:36,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3809it [2:34:39,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3810it [2:34:43,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3811it [2:34:45,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3812it [2:34:48,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3813it [2:34:50,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3814it [2:34:51,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3815it [2:34:54,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3816it [2:34:58,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3817it [2:35:01,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3818it [2:35:03,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3819it [2:35:04,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3820it [2:35:06,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3821it [2:35:09,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3822it [2:35:11,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3823it [2:35:14,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3824it [2:35:16,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3825it [2:35:18,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3826it [2:35:21,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3827it [2:35:23,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:20:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:20:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3828it [2:35:27,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3829it [2:35:31,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3830it [2:35:33,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3831it [2:35:35,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3832it [2:35:38,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3833it [2:35:41,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3834it [2:35:42,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3835it [2:35:44,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3836it [2:35:47,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3837it [2:35:50,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3838it [2:35:52,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3839it [2:35:54,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3840it [2:35:56,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3841it [2:35:58,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3842it [2:36:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3843it [2:36:02,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3844it [2:36:04,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3845it [2:36:06,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3846it [2:36:09,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3847it [2:36:11,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3848it [2:36:13,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3849it [2:36:16,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3850it [2:36:18,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3851it [2:36:21,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3852it [2:36:23,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:21:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:21:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3853it [2:36:26,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3854it [2:36:28,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3855it [2:36:30,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3856it [2:36:32,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3857it [2:36:34,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3858it [2:36:35,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3859it [2:36:39,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3860it [2:36:41,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3861it [2:36:43,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3862it [2:36:46,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3863it [2:36:48,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3864it [2:36:49,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3865it [2:36:51,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3866it [2:36:54,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3867it [2:36:56,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3868it [2:37:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3869it [2:37:04,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3870it [2:37:06,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3871it [2:37:08,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3872it [2:37:10,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3873it [2:37:13,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3874it [2:37:15,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3875it [2:37:16,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3876it [2:37:18,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3877it [2:37:22,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:22:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:22:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3878it [2:37:25,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3879it [2:37:28,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3880it [2:37:30,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3881it [2:37:32,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3882it [2:37:35,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3883it [2:37:37,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3884it [2:37:41,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3885it [2:37:44,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3886it [2:37:48,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3887it [2:37:50,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3888it [2:37:53,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3889it [2:37:55,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3890it [2:37:58,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3891it [2:38:00,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3892it [2:38:02,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3893it [2:38:04,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3894it [2:38:06,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3895it [2:38:08,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3896it [2:38:11,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3897it [2:38:15,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3898it [2:38:18,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3899it [2:38:21,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:23:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:23:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3900it [2:38:23,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3901it [2:38:26,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3902it [2:38:29,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3903it [2:38:31,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3904it [2:38:34,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3905it [2:38:35,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3906it [2:38:37,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3907it [2:38:40,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3908it [2:38:42,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3909it [2:38:45,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3910it [2:38:48,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3911it [2:38:49,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3912it [2:38:51,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3913it [2:38:54,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3914it [2:38:55,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3915it [2:38:58,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3916it [2:39:01,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3917it [2:39:03,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3918it [2:39:06,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3919it [2:39:07,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3920it [2:39:10,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3921it [2:39:14,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3922it [2:39:16,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3923it [2:39:18,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3924it [2:39:21,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3925it [2:39:23,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:24:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:24:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3926it [2:39:26,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3927it [2:39:29,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3928it [2:39:30,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3929it [2:39:34,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3930it [2:39:36,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3931it [2:39:39,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3932it [2:39:42,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3933it [2:39:46,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3934it [2:39:50,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3935it [2:39:51,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3936it [2:39:54,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3937it [2:39:57,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3938it [2:40:00,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3939it [2:40:02,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3940it [2:40:04,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3941it [2:40:05,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3942it [2:40:08,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3943it [2:40:11,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3944it [2:40:13,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3945it [2:40:17,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3946it [2:40:20,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3947it [2:40:22,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:25:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:25:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3948it [2:40:23,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3949it [2:40:25,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3950it [2:40:27,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3951it [2:40:29,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3952it [2:40:32,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3953it [2:40:36,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3954it [2:40:38,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3955it [2:40:40,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3956it [2:40:43,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3957it [2:40:45,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3958it [2:40:48,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3959it [2:40:51,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3960it [2:40:53,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3961it [2:40:56,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3962it [2:40:58,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3963it [2:41:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3964it [2:41:02,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3965it [2:41:06,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3966it [2:41:08,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3967it [2:41:10,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3968it [2:41:12,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3969it [2:41:13,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3970it [2:41:15,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3971it [2:41:18,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3972it [2:41:22,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:26:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:26:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3973it [2:41:25,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3974it [2:41:28,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3975it [2:41:30,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3976it [2:41:33,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3977it [2:41:34,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3978it [2:41:37,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3979it [2:41:40,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3980it [2:41:43,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3981it [2:41:45,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3982it [2:41:47,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3983it [2:41:50,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3984it [2:41:53,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3985it [2:41:54,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3986it [2:41:57,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3987it [2:41:58,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3988it [2:42:01,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3989it [2:42:03,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3990it [2:42:05,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3991it [2:42:06,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3992it [2:42:10,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3993it [2:42:12,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3994it [2:42:16,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3995it [2:42:18,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3996it [2:42:21,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3997it [2:42:22,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:27:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:27:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3998it [2:42:24,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3999it [2:42:27,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [2:42:31,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [2:42:32,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4002it [2:42:35,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4003it [2:42:38,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4004it [2:42:41,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4005it [2:42:43,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4006it [2:42:45,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4007it [2:42:47,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4008it [2:42:49,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4009it [2:42:50,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4010it [2:42:52,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4011it [2:42:54,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4012it [2:42:57,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4013it [2:42:59,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4014it [2:43:01,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4015it [2:43:03,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4016it [2:43:07,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4017it [2:43:11,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4018it [2:43:15,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4019it [2:43:18,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4020it [2:43:21,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4021it [2:43:23,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:28:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:28:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4022it [2:43:25,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4023it [2:43:28,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4024it [2:43:30,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4025it [2:43:32,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4026it [2:43:33,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4027it [2:43:35,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4028it [2:43:38,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4029it [2:43:41,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4030it [2:43:45,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4031it [2:43:48,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4032it [2:43:50,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4033it [2:43:52,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4034it [2:43:54,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4035it [2:43:55,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4036it [2:43:57,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4037it [2:43:59,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4038it [2:44:02,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4039it [2:44:04,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4040it [2:44:07,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4041it [2:44:09,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4042it [2:44:11,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4043it [2:44:14,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4044it [2:44:19,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4045it [2:44:21,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:29:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:29:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4046it [2:44:24,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4047it [2:44:26,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4048it [2:44:28,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4049it [2:44:31,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4050it [2:44:33,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4051it [2:44:36,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4052it [2:44:37,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4053it [2:44:39,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4054it [2:44:41,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4055it [2:44:43,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4056it [2:44:45,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4057it [2:44:48,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4058it [2:44:51,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4059it [2:44:53,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4060it [2:44:57,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4061it [2:44:58,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4062it [2:45:00,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4063it [2:45:03,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4064it [2:45:04,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4065it [2:45:06,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4066it [2:45:08,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4067it [2:45:10,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4068it [2:45:12,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4069it [2:45:14,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4070it [2:45:16,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4071it [2:45:17,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4072it [2:45:19,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4073it [2:45:22,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:30:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:30:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4074it [2:45:24,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4075it [2:45:26,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4076it [2:45:29,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4077it [2:45:32,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4078it [2:45:34,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4079it [2:45:36,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4080it [2:45:39,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4081it [2:45:40,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4082it [2:45:42,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4083it [2:45:44,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4084it [2:45:47,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4085it [2:45:48,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4086it [2:45:53,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4087it [2:45:54,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4088it [2:45:58,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4089it [2:46:02,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4090it [2:46:03,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4091it [2:46:05,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4092it [2:46:07,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4093it [2:46:10,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4094it [2:46:12,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4095it [2:46:14,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4096it [2:46:16,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4097it [2:46:18,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4098it [2:46:20,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:31:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:31:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4099it [2:46:24,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4100it [2:46:26,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4101it [2:46:27,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4102it [2:46:30,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4103it [2:46:31,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4104it [2:46:33,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4105it [2:46:35,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4106it [2:46:36,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4107it [2:46:38,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4108it [2:46:40,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4109it [2:46:43,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4110it [2:46:47,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4111it [2:46:49,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4112it [2:46:50,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4113it [2:46:53,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4114it [2:46:55,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4115it [2:46:57,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4116it [2:47:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4117it [2:47:01,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4118it [2:47:04,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4119it [2:47:06,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4120it [2:47:09,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4121it [2:47:13,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4122it [2:47:16,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4123it [2:47:18,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4124it [2:47:20,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4125it [2:47:22,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:32:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:32:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4126it [2:47:24,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4127it [2:47:26,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4128it [2:47:28,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4129it [2:47:29,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4130it [2:47:34,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4131it [2:47:37,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4132it [2:47:39,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4133it [2:47:42,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4134it [2:47:43,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4135it [2:47:46,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4136it [2:47:48,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4137it [2:47:51,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4138it [2:47:55,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4139it [2:47:56,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4140it [2:48:00,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4141it [2:48:01,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4142it [2:48:05,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4143it [2:48:07,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4144it [2:48:11,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4145it [2:48:14,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4146it [2:48:17,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4147it [2:48:21,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:33:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:33:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4148it [2:48:23,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4149it [2:48:26,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4150it [2:48:28,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4151it [2:48:29,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4152it [2:48:32,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4153it [2:48:34,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4154it [2:48:37,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4155it [2:48:40,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4156it [2:48:42,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4157it [2:48:43,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4158it [2:48:45,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4159it [2:48:48,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4160it [2:48:51,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4161it [2:48:54,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4162it [2:48:57,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4163it [2:49:00,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4164it [2:49:03,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4165it [2:49:05,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4166it [2:49:09,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4167it [2:49:11,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4168it [2:49:15,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4169it [2:49:17,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4170it [2:49:19,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4171it [2:49:21,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:34:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:34:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4172it [2:49:25,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4173it [2:49:27,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4174it [2:49:29,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4175it [2:49:31,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4176it [2:49:33,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4177it [2:49:38,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4178it [2:49:41,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4179it [2:49:44,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4180it [2:49:45,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4181it [2:49:47,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4182it [2:49:49,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4183it [2:49:53,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4184it [2:49:56,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4185it [2:49:59,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4186it [2:50:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4187it [2:50:05,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4188it [2:50:06,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4189it [2:50:10,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4190it [2:50:11,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4191it [2:50:14,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4192it [2:50:17,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4193it [2:50:20,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4194it [2:50:22,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:35:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:35:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4195it [2:50:25,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4196it [2:50:27,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4197it [2:50:29,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4198it [2:50:33,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4199it [2:50:35,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4200it [2:50:38,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4201it [2:50:42,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4202it [2:50:44,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4203it [2:50:48,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4204it [2:50:50,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4205it [2:50:52,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4206it [2:50:54,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4207it [2:50:57,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4208it [2:50:59,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4209it [2:51:01,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4210it [2:51:03,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4211it [2:51:05,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4212it [2:51:07,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4213it [2:51:09,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4214it [2:51:12,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4215it [2:51:14,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4216it [2:51:16,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4217it [2:51:18,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4218it [2:51:22,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:36:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:36:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4219it [2:51:24,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4220it [2:51:27,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4221it [2:51:29,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4222it [2:51:30,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4223it [2:51:34,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4224it [2:51:36,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4225it [2:51:38,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4226it [2:51:41,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4227it [2:51:44,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4228it [2:51:46,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4229it [2:51:48,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4230it [2:51:49,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4231it [2:51:51,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4232it [2:51:52,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4233it [2:51:55,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4234it [2:51:59,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4235it [2:52:01,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4236it [2:52:03,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4237it [2:52:05,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4238it [2:52:07,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4239it [2:52:09,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4240it [2:52:11,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4241it [2:52:14,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4242it [2:52:17,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4243it [2:52:21,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4244it [2:52:22,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:37:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:37:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4245it [2:52:25,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4246it [2:52:28,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4247it [2:52:31,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4248it [2:52:33,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4249it [2:52:35,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4250it [2:52:39,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4251it [2:52:43,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4252it [2:52:45,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4253it [2:52:49,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4254it [2:52:51,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4255it [2:52:54,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4256it [2:52:56,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4257it [2:53:00,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4258it [2:53:01,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4259it [2:53:03,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4260it [2:53:06,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4261it [2:53:08,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4262it [2:53:10,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4263it [2:53:13,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4264it [2:53:14,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4265it [2:53:19,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4266it [2:53:22,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:38:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:38:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4267it [2:53:26,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4268it [2:53:28,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4269it [2:53:31,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4270it [2:53:34,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4271it [2:53:36,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4272it [2:53:40,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4273it [2:53:43,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4274it [2:53:44,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4275it [2:53:48,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4276it [2:53:51,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4277it [2:53:54,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4278it [2:53:55,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4279it [2:53:58,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4280it [2:54:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4281it [2:54:03,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4282it [2:54:06,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4283it [2:54:08,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4284it [2:54:12,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4285it [2:54:14,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4286it [2:54:18,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4287it [2:54:21,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:39:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:39:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4288it [2:54:24,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4289it [2:54:27,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4290it [2:54:30,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4291it [2:54:33,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4292it [2:54:34,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4293it [2:54:38,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4294it [2:54:42,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4295it [2:54:43,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4296it [2:54:45,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4297it [2:54:47,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4298it [2:54:49,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4299it [2:54:51,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4300it [2:54:53,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4301it [2:54:55,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4302it [2:54:57,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4303it [2:55:02,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4304it [2:55:05,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4305it [2:55:07,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4306it [2:55:09,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4307it [2:55:12,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4308it [2:55:14,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4309it [2:55:17,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4310it [2:55:19,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4311it [2:55:22,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:40:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:40:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4312it [2:55:26,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4313it [2:55:27,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4314it [2:55:29,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4315it [2:55:30,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4316it [2:55:32,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4317it [2:55:34,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4318it [2:55:39,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4319it [2:55:41,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4320it [2:55:44,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4321it [2:55:45,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4322it [2:55:48,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4323it [2:55:52,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4324it [2:55:53,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4325it [2:55:55,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4326it [2:55:57,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4327it [2:55:59,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4328it [2:56:01,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4329it [2:56:02,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4330it [2:56:07,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4331it [2:56:08,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4332it [2:56:10,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4333it [2:56:12,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4334it [2:56:15,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4335it [2:56:17,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4336it [2:56:21,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4337it [2:56:22,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:41:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:41:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4338it [2:56:24,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4339it [2:56:25,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4340it [2:56:27,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4341it [2:56:30,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4342it [2:56:33,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4343it [2:56:34,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4344it [2:56:36,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4345it [2:56:38,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4346it [2:56:40,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4347it [2:56:43,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4348it [2:56:45,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4349it [2:56:47,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4350it [2:56:52,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4351it [2:56:55,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4352it [2:56:58,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4353it [2:57:00,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4354it [2:57:02,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4355it [2:57:04,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4356it [2:57:05,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4357it [2:57:07,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4358it [2:57:10,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4359it [2:57:11,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4360it [2:57:13,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4361it [2:57:15,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4362it [2:57:18,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4363it [2:57:22,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:42:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:42:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4364it [2:57:25,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4365it [2:57:27,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4366it [2:57:29,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4367it [2:57:31,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4368it [2:57:34,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4369it [2:57:36,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4370it [2:57:40,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4371it [2:57:43,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4372it [2:57:46,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4373it [2:57:50,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4374it [2:57:51,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4375it [2:57:53,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4376it [2:57:55,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4377it [2:57:56,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4378it [2:57:58,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4379it [2:58:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4380it [2:58:01,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4381it [2:58:03,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4382it [2:58:04,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4383it [2:58:06,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4384it [2:58:08,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4385it [2:58:10,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4386it [2:58:13,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4387it [2:58:16,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4388it [2:58:19,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4389it [2:58:22,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:43:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:43:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4390it [2:58:24,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4391it [2:58:27,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4392it [2:58:29,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4393it [2:58:32,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4394it [2:58:34,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4395it [2:58:36,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4396it [2:58:39,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4397it [2:58:42,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4398it [2:58:43,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4399it [2:58:47,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4400it [2:58:50,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4401it [2:58:53,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4402it [2:58:56,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4403it [2:58:58,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4404it [2:59:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4405it [2:59:02,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4406it [2:59:04,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4407it [2:59:06,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4408it [2:59:08,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4409it [2:59:10,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4410it [2:59:13,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4411it [2:59:14,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4412it [2:59:16,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4413it [2:59:18,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4414it [2:59:20,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:44:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:44:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4415it [2:59:23,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4416it [2:59:25,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4417it [2:59:27,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4418it [2:59:30,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4419it [2:59:32,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4420it [2:59:33,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4421it [2:59:35,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4422it [2:59:38,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4423it [2:59:40,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4424it [2:59:41,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4425it [2:59:43,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4426it [2:59:45,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4427it [2:59:49,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4428it [2:59:52,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4429it [2:59:53,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4430it [2:59:55,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4431it [2:59:58,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4432it [3:00:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4433it [3:00:01,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4434it [3:00:03,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4435it [3:00:05,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4436it [3:00:08,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4437it [3:00:10,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4438it [3:00:12,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4439it [3:00:14,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4440it [3:00:16,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4441it [3:00:19,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4442it [3:00:21,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4443it [3:00:22,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:45:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:45:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4444it [3:00:24,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4445it [3:00:26,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4446it [3:00:27,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4447it [3:00:29,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4448it [3:00:31,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4449it [3:00:34,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4450it [3:00:36,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4451it [3:00:40,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4452it [3:00:42,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4453it [3:00:45,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4454it [3:00:47,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4455it [3:00:49,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4456it [3:00:51,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4457it [3:00:52,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4458it [3:00:55,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4459it [3:00:56,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4460it [3:00:59,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4461it [3:01:02,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4462it [3:01:04,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4463it [3:01:06,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4464it [3:01:10,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4465it [3:01:14,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4466it [3:01:16,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4467it [3:01:20,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:46:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:46:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4468it [3:01:23,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4469it [3:01:26,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4470it [3:01:30,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4471it [3:01:33,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4472it [3:01:35,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4473it [3:01:37,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4474it [3:01:39,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4475it [3:01:42,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4476it [3:01:44,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4477it [3:01:47,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4478it [3:01:51,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4479it [3:01:53,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4480it [3:01:55,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4481it [3:01:58,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4482it [3:02:00,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4483it [3:02:03,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4484it [3:02:06,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4485it [3:02:08,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4486it [3:02:11,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4487it [3:02:13,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4488it [3:02:15,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4489it [3:02:17,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4490it [3:02:18,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4491it [3:02:20,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4492it [3:02:22,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:47:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:47:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4493it [3:02:24,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4494it [3:02:25,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4495it [3:02:27,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4496it [3:02:30,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4497it [3:02:32,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4498it [3:02:36,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4499it [3:02:38,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4500it [3:02:39,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4501it [3:02:42,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4502it [3:02:46,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4503it [3:02:49,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4504it [3:02:53,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4505it [3:02:55,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4506it [3:02:57,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4507it [3:02:59,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4508it [3:03:01,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4509it [3:03:03,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4510it [3:03:04,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4511it [3:03:06,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4512it [3:03:09,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4513it [3:03:12,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4514it [3:03:14,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4515it [3:03:15,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4516it [3:03:17,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4517it [3:03:19,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4518it [3:03:21,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:48:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:48:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4519it [3:03:23,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4520it [3:03:26,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4521it [3:03:30,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4522it [3:03:33,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4523it [3:03:35,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4524it [3:03:37,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4525it [3:03:39,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4526it [3:03:41,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4527it [3:03:43,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4528it [3:03:45,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4529it [3:03:47,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4530it [3:03:50,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4531it [3:03:52,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4532it [3:03:55,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4533it [3:03:58,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4534it [3:04:01,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4535it [3:04:02,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4536it [3:04:04,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4537it [3:04:07,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4538it [3:04:09,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4539it [3:04:12,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4540it [3:04:13,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4541it [3:04:15,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4542it [3:04:17,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4543it [3:04:20,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4544it [3:04:22,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:49:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:49:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4545it [3:04:26,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4546it [3:04:29,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4547it [3:04:31,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4548it [3:04:34,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4549it [3:04:36,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4550it [3:04:40,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4551it [3:04:41,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4552it [3:04:43,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4553it [3:04:46,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4554it [3:04:50,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4555it [3:04:53,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4556it [3:04:56,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4557it [3:04:58,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4558it [3:05:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4559it [3:05:03,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4560it [3:05:05,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4561it [3:05:06,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4562it [3:05:09,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4563it [3:05:11,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4564it [3:05:15,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4565it [3:05:17,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4566it [3:05:19,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4567it [3:05:21,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:50:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:50:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4568it [3:05:23,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4569it [3:05:25,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4570it [3:05:27,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4571it [3:05:30,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4572it [3:05:33,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4573it [3:05:35,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4574it [3:05:37,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4575it [3:05:40,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4576it [3:05:44,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4577it [3:05:46,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4578it [3:05:48,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4579it [3:05:50,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4580it [3:05:52,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4581it [3:05:54,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4582it [3:05:56,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4583it [3:05:59,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4584it [3:06:01,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4585it [3:06:03,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4586it [3:06:05,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4587it [3:06:08,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4588it [3:06:12,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4589it [3:06:16,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4590it [3:06:20,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4591it [3:06:22,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:51:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:51:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4592it [3:06:28,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4593it [3:06:33,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4594it [3:06:35,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4595it [3:06:38,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4596it [3:06:40,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4597it [3:06:42,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4598it [3:06:46,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4599it [3:06:48,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4600it [3:06:50,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4601it [3:06:52,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4602it [3:06:54,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4603it [3:06:57,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4604it [3:06:59,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4605it [3:07:02,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4606it [3:07:04,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4607it [3:07:07,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4608it [3:07:11,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4609it [3:07:13,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4610it [3:07:15,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4611it [3:07:17,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4612it [3:07:19,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4613it [3:07:22,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:52:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:52:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4614it [3:07:26,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4615it [3:07:28,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4616it [3:07:31,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4617it [3:07:32,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4618it [3:07:34,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4619it [3:07:36,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4620it [3:07:38,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4621it [3:07:39,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4622it [3:07:42,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4623it [3:07:44,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4624it [3:07:45,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4625it [3:07:47,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4626it [3:07:49,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4627it [3:07:51,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4628it [3:07:52,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4629it [3:07:54,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4630it [3:07:56,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4631it [3:08:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4632it [3:08:01,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4633it [3:08:04,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4634it [3:08:07,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4635it [3:08:10,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4636it [3:08:12,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4637it [3:08:13,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4638it [3:08:17,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4639it [3:08:19,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4640it [3:08:21,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:53:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:53:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4641it [3:08:23,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4642it [3:08:26,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4643it [3:08:30,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4644it [3:08:34,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4645it [3:08:36,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4646it [3:08:37,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4647it [3:08:40,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4648it [3:08:43,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4649it [3:08:45,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4650it [3:08:47,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4651it [3:08:49,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4652it [3:08:50,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4653it [3:08:53,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4654it [3:08:56,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4655it [3:08:58,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4656it [3:09:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4657it [3:09:03,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4658it [3:09:04,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4659it [3:09:08,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4660it [3:09:12,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4661it [3:09:15,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4662it [3:09:18,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4663it [3:09:21,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4664it [3:09:22,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:54:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:54:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4665it [3:09:24,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4666it [3:09:26,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4667it [3:09:30,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4668it [3:09:34,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4669it [3:09:37,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4670it [3:09:40,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4671it [3:09:41,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4672it [3:09:46,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4673it [3:09:48,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4674it [3:09:49,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4675it [3:09:51,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4676it [3:09:54,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4677it [3:09:56,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4678it [3:10:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4679it [3:10:02,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4680it [3:10:04,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4681it [3:10:05,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4682it [3:10:07,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4683it [3:10:08,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4684it [3:10:12,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4685it [3:10:14,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4686it [3:10:16,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4687it [3:10:18,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4688it [3:10:20,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4689it [3:10:22,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:55:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:55:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4690it [3:10:24,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4691it [3:10:26,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4692it [3:10:28,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4693it [3:10:31,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4694it [3:10:33,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4695it [3:10:34,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4696it [3:10:36,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4697it [3:10:38,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4698it [3:10:40,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4699it [3:10:42,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4700it [3:10:43,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4701it [3:10:45,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4702it [3:10:49,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4703it [3:10:52,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4704it [3:10:54,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4705it [3:10:56,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4706it [3:10:58,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4707it [3:11:01,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4708it [3:11:02,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4709it [3:11:03,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4710it [3:11:05,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4711it [3:11:07,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4712it [3:11:11,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4713it [3:11:13,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4714it [3:11:16,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4715it [3:11:19,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4716it [3:11:21,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4717it [3:11:22,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:56:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:56:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4718it [3:11:25,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4719it [3:11:27,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4720it [3:11:30,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4721it [3:11:33,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4722it [3:11:36,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4723it [3:11:40,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4724it [3:11:44,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4725it [3:11:46,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4726it [3:11:50,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4727it [3:11:52,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4728it [3:11:53,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4729it [3:11:56,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4730it [3:11:58,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4731it [3:12:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4732it [3:12:03,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4733it [3:12:05,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4734it [3:12:08,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4735it [3:12:11,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4736it [3:12:14,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4737it [3:12:17,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4738it [3:12:19,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4739it [3:12:22,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:57:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:57:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4740it [3:12:24,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4741it [3:12:26,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4742it [3:12:29,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4743it [3:12:32,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4744it [3:12:35,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4745it [3:12:37,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4746it [3:12:39,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4747it [3:12:41,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4748it [3:12:42,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4749it [3:12:44,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4750it [3:12:47,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4751it [3:12:48,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4752it [3:12:50,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4753it [3:12:52,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4754it [3:12:55,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4755it [3:12:58,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4756it [3:13:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4757it [3:13:03,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4758it [3:13:05,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4759it [3:13:07,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4760it [3:13:11,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4761it [3:13:12,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4762it [3:13:13,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4763it [3:13:15,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4764it [3:13:17,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4765it [3:13:19,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4766it [3:13:20,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4767it [3:13:22,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:58:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:58:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4768it [3:13:24,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4769it [3:13:26,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4770it [3:13:28,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4771it [3:13:31,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4772it [3:13:32,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4773it [3:13:34,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4774it [3:13:38,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [3:13:42,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4776it [3:13:43,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4777it [3:13:45,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4778it [3:13:47,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4779it [3:13:50,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4780it [3:13:53,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4781it [3:13:54,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4782it [3:13:56,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4783it [3:13:57,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4784it [3:13:58,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4785it [3:14:01,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4786it [3:14:04,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4787it [3:14:08,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4788it [3:14:10,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4789it [3:14:12,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4790it [3:14:14,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4791it [3:14:15,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4792it [3:14:17,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4793it [3:14:19,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4794it [3:14:21,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 08:59:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 08:59:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4795it [3:14:23,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4796it [3:14:25,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4797it [3:14:26,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4798it [3:14:28,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4799it [3:14:29,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4800it [3:14:31,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4801it [3:14:32,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4802it [3:14:35,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4803it [3:14:37,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4804it [3:14:39,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4805it [3:14:43,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4806it [3:14:45,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4807it [3:14:47,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4808it [3:14:49,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4809it [3:14:52,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4810it [3:14:54,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4811it [3:14:56,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4812it [3:14:59,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4813it [3:15:01,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4814it [3:15:03,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4815it [3:15:06,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4816it [3:15:08,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4817it [3:15:10,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4818it [3:15:11,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4819it [3:15:13,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4820it [3:15:15,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4821it [3:15:18,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4822it [3:15:19,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4823it [3:15:22,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:00:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:00:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4824it [3:15:24,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4825it [3:15:27,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4826it [3:15:29,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4827it [3:15:32,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4828it [3:15:34,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4829it [3:15:35,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4830it [3:15:39,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4831it [3:15:40,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4832it [3:15:43,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4833it [3:15:47,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4834it [3:15:50,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4835it [3:15:53,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4836it [3:15:55,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4837it [3:15:58,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4838it [3:16:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4839it [3:16:02,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4840it [3:16:04,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4841it [3:16:07,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4842it [3:16:11,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4843it [3:16:14,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4844it [3:16:17,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4845it [3:16:20,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:01:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:01:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4846it [3:16:24,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4847it [3:16:27,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4848it [3:16:30,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4849it [3:16:33,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4850it [3:16:36,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4851it [3:16:39,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4852it [3:16:41,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4853it [3:16:43,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4854it [3:16:45,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4855it [3:16:48,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4856it [3:16:51,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4857it [3:16:54,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4858it [3:16:57,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4859it [3:17:00,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4860it [3:17:02,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4861it [3:17:04,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4862it [3:17:05,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4863it [3:17:08,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4864it [3:17:10,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4865it [3:17:12,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4866it [3:17:14,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4867it [3:17:17,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4868it [3:17:20,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4869it [3:17:23,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:02:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:02:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4870it [3:17:25,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4871it [3:17:28,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4872it [3:17:29,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4873it [3:17:31,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4874it [3:17:35,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4875it [3:17:37,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4876it [3:17:41,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4877it [3:17:43,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4878it [3:17:46,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4879it [3:17:48,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4880it [3:17:51,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4881it [3:17:52,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4882it [3:17:54,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4883it [3:17:58,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4884it [3:18:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4885it [3:18:02,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4886it [3:18:04,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4887it [3:18:07,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4888it [3:18:10,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4889it [3:18:12,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4890it [3:18:16,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4891it [3:18:18,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4892it [3:18:19,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:03:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:03:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4893it [3:18:23,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4894it [3:18:27,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4895it [3:18:28,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4896it [3:18:30,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4897it [3:18:31,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4898it [3:18:35,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4899it [3:18:38,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4900it [3:18:40,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4901it [3:18:42,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4902it [3:18:45,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4903it [3:18:47,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4904it [3:18:50,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4905it [3:18:51,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4906it [3:18:54,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4907it [3:18:56,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4908it [3:18:58,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4909it [3:19:01,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4910it [3:19:04,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4911it [3:19:06,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4912it [3:19:08,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4913it [3:19:11,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4914it [3:19:13,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4915it [3:19:14,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4916it [3:19:16,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4917it [3:19:18,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4918it [3:19:20,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4919it [3:19:22,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:04:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:04:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4920it [3:19:24,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4921it [3:19:27,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4922it [3:19:28,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4923it [3:19:30,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4924it [3:19:34,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4925it [3:19:36,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4926it [3:19:39,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4927it [3:19:40,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4928it [3:19:42,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4929it [3:19:45,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4930it [3:19:47,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4931it [3:19:50,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4932it [3:19:53,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4933it [3:19:54,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4934it [3:19:56,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4935it [3:20:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4936it [3:20:03,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4937it [3:20:05,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4938it [3:20:09,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4939it [3:20:11,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4940it [3:20:12,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4941it [3:20:16,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4942it [3:20:17,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4943it [3:20:21,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:05:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:05:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4944it [3:20:24,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4945it [3:20:26,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4946it [3:20:28,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4947it [3:20:30,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4948it [3:20:32,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4949it [3:20:37,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4950it [3:20:40,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4951it [3:20:42,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4952it [3:20:44,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4953it [3:20:47,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4954it [3:20:50,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4955it [3:20:52,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4956it [3:20:55,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4957it [3:20:56,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4958it [3:20:58,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4959it [3:21:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4960it [3:21:03,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4961it [3:21:06,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4962it [3:21:09,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4963it [3:21:11,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4964it [3:21:13,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4965it [3:21:15,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4966it [3:21:19,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4967it [3:21:21,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:06:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:06:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4968it [3:21:24,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4969it [3:21:26,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4970it [3:21:28,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4971it [3:21:30,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4972it [3:21:32,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4973it [3:21:35,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4974it [3:21:36,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4975it [3:21:38,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4976it [3:21:40,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4977it [3:21:44,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4978it [3:21:46,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4979it [3:21:47,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4980it [3:21:49,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4981it [3:21:51,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4982it [3:21:54,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4983it [3:21:56,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4984it [3:22:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4985it [3:22:02,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4986it [3:22:03,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4987it [3:22:06,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4988it [3:22:07,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4989it [3:22:10,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4990it [3:22:13,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4991it [3:22:18,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4992it [3:22:20,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:07:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:07:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4993it [3:22:24,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4994it [3:22:25,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4995it [3:22:27,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4996it [3:22:29,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4997it [3:22:31,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4998it [3:22:34,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4999it [3:22:36,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [3:22:38,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5001it [3:22:41,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5002it [3:22:45,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5003it [3:22:49,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5004it [3:22:52,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5005it [3:22:54,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5006it [3:22:56,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5007it [3:22:59,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5008it [3:23:01,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5009it [3:23:03,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5010it [3:23:05,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5011it [3:23:07,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5012it [3:23:09,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5013it [3:23:11,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5014it [3:23:14,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5015it [3:23:15,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5016it [3:23:17,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5017it [3:23:20,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5018it [3:23:22,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:08:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:08:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5019it [3:23:26,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5020it [3:23:28,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5021it [3:23:30,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5022it [3:23:33,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5023it [3:23:35,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5024it [3:23:36,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5025it [3:23:39,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5026it [3:23:42,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5027it [3:23:44,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5028it [3:23:47,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5029it [3:23:48,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5030it [3:23:51,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5031it [3:23:52,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5032it [3:23:54,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5033it [3:23:56,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5034it [3:24:00,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5035it [3:24:03,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5036it [3:24:04,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5037it [3:24:08,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5038it [3:24:10,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5039it [3:24:12,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5040it [3:24:16,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5041it [3:24:18,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5042it [3:24:20,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:09:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:09:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5043it [3:24:23,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5044it [3:24:25,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5045it [3:24:27,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5046it [3:24:29,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5047it [3:24:31,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5048it [3:24:33,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5049it [3:24:34,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5050it [3:24:36,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5051it [3:24:40,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5052it [3:24:42,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5053it [3:24:45,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5054it [3:24:46,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5055it [3:24:49,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5056it [3:24:51,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5057it [3:24:52,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5058it [3:24:55,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5059it [3:24:57,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5060it [3:24:59,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5061it [3:25:01,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5062it [3:25:04,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5063it [3:25:08,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5064it [3:25:10,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5065it [3:25:13,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5066it [3:25:14,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5067it [3:25:17,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5068it [3:25:20,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:10:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:10:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5069it [3:25:24,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5070it [3:25:26,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5071it [3:25:27,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5072it [3:25:29,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5073it [3:25:31,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5074it [3:25:33,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5075it [3:25:35,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5076it [3:25:37,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5077it [3:25:38,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5078it [3:25:43,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5079it [3:25:46,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5080it [3:25:50,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5081it [3:25:53,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5082it [3:25:55,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5083it [3:25:57,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5084it [3:26:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5085it [3:26:03,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5086it [3:26:05,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5087it [3:26:07,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5088it [3:26:09,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5089it [3:26:13,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5090it [3:26:15,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5091it [3:26:16,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5092it [3:26:19,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5093it [3:26:21,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:11:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:11:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5094it [3:26:23,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5095it [3:26:25,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5096it [3:26:28,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5097it [3:26:32,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5098it [3:26:35,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5099it [3:26:38,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5100it [3:26:40,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5101it [3:26:43,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5102it [3:26:44,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5103it [3:26:48,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5104it [3:26:51,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5105it [3:26:53,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5106it [3:26:54,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5107it [3:26:56,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5108it [3:26:58,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5109it [3:27:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5110it [3:27:03,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5111it [3:27:05,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5112it [3:27:07,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5113it [3:27:11,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5114it [3:27:14,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5115it [3:27:16,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5116it [3:27:17,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5117it [3:27:21,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5118it [3:27:22,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:12:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:12:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5119it [3:27:24,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [3:27:26,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5121it [3:27:30,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5122it [3:27:32,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5123it [3:27:34,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5124it [3:27:36,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5125it [3:27:38,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5126it [3:27:40,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5127it [3:27:42,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5128it [3:27:44,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5129it [3:27:45,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5130it [3:27:47,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5131it [3:27:49,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5132it [3:27:52,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5133it [3:27:54,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5134it [3:27:57,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5135it [3:27:58,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5136it [3:28:02,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5137it [3:28:06,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5138it [3:28:10,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5139it [3:28:12,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5140it [3:28:13,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5141it [3:28:15,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5142it [3:28:20,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5143it [3:28:21,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:13:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:13:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5144it [3:28:23,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5145it [3:28:26,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5146it [3:28:28,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5147it [3:28:29,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5148it [3:28:33,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5149it [3:28:38,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5150it [3:28:39,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5151it [3:28:42,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5152it [3:28:44,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5153it [3:28:48,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5154it [3:28:49,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5155it [3:28:51,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5156it [3:28:54,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5157it [3:28:57,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5158it [3:28:58,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5159it [3:29:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5160it [3:29:02,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5161it [3:29:04,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5162it [3:29:06,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5163it [3:29:08,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5164it [3:29:09,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5165it [3:29:12,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5166it [3:29:14,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5167it [3:29:16,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5168it [3:29:18,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5169it [3:29:21,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5170it [3:29:23,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:14:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:14:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5171it [3:29:24,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5172it [3:29:27,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5173it [3:29:30,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5174it [3:29:31,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5175it [3:29:34,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5176it [3:29:35,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5177it [3:29:37,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5178it [3:29:39,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5179it [3:29:41,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5180it [3:29:42,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5181it [3:29:44,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5182it [3:29:47,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5183it [3:29:49,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5184it [3:29:52,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5185it [3:29:56,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5186it [3:30:00,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5187it [3:30:03,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5188it [3:30:06,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5189it [3:30:06,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5190it [3:30:09,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5191it [3:30:11,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5192it [3:30:13,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5193it [3:30:15,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5194it [3:30:17,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5195it [3:30:21,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:15:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:15:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5196it [3:30:23,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5197it [3:30:25,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5198it [3:30:26,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5199it [3:30:29,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5200it [3:30:31,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5201it [3:30:35,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5202it [3:30:37,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5203it [3:30:38,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5204it [3:30:41,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5205it [3:30:44,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5206it [3:30:48,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5207it [3:30:50,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5208it [3:30:51,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5209it [3:30:54,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5210it [3:30:55,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5211it [3:30:58,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5212it [3:31:02,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5213it [3:31:04,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5214it [3:31:05,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5215it [3:31:08,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5216it [3:31:09,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5217it [3:31:13,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5218it [3:31:19,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5219it [3:31:20,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5220it [3:31:22,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:16:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:16:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5221it [3:31:25,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5222it [3:31:29,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5223it [3:31:31,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5224it [3:31:35,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5225it [3:31:37,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5226it [3:31:40,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5227it [3:31:42,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5228it [3:31:44,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5229it [3:31:47,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5230it [3:31:50,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5231it [3:31:52,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5232it [3:31:53,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5233it [3:31:57,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5234it [3:32:00,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5235it [3:32:02,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5236it [3:32:05,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5237it [3:32:07,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5238it [3:32:09,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5239it [3:32:11,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5240it [3:32:14,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5241it [3:32:16,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5242it [3:32:18,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5243it [3:32:21,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:17:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:17:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5244it [3:32:23,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5245it [3:32:25,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5246it [3:32:29,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5247it [3:32:32,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5248it [3:32:35,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5249it [3:32:36,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5250it [3:32:38,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5251it [3:32:41,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5252it [3:32:45,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5253it [3:32:48,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5254it [3:32:51,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5255it [3:32:53,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5256it [3:32:55,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5257it [3:32:56,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5258it [3:32:58,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5259it [3:33:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5260it [3:33:02,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5261it [3:33:04,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5262it [3:33:07,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5263it [3:33:09,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5264it [3:33:12,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5265it [3:33:14,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5266it [3:33:17,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5267it [3:33:19,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5268it [3:33:21,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:18:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:18:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5269it [3:33:23,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5270it [3:33:26,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5271it [3:33:28,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5272it [3:33:29,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5273it [3:33:31,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5274it [3:33:33,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5275it [3:33:35,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5276it [3:33:37,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5277it [3:33:40,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5278it [3:33:43,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5279it [3:33:45,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5280it [3:33:47,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5281it [3:33:48,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5282it [3:33:52,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5283it [3:33:55,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5284it [3:33:58,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5285it [3:34:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5286it [3:34:01,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5287it [3:34:04,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5288it [3:34:06,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5289it [3:34:09,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5290it [3:34:11,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5291it [3:34:14,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5292it [3:34:17,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5293it [3:34:19,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5294it [3:34:21,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:19:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:19:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5295it [3:34:23,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5296it [3:34:25,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5297it [3:34:27,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5298it [3:34:30,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5299it [3:34:32,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5300it [3:34:35,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5301it [3:34:38,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5302it [3:34:42,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5303it [3:34:44,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5304it [3:34:46,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5305it [3:34:48,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5306it [3:34:50,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5307it [3:34:52,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5308it [3:34:55,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5309it [3:34:57,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5310it [3:35:00,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5311it [3:35:01,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5312it [3:35:03,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5313it [3:35:05,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5314it [3:35:07,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5315it [3:35:09,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5316it [3:35:11,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5317it [3:35:15,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5318it [3:35:17,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5319it [3:35:19,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5320it [3:35:20,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5321it [3:35:23,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:20:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:20:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5322it [3:35:26,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5323it [3:35:29,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5324it [3:35:30,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5325it [3:35:32,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5326it [3:35:34,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5327it [3:35:35,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5328it [3:35:38,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5329it [3:35:40,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5330it [3:35:43,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5331it [3:35:45,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5332it [3:35:47,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5333it [3:35:50,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5334it [3:35:56,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5335it [3:35:57,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5336it [3:35:59,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5337it [3:36:01,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5338it [3:36:03,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5339it [3:36:07,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5340it [3:36:11,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5341it [3:36:12,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5342it [3:36:15,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5343it [3:36:18,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5344it [3:36:19,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5345it [3:36:23,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:21:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:21:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5346it [3:36:25,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5347it [3:36:28,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5348it [3:36:31,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5349it [3:36:33,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5350it [3:36:34,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5351it [3:36:36,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5352it [3:36:39,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5353it [3:36:42,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5354it [3:36:44,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5355it [3:36:46,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5356it [3:36:47,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5357it [3:36:50,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5358it [3:36:52,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5359it [3:36:56,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5360it [3:36:58,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5361it [3:37:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5362it [3:37:02,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5363it [3:37:05,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5364it [3:37:06,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5365it [3:37:09,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5366it [3:37:11,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5367it [3:37:13,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5368it [3:37:18,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5369it [3:37:19,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:22:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:22:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5370it [3:37:23,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5371it [3:37:26,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5372it [3:37:29,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5373it [3:37:31,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5374it [3:37:32,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5375it [3:37:34,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5376it [3:37:36,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5377it [3:37:38,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5378it [3:37:40,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5379it [3:37:42,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5380it [3:37:44,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5381it [3:37:47,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5382it [3:37:48,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5383it [3:37:51,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5384it [3:37:53,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5385it [3:37:56,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5386it [3:38:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5387it [3:38:01,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5388it [3:38:04,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5389it [3:38:06,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5390it [3:38:09,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5391it [3:38:11,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5392it [3:38:13,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5393it [3:38:15,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5394it [3:38:19,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5395it [3:38:22,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:23:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:23:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5396it [3:38:24,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5397it [3:38:25,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5398it [3:38:28,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5399it [3:38:30,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5400it [3:38:33,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5401it [3:38:36,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5402it [3:38:38,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5403it [3:38:40,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5404it [3:38:42,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5405it [3:38:43,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5406it [3:38:46,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5407it [3:38:48,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5408it [3:38:51,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5409it [3:38:53,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5410it [3:38:55,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5411it [3:38:59,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5412it [3:39:03,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5413it [3:39:08,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5414it [3:39:12,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5415it [3:39:13,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5416it [3:39:16,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5417it [3:39:19,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5418it [3:39:21,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5419it [3:39:22,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:24:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:24:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5420it [3:39:24,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5421it [3:39:26,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5422it [3:39:29,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5423it [3:39:31,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5424it [3:39:33,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5425it [3:39:35,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5426it [3:39:38,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5427it [3:39:40,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5428it [3:39:43,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5429it [3:39:46,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5430it [3:39:47,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5431it [3:39:50,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5432it [3:39:51,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5433it [3:39:55,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5434it [3:39:58,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5435it [3:40:01,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5436it [3:40:03,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5437it [3:40:05,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5438it [3:40:07,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5439it [3:40:09,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5440it [3:40:11,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5441it [3:40:15,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5442it [3:40:17,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5443it [3:40:21,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:25:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:25:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444it [3:40:24,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5445it [3:40:27,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5446it [3:40:31,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5447it [3:40:33,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5448it [3:40:36,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5449it [3:40:39,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5450it [3:40:41,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5451it [3:40:43,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5452it [3:40:46,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5453it [3:40:48,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5454it [3:40:50,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5455it [3:40:51,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5456it [3:40:54,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5457it [3:40:57,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5458it [3:40:59,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5459it [3:41:01,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5460it [3:41:03,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5461it [3:41:05,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5462it [3:41:07,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5463it [3:41:09,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5464it [3:41:10,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5465it [3:41:14,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5466it [3:41:16,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5467it [3:41:20,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5468it [3:41:22,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:26:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:26:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5469it [3:41:25,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5470it [3:41:27,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5471it [3:41:28,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5472it [3:41:30,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5473it [3:41:32,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5474it [3:41:35,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5475it [3:41:39,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5476it [3:41:40,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5477it [3:41:41,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5478it [3:41:43,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5479it [3:41:45,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5480it [3:41:48,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5481it [3:41:51,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5482it [3:41:53,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5483it [3:41:56,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5484it [3:41:58,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5485it [3:42:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5486it [3:42:03,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5487it [3:42:05,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5488it [3:42:08,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5489it [3:42:10,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5490it [3:42:12,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5491it [3:42:15,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5492it [3:42:18,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5493it [3:42:20,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5494it [3:42:23,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:27:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:27:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5495it [3:42:27,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5496it [3:42:28,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5497it [3:42:30,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5498it [3:42:34,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5499it [3:42:35,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5500it [3:42:38,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5501it [3:42:40,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5502it [3:42:43,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5503it [3:42:46,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5504it [3:42:48,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5505it [3:42:50,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5506it [3:42:52,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5507it [3:42:55,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5508it [3:42:56,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5509it [3:43:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5510it [3:43:01,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5511it [3:43:04,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5512it [3:43:06,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5513it [3:43:10,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5514it [3:43:14,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5515it [3:43:16,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5516it [3:43:19,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5517it [3:43:21,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:28:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:28:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5518it [3:43:24,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5519it [3:43:28,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5520it [3:43:31,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5521it [3:43:34,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5522it [3:43:36,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5523it [3:43:40,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5524it [3:43:43,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5525it [3:43:46,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5526it [3:43:48,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5527it [3:43:50,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5528it [3:43:51,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5529it [3:43:53,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5530it [3:43:55,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5531it [3:43:57,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5532it [3:44:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5533it [3:44:02,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5534it [3:44:06,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5535it [3:44:10,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5536it [3:44:13,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5537it [3:44:14,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5538it [3:44:17,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5539it [3:44:20,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5540it [3:44:22,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:29:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:29:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5541it [3:44:24,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5542it [3:44:26,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5543it [3:44:28,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5544it [3:44:31,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5545it [3:44:33,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5546it [3:44:36,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5547it [3:44:38,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5548it [3:44:41,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5549it [3:44:42,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5550it [3:44:46,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5551it [3:44:49,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5552it [3:44:51,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5553it [3:44:54,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5554it [3:44:57,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5555it [3:44:59,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5556it [3:45:02,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5557it [3:45:06,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5558it [3:45:08,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5559it [3:45:13,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5560it [3:45:18,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5561it [3:45:21,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:30:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:30:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5562it [3:45:24,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5563it [3:45:27,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5564it [3:45:30,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5565it [3:45:33,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5566it [3:45:35,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5567it [3:45:38,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5568it [3:45:40,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5569it [3:45:43,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5570it [3:45:44,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5571it [3:45:47,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5572it [3:45:49,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5573it [3:45:53,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5574it [3:45:55,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5575it [3:45:56,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5576it [3:45:58,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5577it [3:46:01,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5578it [3:46:04,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5579it [3:46:07,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5580it [3:46:09,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5581it [3:46:11,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5582it [3:46:13,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5583it [3:46:14,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5584it [3:46:16,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5585it [3:46:19,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5586it [3:46:21,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:31:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:31:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5587it [3:46:24,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5588it [3:46:26,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5589it [3:46:28,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5590it [3:46:33,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5591it [3:46:35,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5592it [3:46:37,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5593it [3:46:39,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5594it [3:46:41,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5595it [3:46:43,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5596it [3:46:45,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5597it [3:46:48,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5598it [3:46:51,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5599it [3:46:53,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5600it [3:46:54,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5601it [3:46:56,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5602it [3:46:58,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5603it [3:47:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5604it [3:47:02,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5605it [3:47:04,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5606it [3:47:07,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5607it [3:47:10,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5608it [3:47:13,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5609it [3:47:15,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5610it [3:47:17,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5611it [3:47:19,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5612it [3:47:23,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:32:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:32:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5613it [3:47:24,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5614it [3:47:26,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5615it [3:47:28,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5616it [3:47:30,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5617it [3:47:33,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5618it [3:47:34,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5619it [3:47:37,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5620it [3:47:41,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5621it [3:47:44,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5622it [3:47:47,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5623it [3:47:49,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5624it [3:47:51,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5625it [3:47:53,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5626it [3:47:57,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5627it [3:47:59,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5628it [3:48:01,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5629it [3:48:03,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5630it [3:48:04,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5631it [3:48:07,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5632it [3:48:08,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5633it [3:48:10,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5634it [3:48:12,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5635it [3:48:15,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5636it [3:48:17,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5637it [3:48:20,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:33:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:33:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5638it [3:48:24,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5639it [3:48:26,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5640it [3:48:28,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5641it [3:48:30,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5642it [3:48:32,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5643it [3:48:35,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5644it [3:48:37,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5645it [3:48:38,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5646it [3:48:40,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5647it [3:48:42,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5648it [3:48:45,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5649it [3:48:47,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5650it [3:48:49,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5651it [3:48:51,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5652it [3:48:53,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5653it [3:48:54,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5654it [3:48:56,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5655it [3:48:58,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5656it [3:49:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5657it [3:49:02,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5658it [3:49:04,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5659it [3:49:06,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5660it [3:49:08,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5661it [3:49:12,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5662it [3:49:14,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5663it [3:49:16,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5664it [3:49:18,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5665it [3:49:21,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:34:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:34:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5666it [3:49:24,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5667it [3:49:26,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5668it [3:49:28,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5669it [3:49:30,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5670it [3:49:33,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5671it [3:49:37,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5672it [3:49:40,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5673it [3:49:42,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5674it [3:49:43,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5675it [3:49:46,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5676it [3:49:48,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5677it [3:49:52,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5678it [3:49:55,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5679it [3:49:57,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5680it [3:50:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5681it [3:50:02,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5682it [3:50:04,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5683it [3:50:05,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5684it [3:50:08,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5685it [3:50:10,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5686it [3:50:12,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5687it [3:50:14,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5688it [3:50:16,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5689it [3:50:20,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5690it [3:50:22,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:35:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:35:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5691it [3:50:23,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5692it [3:50:24,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5693it [3:50:26,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5694it [3:50:30,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5695it [3:50:32,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5696it [3:50:35,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5697it [3:50:37,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5698it [3:50:41,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5699it [3:50:45,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5700it [3:50:47,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5701it [3:50:49,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5702it [3:50:51,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5703it [3:50:55,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5704it [3:50:57,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5705it [3:51:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5706it [3:51:02,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5707it [3:51:04,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5708it [3:51:05,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5709it [3:51:08,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5710it [3:51:11,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5711it [3:51:12,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5712it [3:51:16,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5713it [3:51:18,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5714it [3:51:21,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:36:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:36:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5715it [3:51:23,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5716it [3:51:25,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5717it [3:51:28,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5718it [3:51:31,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5719it [3:51:34,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5720it [3:51:36,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5721it [3:51:38,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5722it [3:51:40,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5723it [3:51:42,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5724it [3:51:44,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5725it [3:51:46,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5726it [3:51:47,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5727it [3:51:49,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5728it [3:51:51,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5729it [3:51:53,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5730it [3:51:58,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5731it [3:52:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5732it [3:52:03,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5733it [3:52:05,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5734it [3:52:07,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5735it [3:52:09,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5736it [3:52:10,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5737it [3:52:13,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5738it [3:52:16,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5739it [3:52:20,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:37:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:37:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5740it [3:52:23,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5741it [3:52:25,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5742it [3:52:30,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5743it [3:52:32,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5744it [3:52:34,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5745it [3:52:36,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5746it [3:52:38,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5747it [3:52:39,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5748it [3:52:42,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5749it [3:52:46,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5750it [3:52:48,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5751it [3:52:50,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5752it [3:52:52,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5753it [3:52:55,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5754it [3:52:58,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5755it [3:53:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5756it [3:53:03,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5757it [3:53:05,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5758it [3:53:06,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5759it [3:53:09,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5760it [3:53:12,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5761it [3:53:15,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5762it [3:53:17,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5763it [3:53:20,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:38:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:38:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5764it [3:53:23,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5765it [3:53:26,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5766it [3:53:30,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5767it [3:53:32,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5768it [3:53:35,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5769it [3:53:38,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5770it [3:53:41,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5771it [3:53:43,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5772it [3:53:46,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5773it [3:53:49,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5774it [3:53:51,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5775it [3:53:53,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5776it [3:53:56,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5777it [3:53:58,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5778it [3:53:59,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5779it [3:54:01,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5780it [3:54:04,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5781it [3:54:07,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5782it [3:54:08,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5783it [3:54:10,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5784it [3:54:12,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5785it [3:54:14,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5786it [3:54:16,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5787it [3:54:19,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5788it [3:54:20,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:39:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:39:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5789it [3:54:25,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5790it [3:54:28,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5791it [3:54:32,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5792it [3:54:36,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5793it [3:54:38,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5794it [3:54:41,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5795it [3:54:43,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5796it [3:54:45,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5797it [3:54:47,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5798it [3:54:49,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5799it [3:54:52,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5800it [3:54:55,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5801it [3:54:57,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5802it [3:54:59,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5803it [3:55:01,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5804it [3:55:03,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5805it [3:55:04,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5806it [3:55:06,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5807it [3:55:09,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5808it [3:55:12,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5809it [3:55:14,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5810it [3:55:15,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5811it [3:55:19,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5812it [3:55:20,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:40:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:40:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5813it [3:55:24,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5814it [3:55:26,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5815it [3:55:30,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5816it [3:55:35,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5817it [3:55:36,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5818it [3:55:38,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5819it [3:55:40,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5820it [3:55:42,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5821it [3:55:46,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5822it [3:55:48,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5823it [3:55:50,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5824it [3:55:53,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5825it [3:55:55,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5826it [3:55:57,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5827it [3:55:59,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5828it [3:56:01,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5829it [3:56:03,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5830it [3:56:06,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5831it [3:56:09,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5832it [3:56:11,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5833it [3:56:15,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5834it [3:56:19,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5835it [3:56:20,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5836it [3:56:22,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:41:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:41:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5837it [3:56:25,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5838it [3:56:26,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5839it [3:56:28,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5840it [3:56:31,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5841it [3:56:32,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5842it [3:56:34,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5843it [3:56:36,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5844it [3:56:38,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5845it [3:56:40,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5846it [3:56:43,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5847it [3:56:46,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5848it [3:56:48,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5849it [3:56:50,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5850it [3:56:52,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5851it [3:56:54,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5852it [3:56:57,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5853it [3:56:59,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5854it [3:57:02,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5855it [3:57:04,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5856it [3:57:05,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5857it [3:57:08,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5858it [3:57:12,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5859it [3:57:15,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5860it [3:57:18,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5861it [3:57:22,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:42:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:42:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5862it [3:57:26,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5863it [3:57:28,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5864it [3:57:31,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5865it [3:57:33,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5866it [3:57:35,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5867it [3:57:38,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5868it [3:57:40,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5869it [3:57:42,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5870it [3:57:44,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5871it [3:57:47,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5872it [3:57:50,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5873it [3:57:52,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5874it [3:57:54,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5875it [3:57:56,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5876it [3:57:58,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5877it [3:58:01,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5878it [3:58:02,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5879it [3:58:04,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5880it [3:58:06,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5881it [3:58:08,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5882it [3:58:10,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5883it [3:58:14,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5884it [3:58:15,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5885it [3:58:18,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5886it [3:58:21,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5887it [3:58:23,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:43:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:43:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5888it [3:58:24,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5889it [3:58:29,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5890it [3:58:32,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5891it [3:58:36,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5892it [3:58:39,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5893it [3:58:43,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5894it [3:58:44,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5895it [3:58:47,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5896it [3:58:50,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5897it [3:58:52,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5898it [3:58:56,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5899it [3:58:59,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5900it [3:59:02,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5901it [3:59:04,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5902it [3:59:07,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5903it [3:59:09,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5904it [3:59:11,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5905it [3:59:13,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5906it [3:59:15,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5907it [3:59:16,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5908it [3:59:18,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5909it [3:59:20,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:44:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:44:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5910it [3:59:24,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5911it [3:59:26,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5912it [3:59:28,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5913it [3:59:30,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5914it [3:59:31,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5915it [3:59:33,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5916it [3:59:35,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5917it [3:59:40,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5918it [3:59:42,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5919it [3:59:46,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5920it [3:59:48,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5921it [3:59:50,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5922it [3:59:53,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5923it [3:59:55,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5924it [3:59:58,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5925it [4:00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5926it [4:00:03,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5927it [4:00:07,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5928it [4:00:08,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5929it [4:00:11,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5930it [4:00:12,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5931it [4:00:14,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5932it [4:00:16,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5933it [4:00:20,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5934it [4:00:22,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:45:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:45:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5935it [4:00:26,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5936it [4:00:28,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5937it [4:00:30,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5938it [4:00:32,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5939it [4:00:33,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5940it [4:00:36,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5941it [4:00:39,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5942it [4:00:40,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5943it [4:00:42,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5944it [4:00:44,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5945it [4:00:46,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5946it [4:00:50,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5947it [4:00:52,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5948it [4:00:54,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5949it [4:00:56,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5950it [4:00:59,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5951it [4:01:02,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5952it [4:01:05,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5953it [4:01:07,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5954it [4:01:10,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5955it [4:01:13,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5956it [4:01:15,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5957it [4:01:17,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5958it [4:01:19,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5959it [4:01:21,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:46:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:46:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5960it [4:01:23,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5961it [4:01:26,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5962it [4:01:27,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5963it [4:01:30,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5964it [4:01:32,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5965it [4:01:34,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5966it [4:01:36,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5967it [4:01:37,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5968it [4:01:41,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5969it [4:01:44,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5970it [4:01:47,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5971it [4:01:50,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5972it [4:01:53,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5973it [4:01:55,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5974it [4:01:56,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5975it [4:02:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5976it [4:02:04,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5977it [4:02:06,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5978it [4:02:08,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5979it [4:02:12,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5980it [4:02:13,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5981it [4:02:15,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5982it [4:02:16,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5983it [4:02:20,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5984it [4:02:21,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:47:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:47:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5985it [4:02:24,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5986it [4:02:28,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5987it [4:02:31,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5988it [4:02:33,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5989it [4:02:35,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5990it [4:02:38,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5991it [4:02:40,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5992it [4:02:44,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5993it [4:02:46,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5994it [4:02:48,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5995it [4:02:49,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5996it [4:02:51,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5997it [4:02:54,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5998it [4:02:57,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5999it [4:03:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6000it [4:03:02,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6001it [4:03:04,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6002it [4:03:05,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6003it [4:03:09,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6004it [4:03:11,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6005it [4:03:13,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6006it [4:03:15,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6007it [4:03:18,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6008it [4:03:20,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6009it [4:03:22,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:48:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:48:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6010it [4:03:25,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6011it [4:03:27,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6012it [4:03:31,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6013it [4:03:34,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6014it [4:03:36,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6015it [4:03:38,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6016it [4:03:39,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6017it [4:03:42,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6018it [4:03:44,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6019it [4:03:45,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6020it [4:03:48,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6021it [4:03:50,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6022it [4:03:52,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6023it [4:03:54,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6024it [4:03:58,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6025it [4:04:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6026it [4:04:02,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6027it [4:04:04,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6028it [4:04:05,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6029it [4:04:07,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6030it [4:04:09,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6031it [4:04:12,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6032it [4:04:14,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6033it [4:04:16,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6034it [4:04:18,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6035it [4:04:21,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:49:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:49:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6036it [4:04:23,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6037it [4:04:28,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6038it [4:04:30,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6039it [4:04:32,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6040it [4:04:34,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6041it [4:04:36,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6042it [4:04:38,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6043it [4:04:40,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6044it [4:04:41,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6045it [4:04:44,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6046it [4:04:46,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6047it [4:04:48,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6048it [4:04:49,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6049it [4:04:51,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6050it [4:04:53,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6051it [4:04:54,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6052it [4:04:57,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6053it [4:05:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6054it [4:05:03,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6055it [4:05:05,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6056it [4:05:08,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6057it [4:05:12,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6058it [4:05:16,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6059it [4:05:18,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6060it [4:05:20,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6061it [4:05:21,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:50:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:50:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6062it [4:05:25,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6063it [4:05:27,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6064it [4:05:30,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6065it [4:05:31,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6066it [4:05:35,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6067it [4:05:38,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6068it [4:05:41,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6069it [4:05:44,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6070it [4:05:46,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6071it [4:05:49,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6072it [4:05:51,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6073it [4:05:52,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6074it [4:05:55,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6075it [4:05:59,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6076it [4:06:02,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6077it [4:06:04,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6078it [4:06:05,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [4:06:07,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6080it [4:06:09,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6081it [4:06:12,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6082it [4:06:15,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6083it [4:06:17,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6084it [4:06:19,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6085it [4:06:22,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:51:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:51:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6086it [4:06:23,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6087it [4:06:28,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6088it [4:06:30,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6089it [4:06:32,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6090it [4:06:34,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6091it [4:06:36,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6092it [4:06:38,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6093it [4:06:40,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6094it [4:06:42,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6095it [4:06:43,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6096it [4:06:45,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6097it [4:06:48,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6098it [4:06:50,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6099it [4:06:52,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6100it [4:06:54,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6101it [4:06:58,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6102it [4:07:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6103it [4:07:02,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6104it [4:07:04,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6105it [4:07:08,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6106it [4:07:11,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6107it [4:07:14,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6108it [4:07:16,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6109it [4:07:19,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6110it [4:07:22,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:52:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:52:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6111it [4:07:25,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6112it [4:07:27,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6113it [4:07:29,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6114it [4:07:32,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6115it [4:07:34,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6116it [4:07:35,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6117it [4:07:38,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6118it [4:07:41,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6119it [4:07:45,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6120it [4:07:49,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6121it [4:07:50,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6122it [4:07:52,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6123it [4:07:55,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6124it [4:07:57,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6125it [4:07:59,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6126it [4:08:01,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6127it [4:08:04,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6128it [4:08:06,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6129it [4:08:07,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6130it [4:08:09,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6131it [4:08:10,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6132it [4:08:12,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6133it [4:08:14,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6134it [4:08:16,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6135it [4:08:18,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6136it [4:08:20,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6137it [4:08:22,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:53:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:53:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6138it [4:08:24,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6139it [4:08:27,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6140it [4:08:28,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6141it [4:08:30,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6142it [4:08:32,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6143it [4:08:34,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6144it [4:08:35,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6145it [4:08:38,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6146it [4:08:41,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6147it [4:08:44,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6148it [4:08:45,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6149it [4:08:49,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6150it [4:08:51,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6151it [4:08:55,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6152it [4:08:57,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6153it [4:08:59,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6154it [4:09:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6155it [4:09:02,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6156it [4:09:04,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6157it [4:09:09,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6158it [4:09:10,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6159it [4:09:14,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6160it [4:09:17,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6161it [4:09:19,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6162it [4:09:20,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6163it [4:09:22,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:54:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:54:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6164it [4:09:24,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6165it [4:09:27,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6166it [4:09:28,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6167it [4:09:30,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6168it [4:09:32,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6169it [4:09:33,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6170it [4:09:36,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6171it [4:09:39,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6172it [4:09:42,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6173it [4:09:44,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6174it [4:09:46,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6175it [4:09:48,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6176it [4:09:51,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6177it [4:09:53,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6178it [4:09:55,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6179it [4:09:57,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6180it [4:10:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6181it [4:10:03,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6182it [4:10:06,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6183it [4:10:09,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6184it [4:10:11,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6185it [4:10:12,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6186it [4:10:15,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6187it [4:10:18,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6188it [4:10:21,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6189it [4:10:23,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:55:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:55:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6190it [4:10:25,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6191it [4:10:28,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6192it [4:10:31,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6193it [4:10:33,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6194it [4:10:35,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6195it [4:10:37,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6196it [4:10:39,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6197it [4:10:41,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6198it [4:10:43,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6199it [4:10:46,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6200it [4:10:47,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6201it [4:10:50,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6202it [4:10:54,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6203it [4:10:57,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6204it [4:10:58,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6205it [4:11:01,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6206it [4:11:03,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6207it [4:11:05,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6208it [4:11:07,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6209it [4:11:10,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6210it [4:11:14,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6211it [4:11:17,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6212it [4:11:22,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:56:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:56:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6213it [4:11:25,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6214it [4:11:27,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6215it [4:11:30,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6216it [4:11:32,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6217it [4:11:33,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6218it [4:11:35,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6219it [4:11:38,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6220it [4:11:39,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6221it [4:11:41,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6222it [4:11:45,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6223it [4:11:46,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6224it [4:11:51,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6225it [4:11:53,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6226it [4:11:56,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6227it [4:11:58,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6228it [4:12:00,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6229it [4:12:03,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6230it [4:12:05,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6231it [4:12:09,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6232it [4:12:11,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6233it [4:12:13,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6234it [4:12:17,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6235it [4:12:19,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6236it [4:12:22,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:57:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:57:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6237it [4:12:26,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6238it [4:12:31,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6239it [4:12:34,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6240it [4:12:37,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6241it [4:12:40,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6242it [4:12:43,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6243it [4:12:45,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6244it [4:12:48,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6245it [4:12:50,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6246it [4:12:52,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6247it [4:12:53,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6248it [4:12:55,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6249it [4:12:57,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6250it [4:13:01,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6251it [4:13:03,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6252it [4:13:05,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6253it [4:13:07,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6254it [4:13:10,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6255it [4:13:13,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6256it [4:13:15,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6257it [4:13:17,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6258it [4:13:19,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6259it [4:13:22,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:58:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:58:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6260it [4:13:24,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6261it [4:13:28,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6262it [4:13:30,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6263it [4:13:33,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6264it [4:13:36,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6265it [4:13:38,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6266it [4:13:40,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6267it [4:13:43,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6268it [4:13:45,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6269it [4:13:46,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6270it [4:13:48,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6271it [4:13:51,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6272it [4:13:52,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6273it [4:13:54,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6274it [4:13:57,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6275it [4:14:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6276it [4:14:02,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6277it [4:14:05,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6278it [4:14:08,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6279it [4:14:10,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6280it [4:14:13,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6281it [4:14:15,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6282it [4:14:19,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6283it [4:14:20,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 09:59:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 09:59:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6284it [4:14:24,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6285it [4:14:25,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6286it [4:14:28,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6287it [4:14:30,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6288it [4:14:33,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6289it [4:14:36,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6290it [4:14:40,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6291it [4:14:41,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6292it [4:14:43,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6293it [4:14:47,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6294it [4:14:50,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6295it [4:14:52,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6296it [4:14:54,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6297it [4:14:56,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6298it [4:14:59,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6299it [4:15:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6300it [4:15:04,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6301it [4:15:07,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6302it [4:15:11,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6303it [4:15:14,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6304it [4:15:17,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6305it [4:15:20,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6306it [4:15:23,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:00:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:00:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6307it [4:15:26,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6308it [4:15:28,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6309it [4:15:31,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6310it [4:15:33,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6311it [4:15:37,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6312it [4:15:39,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6313it [4:15:41,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6314it [4:15:43,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6315it [4:15:47,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6316it [4:15:51,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6317it [4:15:53,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6318it [4:15:56,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6319it [4:15:59,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6320it [4:16:01,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6321it [4:16:04,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6322it [4:16:06,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6323it [4:16:08,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6324it [4:16:10,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6325it [4:16:12,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6326it [4:16:14,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6327it [4:16:16,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6328it [4:16:17,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6329it [4:16:19,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6330it [4:16:21,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:01:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:01:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6331it [4:16:24,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6332it [4:16:29,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6333it [4:16:33,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6334it [4:16:35,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6335it [4:16:37,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6336it [4:16:40,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6337it [4:16:42,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6338it [4:16:45,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6339it [4:16:50,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6340it [4:16:52,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6341it [4:16:54,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6342it [4:16:56,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6343it [4:16:59,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6344it [4:17:01,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6345it [4:17:03,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6346it [4:17:07,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6347it [4:17:09,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6348it [4:17:11,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6349it [4:17:13,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6350it [4:17:16,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6351it [4:17:20,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:02:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:02:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6352it [4:17:25,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6353it [4:17:27,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6354it [4:17:29,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6355it [4:17:32,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6356it [4:17:34,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6357it [4:17:37,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6358it [4:17:38,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6359it [4:17:39,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6360it [4:17:41,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6361it [4:17:45,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6362it [4:17:49,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6363it [4:17:53,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6364it [4:17:57,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6365it [4:17:58,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6366it [4:18:00,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6367it [4:18:02,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6368it [4:18:06,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6369it [4:18:08,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6370it [4:18:10,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6371it [4:18:11,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6372it [4:18:14,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6373it [4:18:16,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6374it [4:18:20,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6375it [4:18:21,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6376it [4:18:23,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:03:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:03:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6377it [4:18:25,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6378it [4:18:26,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6379it [4:18:31,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6380it [4:18:35,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6381it [4:18:37,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6382it [4:18:40,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6383it [4:18:43,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6384it [4:18:47,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6385it [4:18:49,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6386it [4:18:51,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6387it [4:18:53,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6388it [4:18:56,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6389it [4:18:59,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6390it [4:19:03,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6391it [4:19:06,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6392it [4:19:09,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6393it [4:19:11,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6394it [4:19:14,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6395it [4:19:16,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6396it [4:19:19,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6397it [4:19:20,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6398it [4:19:22,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:04:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:04:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6399it [4:19:23,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6400it [4:19:27,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6401it [4:19:31,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6402it [4:19:34,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6403it [4:19:36,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6404it [4:19:37,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6405it [4:19:40,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6406it [4:19:44,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6407it [4:19:46,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6408it [4:19:48,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6409it [4:19:51,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6410it [4:19:53,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6411it [4:19:57,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6412it [4:19:59,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6413it [4:20:02,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6414it [4:20:03,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6415it [4:20:08,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6416it [4:20:10,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6417it [4:20:14,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6418it [4:20:18,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6419it [4:20:20,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:05:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:05:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6420it [4:20:23,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6421it [4:20:25,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6422it [4:20:28,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6423it [4:20:31,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6424it [4:20:34,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6425it [4:20:37,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6426it [4:20:37,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6427it [4:20:39,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6428it [4:20:43,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6429it [4:20:45,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6430it [4:20:48,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6431it [4:20:51,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6432it [4:20:53,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6433it [4:20:55,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6434it [4:20:56,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6435it [4:20:58,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6436it [4:21:01,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6437it [4:21:04,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6438it [4:21:07,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6439it [4:21:09,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6440it [4:21:11,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6441it [4:21:14,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6442it [4:21:17,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6443it [4:21:18,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6444it [4:21:20,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6445it [4:21:22,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:06:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:06:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6446it [4:21:24,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6447it [4:21:26,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6448it [4:21:29,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6449it [4:21:30,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6450it [4:21:33,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6451it [4:21:36,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6452it [4:21:39,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6453it [4:21:41,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6454it [4:21:43,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6455it [4:21:46,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6456it [4:21:48,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6457it [4:21:51,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6458it [4:21:54,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6459it [4:21:55,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6460it [4:21:58,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6461it [4:22:03,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6462it [4:22:06,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6463it [4:22:08,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6464it [4:22:10,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6465it [4:22:13,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6466it [4:22:15,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6467it [4:22:18,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6468it [4:22:22,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:07:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:07:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6469it [4:22:23,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6470it [4:22:26,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6471it [4:22:29,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6472it [4:22:31,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6473it [4:22:34,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6474it [4:22:35,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6475it [4:22:38,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6476it [4:22:39,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6477it [4:22:42,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6478it [4:22:45,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6479it [4:22:46,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6480it [4:22:50,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6481it [4:22:52,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6482it [4:22:54,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6483it [4:22:55,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6484it [4:22:57,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6485it [4:23:01,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6486it [4:23:03,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6487it [4:23:07,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6488it [4:23:11,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6489it [4:23:12,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6490it [4:23:15,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6491it [4:23:18,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6492it [4:23:20,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6493it [4:23:21,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6494it [4:23:23,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:08:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:08:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6495it [4:23:27,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6496it [4:23:28,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6497it [4:23:31,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6498it [4:23:34,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6499it [4:23:38,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6500it [4:23:40,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6501it [4:23:42,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6502it [4:23:43,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6503it [4:23:45,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6504it [4:23:47,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6505it [4:23:49,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6506it [4:23:51,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6507it [4:23:52,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6508it [4:23:54,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6509it [4:23:57,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6510it [4:23:59,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6511it [4:24:01,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6512it [4:24:03,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6513it [4:24:05,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6514it [4:24:09,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6515it [4:24:11,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6516it [4:24:12,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6517it [4:24:15,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6518it [4:24:18,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6519it [4:24:20,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:09:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:09:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6520it [4:24:26,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6521it [4:24:28,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6522it [4:24:30,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6523it [4:24:33,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6524it [4:24:36,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6525it [4:24:38,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6526it [4:24:40,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6527it [4:24:42,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6528it [4:24:45,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6529it [4:24:46,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6530it [4:24:50,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6531it [4:24:51,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6532it [4:24:55,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6533it [4:24:58,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6534it [4:25:00,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6535it [4:25:03,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6536it [4:25:05,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6537it [4:25:07,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6538it [4:25:09,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6539it [4:25:11,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6540it [4:25:14,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6541it [4:25:17,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6542it [4:25:19,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6543it [4:25:21,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6544it [4:25:23,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:10:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:10:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6545it [4:25:25,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6546it [4:25:27,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6547it [4:25:29,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6548it [4:25:31,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6549it [4:25:33,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6550it [4:25:35,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6551it [4:25:37,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6552it [4:25:39,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6553it [4:25:42,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6554it [4:25:43,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6555it [4:25:45,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6556it [4:25:47,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6557it [4:25:50,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6558it [4:25:51,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6559it [4:25:53,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6560it [4:25:57,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6561it [4:25:59,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6562it [4:26:03,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6563it [4:26:05,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6564it [4:26:06,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6565it [4:26:09,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6566it [4:26:12,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6567it [4:26:14,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6568it [4:26:16,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6569it [4:26:18,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6570it [4:26:20,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6571it [4:26:23,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:11:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:11:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6572it [4:26:25,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6573it [4:26:29,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6574it [4:26:32,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6575it [4:26:34,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6576it [4:26:36,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6577it [4:26:38,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6578it [4:26:40,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6579it [4:26:44,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6580it [4:26:46,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6581it [4:26:49,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6582it [4:26:50,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6583it [4:26:54,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6584it [4:26:56,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6585it [4:26:58,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6586it [4:27:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6587it [4:27:01,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6588it [4:27:03,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6589it [4:27:09,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6590it [4:27:11,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6591it [4:27:15,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6592it [4:27:17,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6593it [4:27:21,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:12:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:12:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6594it [4:27:23,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6595it [4:27:25,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6596it [4:27:27,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6597it [4:27:29,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6598it [4:27:30,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6599it [4:27:33,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6600it [4:27:35,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6601it [4:27:37,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6602it [4:27:40,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6603it [4:27:43,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6604it [4:27:45,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6605it [4:27:48,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6606it [4:27:51,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6607it [4:27:53,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6608it [4:27:55,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6609it [4:27:56,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6610it [4:27:58,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6611it [4:28:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6612it [4:28:02,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6613it [4:28:04,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6614it [4:28:06,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6615it [4:28:08,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6616it [4:28:10,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6617it [4:28:13,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6618it [4:28:16,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6619it [4:28:18,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6620it [4:28:21,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:13:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:13:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6621it [4:28:24,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6622it [4:28:26,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6623it [4:28:28,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6624it [4:28:32,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6625it [4:28:34,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6626it [4:28:35,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6627it [4:28:37,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6628it [4:28:42,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6629it [4:28:43,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6630it [4:28:45,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6631it [4:28:48,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6632it [4:28:49,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6633it [4:28:50,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6634it [4:28:54,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6635it [4:28:56,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6636it [4:28:57,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6637it [4:29:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6638it [4:29:02,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6639it [4:29:03,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6640it [4:29:05,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6641it [4:29:07,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6642it [4:29:10,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6643it [4:29:11,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6644it [4:29:13,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6645it [4:29:16,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6646it [4:29:18,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6647it [4:29:23,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:14:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:14:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6648it [4:29:26,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6649it [4:29:28,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6650it [4:29:31,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6651it [4:29:35,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6652it [4:29:36,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6653it [4:29:41,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6654it [4:29:45,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6655it [4:29:47,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6656it [4:29:48,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6657it [4:29:52,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6658it [4:29:54,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6659it [4:29:56,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6660it [4:29:58,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [4:30:00,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6662it [4:30:04,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6663it [4:30:05,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6664it [4:30:07,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6665it [4:30:09,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6666it [4:30:11,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6667it [4:30:14,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6668it [4:30:17,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6669it [4:30:19,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6670it [4:30:22,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:15:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:15:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6671it [4:30:26,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6672it [4:30:29,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6673it [4:30:31,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6674it [4:30:34,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6675it [4:30:35,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6676it [4:30:38,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6677it [4:30:40,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6678it [4:30:42,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6679it [4:30:46,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6680it [4:30:47,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6681it [4:30:49,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6682it [4:30:51,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6683it [4:30:53,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6684it [4:30:54,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6685it [4:30:57,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6686it [4:31:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6687it [4:31:03,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6688it [4:31:04,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6689it [4:31:08,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6690it [4:31:09,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6691it [4:31:11,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6692it [4:31:13,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6693it [4:31:17,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6694it [4:31:19,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6695it [4:31:21,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6696it [4:31:22,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:16:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:16:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6697it [4:31:24,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6698it [4:31:27,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6699it [4:31:28,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6700it [4:31:30,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6701it [4:31:32,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6702it [4:31:35,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6703it [4:31:37,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6704it [4:31:39,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6705it [4:31:42,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6706it [4:31:44,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6707it [4:31:46,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6708it [4:31:48,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6709it [4:31:51,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6710it [4:31:52,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6711it [4:31:55,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6712it [4:31:58,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6713it [4:32:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6714it [4:32:04,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6715it [4:32:06,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6716it [4:32:08,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6717it [4:32:12,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6718it [4:32:13,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6719it [4:32:15,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6720it [4:32:17,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6721it [4:32:18,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6722it [4:32:21,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6723it [4:32:22,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:17:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:17:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6724it [4:32:24,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6725it [4:32:27,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6726it [4:32:29,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6727it [4:32:30,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6728it [4:32:33,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6729it [4:32:36,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6730it [4:32:40,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6731it [4:32:44,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6732it [4:32:45,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6733it [4:32:47,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6734it [4:32:48,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6735it [4:32:51,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6736it [4:32:52,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6737it [4:32:55,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6738it [4:32:58,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6739it [4:33:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6740it [4:33:03,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6741it [4:33:07,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6742it [4:33:09,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6743it [4:33:13,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6744it [4:33:14,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6745it [4:33:17,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6746it [4:33:18,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6747it [4:33:21,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:18:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:18:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6748it [4:33:23,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6749it [4:33:25,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6750it [4:33:27,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6751it [4:33:29,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6752it [4:33:30,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6753it [4:33:32,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6754it [4:33:36,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6755it [4:33:37,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6756it [4:33:41,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6757it [4:33:45,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6758it [4:33:46,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6759it [4:33:49,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6760it [4:33:51,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6761it [4:33:53,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6762it [4:33:54,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6763it [4:33:57,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6764it [4:34:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6765it [4:34:02,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6766it [4:34:04,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6767it [4:34:06,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6768it [4:34:09,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6769it [4:34:11,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6770it [4:34:12,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6771it [4:34:15,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6772it [4:34:17,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6773it [4:34:19,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6774it [4:34:21,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:19:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:19:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6775it [4:34:23,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6776it [4:34:26,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6777it [4:34:27,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6778it [4:34:29,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6779it [4:34:32,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6780it [4:34:33,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6781it [4:34:35,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6782it [4:34:36,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6783it [4:34:38,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6784it [4:34:40,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6785it [4:34:42,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6786it [4:34:46,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6787it [4:34:49,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6788it [4:34:52,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6789it [4:34:55,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6790it [4:34:57,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6791it [4:35:01,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6792it [4:35:03,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6793it [4:35:06,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6794it [4:35:10,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6795it [4:35:12,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6796it [4:35:15,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6797it [4:35:18,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6798it [4:35:20,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6799it [4:35:22,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:20:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:20:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6800it [4:35:24,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6801it [4:35:27,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6802it [4:35:28,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6803it [4:35:32,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6804it [4:35:34,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6805it [4:35:37,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6806it [4:35:40,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6807it [4:35:43,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6808it [4:35:45,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6809it [4:35:46,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6810it [4:35:48,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6811it [4:35:51,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6812it [4:35:53,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6813it [4:35:55,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6814it [4:35:59,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6815it [4:36:01,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6816it [4:36:03,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6817it [4:36:05,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6818it [4:36:07,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6819it [4:36:09,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6820it [4:36:10,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6821it [4:36:12,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6822it [4:36:14,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6823it [4:36:16,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6824it [4:36:20,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6825it [4:36:22,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:21:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:21:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6826it [4:36:24,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6827it [4:36:27,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6828it [4:36:28,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6829it [4:36:30,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6830it [4:36:33,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6831it [4:36:34,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6832it [4:36:36,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6833it [4:36:38,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6834it [4:36:40,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6835it [4:36:42,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6836it [4:36:45,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6837it [4:36:47,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6838it [4:36:50,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6839it [4:36:52,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6840it [4:36:54,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6841it [4:36:57,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6842it [4:36:59,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6843it [4:37:02,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6844it [4:37:05,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6845it [4:37:09,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6846it [4:37:12,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6847it [4:37:16,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6848it [4:37:21,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:22:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:22:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6849it [4:37:23,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6850it [4:37:26,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6851it [4:37:27,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6852it [4:37:29,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6853it [4:37:31,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6854it [4:37:33,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6855it [4:37:35,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6856it [4:37:37,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6857it [4:37:40,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6858it [4:37:42,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6859it [4:37:44,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6860it [4:37:48,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6861it [4:37:50,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6862it [4:37:53,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6863it [4:37:55,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6864it [4:37:56,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6865it [4:37:57,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6866it [4:37:59,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6867it [4:38:01,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6868it [4:38:04,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6869it [4:38:05,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6870it [4:38:07,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6871it [4:38:10,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6872it [4:38:12,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6873it [4:38:16,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6874it [4:38:20,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6875it [4:38:22,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:23:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:23:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6876it [4:38:25,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6877it [4:38:27,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6878it [4:38:30,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6879it [4:38:31,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6880it [4:38:35,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6881it [4:38:36,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6882it [4:38:40,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6883it [4:38:42,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6884it [4:38:44,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6885it [4:38:46,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6886it [4:38:50,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6887it [4:38:54,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6888it [4:38:59,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6889it [4:39:01,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6890it [4:39:05,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6891it [4:39:06,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6892it [4:39:09,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6893it [4:39:11,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6894it [4:39:13,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6895it [4:39:16,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6896it [4:39:19,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6897it [4:39:22,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:24:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:24:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6898it [4:39:25,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6899it [4:39:28,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6900it [4:39:30,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6901it [4:39:32,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6902it [4:39:35,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6903it [4:39:38,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6904it [4:39:39,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6905it [4:39:40,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6906it [4:39:43,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6907it [4:39:44,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6908it [4:39:46,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6909it [4:39:49,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6910it [4:39:51,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6911it [4:39:54,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6912it [4:39:55,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6913it [4:39:58,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6914it [4:40:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6915it [4:40:03,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6916it [4:40:05,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6917it [4:40:06,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6918it [4:40:08,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6919it [4:40:11,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6920it [4:40:14,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6921it [4:40:16,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6922it [4:40:19,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6923it [4:40:21,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:25:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:25:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6924it [4:40:23,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6925it [4:40:25,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6926it [4:40:27,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6927it [4:40:29,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6928it [4:40:31,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6929it [4:40:32,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6930it [4:40:35,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6931it [4:40:38,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6932it [4:40:41,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6933it [4:40:45,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6934it [4:40:50,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6935it [4:40:51,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6936it [4:40:53,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6937it [4:40:56,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6938it [4:40:57,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6939it [4:41:00,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6940it [4:41:04,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6941it [4:41:06,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6942it [4:41:08,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6943it [4:41:11,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6944it [4:41:15,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6945it [4:41:16,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6946it [4:41:17,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6947it [4:41:19,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6948it [4:41:21,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6949it [4:41:23,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:26:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:26:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6950it [4:41:25,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6951it [4:41:28,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6952it [4:41:32,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6953it [4:41:36,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6954it [4:41:38,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6955it [4:41:40,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6956it [4:41:42,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6957it [4:41:46,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6958it [4:41:50,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6959it [4:41:52,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6960it [4:41:54,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6961it [4:41:55,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6962it [4:41:57,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6963it [4:41:59,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6964it [4:42:01,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6965it [4:42:04,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6966it [4:42:06,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6967it [4:42:08,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6968it [4:42:11,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6969it [4:42:13,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6970it [4:42:16,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6971it [4:42:18,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6972it [4:42:21,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:27:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:27:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6973it [4:42:24,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6974it [4:42:26,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6975it [4:42:28,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6976it [4:42:31,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6977it [4:42:35,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6978it [4:42:39,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6979it [4:42:40,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6980it [4:42:42,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6981it [4:42:45,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6982it [4:42:49,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6983it [4:42:51,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6984it [4:42:52,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6985it [4:42:54,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6986it [4:42:57,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6987it [4:42:59,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6988it [4:43:02,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6989it [4:43:05,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6990it [4:43:09,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6991it [4:43:10,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6992it [4:43:13,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6993it [4:43:15,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6994it [4:43:16,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6995it [4:43:19,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6996it [4:43:21,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:28:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:28:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6997it [4:43:25,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6998it [4:43:28,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6999it [4:43:30,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7000it [4:43:32,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7001it [4:43:35,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7002it [4:43:38,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7003it [4:43:40,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7004it [4:43:44,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7005it [4:43:46,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7006it [4:43:47,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7007it [4:43:49,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7008it [4:43:53,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7009it [4:43:55,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7010it [4:43:56,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7011it [4:44:01,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7012it [4:44:05,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7013it [4:44:07,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7014it [4:44:11,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7015it [4:44:13,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7016it [4:44:14,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7017it [4:44:16,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7018it [4:44:18,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7019it [4:44:21,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:29:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:29:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7020it [4:44:24,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7021it [4:44:28,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7022it [4:44:30,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7023it [4:44:32,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7024it [4:44:33,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7025it [4:44:36,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7026it [4:44:38,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7027it [4:44:40,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7028it [4:44:41,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7029it [4:44:43,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7030it [4:44:48,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7031it [4:44:51,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7032it [4:44:53,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7033it [4:44:56,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7034it [4:44:59,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7035it [4:45:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7036it [4:45:03,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7037it [4:45:05,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7038it [4:45:07,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7039it [4:45:09,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7040it [4:45:11,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7041it [4:45:14,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7042it [4:45:16,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7043it [4:45:18,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7044it [4:45:21,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7045it [4:45:23,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:30:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:30:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7046it [4:45:26,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7047it [4:45:29,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7048it [4:45:31,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7049it [4:45:33,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7050it [4:45:35,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7051it [4:45:38,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7052it [4:45:41,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7053it [4:45:42,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7054it [4:45:44,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7055it [4:45:46,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7056it [4:45:48,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7057it [4:45:52,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7058it [4:45:55,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7059it [4:45:57,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7060it [4:45:59,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7061it [4:46:01,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7062it [4:46:03,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7063it [4:46:05,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7064it [4:46:07,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7065it [4:46:11,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7066it [4:46:12,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7067it [4:46:15,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7068it [4:46:18,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7069it [4:46:20,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7070it [4:46:22,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:31:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:31:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7071it [4:46:25,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7072it [4:46:28,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7073it [4:46:31,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7074it [4:46:34,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7075it [4:46:37,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7076it [4:46:39,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7077it [4:46:40,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7078it [4:46:42,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7079it [4:46:46,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7080it [4:46:50,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7081it [4:46:52,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7082it [4:46:55,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7083it [4:46:58,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7084it [4:47:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7085it [4:47:02,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7086it [4:47:06,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7087it [4:47:09,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7088it [4:47:14,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7089it [4:47:17,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7090it [4:47:20,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7091it [4:47:21,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:32:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:32:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7092it [4:47:24,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7093it [4:47:26,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7094it [4:47:28,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7095it [4:47:30,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7096it [4:47:32,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7097it [4:47:35,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7098it [4:47:38,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7099it [4:47:41,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7100it [4:47:42,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7101it [4:47:46,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7102it [4:47:50,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7103it [4:47:52,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7104it [4:47:56,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7105it [4:47:59,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7106it [4:48:02,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7107it [4:48:03,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7108it [4:48:05,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7109it [4:48:07,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7110it [4:48:09,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7111it [4:48:12,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7112it [4:48:13,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7113it [4:48:16,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7114it [4:48:19,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7115it [4:48:21,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7116it [4:48:23,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:33:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:33:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7117it [4:48:25,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7118it [4:48:27,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7119it [4:48:31,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7120it [4:48:34,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7121it [4:48:37,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7122it [4:48:39,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7123it [4:48:41,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7124it [4:48:44,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7125it [4:48:47,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7126it [4:48:48,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7127it [4:48:52,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7128it [4:48:55,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7129it [4:48:58,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7130it [4:49:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7131it [4:49:04,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7132it [4:49:05,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7133it [4:49:09,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7134it [4:49:12,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7135it [4:49:15,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7136it [4:49:16,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7137it [4:49:20,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7138it [4:49:23,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:34:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:34:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7139it [4:49:25,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7140it [4:49:27,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7141it [4:49:30,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7142it [4:49:32,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7143it [4:49:34,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7144it [4:49:38,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7145it [4:49:40,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7146it [4:49:42,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7147it [4:49:44,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7148it [4:49:47,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7149it [4:49:48,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7150it [4:49:50,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7151it [4:49:52,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7152it [4:49:55,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7153it [4:49:58,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7154it [4:50:02,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7155it [4:50:05,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7156it [4:50:08,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7157it [4:50:09,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7158it [4:50:12,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7159it [4:50:14,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7160it [4:50:17,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7161it [4:50:19,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7162it [4:50:21,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7163it [4:50:23,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:35:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:35:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7164it [4:50:25,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7165it [4:50:28,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7166it [4:50:33,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7167it [4:50:37,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7168it [4:50:39,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7169it [4:50:41,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7170it [4:50:45,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7171it [4:50:48,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7172it [4:50:51,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7173it [4:50:54,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7174it [4:50:56,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7175it [4:50:59,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7176it [4:51:02,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7177it [4:51:04,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7178it [4:51:08,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7179it [4:51:10,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7180it [4:51:14,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7181it [4:51:16,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7182it [4:51:17,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7183it [4:51:19,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7184it [4:51:21,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:36:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:36:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7185it [4:51:23,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7186it [4:51:26,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7187it [4:51:29,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7188it [4:51:32,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7189it [4:51:35,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7190it [4:51:37,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7191it [4:51:39,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7192it [4:51:41,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7193it [4:51:43,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7194it [4:51:45,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7195it [4:51:48,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7196it [4:51:52,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7197it [4:51:54,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7198it [4:51:56,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7199it [4:51:58,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7200it [4:52:02,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7201it [4:52:06,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7202it [4:52:08,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7203it [4:52:11,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7204it [4:52:15,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7205it [4:52:17,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7206it [4:52:19,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7207it [4:52:21,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7208it [4:52:23,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:37:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:37:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7209it [4:52:25,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7210it [4:52:26,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7211it [4:52:28,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7212it [4:52:30,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7213it [4:52:32,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7214it [4:52:35,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7215it [4:52:38,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7216it [4:52:43,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7217it [4:52:46,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7218it [4:52:48,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7219it [4:52:50,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7220it [4:52:52,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7221it [4:52:54,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7222it [4:52:57,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7223it [4:53:01,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7224it [4:53:05,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7225it [4:53:10,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7226it [4:53:11,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7227it [4:53:14,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7228it [4:53:16,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7229it [4:53:20,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7230it [4:53:22,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:38:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:38:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7231it [4:53:24,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7232it [4:53:26,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7233it [4:53:29,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7234it [4:53:30,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7235it [4:53:33,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7236it [4:53:36,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7237it [4:53:37,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7238it [4:53:39,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7239it [4:53:40,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7240it [4:53:42,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7241it [4:53:44,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7242it [4:53:46,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7243it [4:53:47,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7244it [4:53:52,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7245it [4:53:54,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7246it [4:53:57,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7247it [4:54:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7248it [4:54:05,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7249it [4:54:07,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7250it [4:54:10,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7251it [4:54:14,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7252it [4:54:15,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7253it [4:54:18,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7254it [4:54:19,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:39:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:39:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7255it [4:54:23,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7256it [4:54:27,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7257it [4:54:29,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7258it [4:54:31,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7259it [4:54:32,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7260it [4:54:34,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7261it [4:54:37,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7262it [4:54:39,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7263it [4:54:42,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7264it [4:54:45,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7265it [4:54:46,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7266it [4:54:50,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7267it [4:54:52,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7268it [4:54:54,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7269it [4:54:56,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7270it [4:55:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7271it [4:55:02,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7272it [4:55:03,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7273it [4:55:05,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7274it [4:55:07,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7275it [4:55:09,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7276it [4:55:10,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7277it [4:55:12,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7278it [4:55:14,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7279it [4:55:18,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7280it [4:55:22,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:40:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:40:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7281it [4:55:25,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7282it [4:55:28,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7283it [4:55:30,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7284it [4:55:32,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7285it [4:55:34,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7286it [4:55:35,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7287it [4:55:38,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7288it [4:55:41,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7289it [4:55:42,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7290it [4:55:45,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7291it [4:55:47,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7292it [4:55:50,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7293it [4:55:52,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7294it [4:55:54,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7295it [4:55:57,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7296it [4:56:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7297it [4:56:01,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7298it [4:56:03,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7299it [4:56:05,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7300it [4:56:06,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7301it [4:56:10,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7302it [4:56:14,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7303it [4:56:17,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7304it [4:56:20,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7305it [4:56:22,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:41:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:41:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7306it [4:56:26,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7307it [4:56:30,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7308it [4:56:33,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7309it [4:56:35,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7310it [4:56:37,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7311it [4:56:39,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7312it [4:56:41,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7313it [4:56:45,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7314it [4:56:46,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7315it [4:56:48,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7316it [4:56:50,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7317it [4:56:53,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7318it [4:56:56,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7319it [4:56:58,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7320it [4:57:00,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7321it [4:57:02,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7322it [4:57:06,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7323it [4:57:08,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7324it [4:57:09,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7325it [4:57:13,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7326it [4:57:15,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7327it [4:57:17,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7328it [4:57:19,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:42:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:42:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7329it [4:57:23,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7330it [4:57:27,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7331it [4:57:30,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7332it [4:57:31,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7333it [4:57:33,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7334it [4:57:35,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7335it [4:57:38,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7336it [4:57:40,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7337it [4:57:43,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7338it [4:57:45,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7339it [4:57:48,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7340it [4:57:50,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7341it [4:57:53,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7342it [4:57:54,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7343it [4:57:58,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7344it [4:58:01,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7345it [4:58:02,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7346it [4:58:06,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7347it [4:58:08,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7348it [4:58:12,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7349it [4:58:14,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7350it [4:58:17,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7351it [4:58:21,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:43:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:43:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7352it [4:58:23,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7353it [4:58:27,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7354it [4:58:31,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7355it [4:58:33,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7356it [4:58:34,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7357it [4:58:36,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7358it [4:58:38,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7359it [4:58:42,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7360it [4:58:43,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7361it [4:58:45,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7362it [4:58:48,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7363it [4:58:51,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7364it [4:58:54,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7365it [4:58:56,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7366it [4:58:58,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7367it [4:59:02,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7368it [4:59:04,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7369it [4:59:07,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7370it [4:59:08,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7371it [4:59:11,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7372it [4:59:15,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7373it [4:59:16,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7374it [4:59:20,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7375it [4:59:22,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:44:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:44:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7376it [4:59:25,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7377it [4:59:27,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7378it [4:59:30,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7379it [4:59:31,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7380it [4:59:33,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7381it [4:59:36,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7382it [4:59:40,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7383it [4:59:42,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7384it [4:59:44,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7385it [4:59:45,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7386it [4:59:48,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7387it [4:59:50,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7388it [4:59:53,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7389it [4:59:54,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7390it [4:59:56,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7391it [4:59:58,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7392it [5:00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7393it [5:00:03,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7394it [5:00:06,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7395it [5:00:09,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7396it [5:00:11,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7397it [5:00:13,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7398it [5:00:15,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7399it [5:00:17,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7400it [5:00:19,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7401it [5:00:20,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:45:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:45:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7402it [5:00:24,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7403it [5:00:27,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7404it [5:00:29,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7405it [5:00:32,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7406it [5:00:34,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7407it [5:00:36,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7408it [5:00:38,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7409it [5:00:41,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7410it [5:00:44,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7411it [5:00:45,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7412it [5:00:47,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7413it [5:00:50,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7414it [5:00:52,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7415it [5:00:54,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7416it [5:00:56,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7417it [5:00:58,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7418it [5:01:00,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7419it [5:01:04,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7420it [5:01:08,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7421it [5:01:10,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7422it [5:01:11,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7423it [5:01:14,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7424it [5:01:16,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7425it [5:01:18,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7426it [5:01:21,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:46:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:46:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7427it [5:01:25,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7428it [5:01:26,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7429it [5:01:30,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7430it [5:01:32,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7431it [5:01:34,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7432it [5:01:37,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7433it [5:01:39,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7434it [5:01:40,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7435it [5:01:44,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7436it [5:01:45,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7437it [5:01:47,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7438it [5:01:49,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7439it [5:01:53,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7440it [5:01:55,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7441it [5:01:57,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7442it [5:01:59,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7443it [5:02:03,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7444it [5:02:06,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7445it [5:02:08,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7446it [5:02:10,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7447it [5:02:12,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7448it [5:02:16,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7449it [5:02:18,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7450it [5:02:20,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7451it [5:02:21,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7452it [5:02:23,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:47:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:47:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7453it [5:02:25,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7454it [5:02:27,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7455it [5:02:29,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7456it [5:02:32,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7457it [5:02:35,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7458it [5:02:37,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7459it [5:02:40,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7460it [5:02:43,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7461it [5:02:46,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7462it [5:02:49,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7463it [5:02:50,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7464it [5:02:53,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7465it [5:02:55,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7466it [5:02:58,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7467it [5:03:01,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7468it [5:03:03,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7469it [5:03:04,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7470it [5:03:06,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7471it [5:03:08,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7472it [5:03:09,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7473it [5:03:11,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7474it [5:03:14,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7475it [5:03:17,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7476it [5:03:21,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7477it [5:03:23,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:48:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:48:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7478it [5:03:25,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7479it [5:03:26,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7480it [5:03:30,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7481it [5:03:33,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7482it [5:03:35,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7483it [5:03:38,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7484it [5:03:40,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7485it [5:03:43,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7486it [5:03:46,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7487it [5:03:49,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7488it [5:03:51,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7489it [5:03:54,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7490it [5:03:57,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7491it [5:03:59,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7492it [5:04:02,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7493it [5:04:04,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7494it [5:04:07,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7495it [5:04:10,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7496it [5:04:12,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7497it [5:04:14,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7498it [5:04:16,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7499it [5:04:19,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7500it [5:04:21,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7501it [5:04:22,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:49:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:49:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7502it [5:04:25,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7503it [5:04:28,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7504it [5:04:31,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7505it [5:04:33,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7506it [5:04:35,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7507it [5:04:37,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7508it [5:04:39,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7509it [5:04:42,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7510it [5:04:45,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7511it [5:04:48,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7512it [5:04:49,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7513it [5:04:52,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7514it [5:04:54,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7515it [5:04:56,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7516it [5:04:59,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7517it [5:05:01,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7518it [5:05:03,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7519it [5:05:06,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7520it [5:05:08,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7521it [5:05:10,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7522it [5:05:13,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7523it [5:05:17,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7524it [5:05:19,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7525it [5:05:23,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:50:59 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:50:59 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7526it [5:05:25,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7527it [5:05:28,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7528it [5:05:31,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7529it [5:05:33,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7530it [5:05:35,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7531it [5:05:37,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7532it [5:05:40,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7533it [5:05:44,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7534it [5:05:47,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7535it [5:05:49,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7536it [5:05:51,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7537it [5:05:52,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:29 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:29 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7538it [5:05:55,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7539it [5:05:57,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7540it [5:05:58,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7541it [5:06:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7542it [5:06:03,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7543it [5:06:05,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7544it [5:06:07,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7545it [5:06:12,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7546it [5:06:14,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7547it [5:06:18,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7548it [5:06:20,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7549it [5:06:21,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:51:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:51:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7550it [5:06:23,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7551it [5:06:27,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7552it [5:06:30,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7553it [5:06:32,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7554it [5:06:33,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7555it [5:06:36,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7556it [5:06:38,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7557it [5:06:39,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7558it [5:06:42,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7559it [5:06:45,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7560it [5:06:48,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7561it [5:06:51,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7562it [5:06:54,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7563it [5:06:57,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7564it [5:06:59,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7565it [5:07:01,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7566it [5:07:03,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7567it [5:07:05,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7568it [5:07:08,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7569it [5:07:12,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7570it [5:07:15,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7571it [5:07:17,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7572it [5:07:18,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7573it [5:07:20,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:52:57 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:52:57 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7574it [5:07:23,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7575it [5:07:25,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7576it [5:07:27,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7577it [5:07:28,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7578it [5:07:31,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7579it [5:07:33,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7580it [5:07:36,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7581it [5:07:38,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7582it [5:07:42,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7583it [5:07:44,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7584it [5:07:45,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7585it [5:07:47,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7586it [5:07:49,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7587it [5:07:51,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7588it [5:07:54,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7589it [5:07:57,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7590it [5:08:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7591it [5:08:05,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7592it [5:08:07,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7593it [5:08:10,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7594it [5:08:13,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7595it [5:08:17,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7596it [5:08:19,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:55 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:55 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7597it [5:08:21,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:53:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:53:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7598it [5:08:24,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7599it [5:08:25,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7600it [5:08:28,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7601it [5:08:31,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7602it [5:08:35,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:11 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:11 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7603it [5:08:37,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7604it [5:08:41,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7605it [5:08:43,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7606it [5:08:46,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7607it [5:08:48,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7608it [5:08:50,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7609it [5:08:51,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7610it [5:08:54,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7611it [5:08:56,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7612it [5:08:59,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7613it [5:09:01,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:38 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:38 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7614it [5:09:03,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7615it [5:09:05,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7616it [5:09:08,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7617it [5:09:11,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:47 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:47 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7618it [5:09:12,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7619it [5:09:15,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7620it [5:09:17,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7621it [5:09:20,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7622it [5:09:22,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:54:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:54:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7623it [5:09:23,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7624it [5:09:25,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7625it [5:09:27,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7626it [5:09:29,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7627it [5:09:31,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7628it [5:09:36,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7629it [5:09:40,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7630it [5:09:44,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7631it [5:09:47,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7632it [5:09:48,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7633it [5:09:51,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7634it [5:09:56,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7635it [5:09:58,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7636it [5:09:59,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7637it [5:10:02,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7638it [5:10:04,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7639it [5:10:06,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:43 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:43 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7640it [5:10:09,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7641it [5:10:12,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7642it [5:10:14,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:51 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:51 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7643it [5:10:16,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7644it [5:10:17,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7645it [5:10:19,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7646it [5:10:21,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:55:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:55:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7647it [5:10:23,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7648it [5:10:27,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7649it [5:10:29,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7650it [5:10:31,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7651it [5:10:33,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:09 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:09 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7652it [5:10:36,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7653it [5:10:38,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7654it [5:10:41,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:17 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:17 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7655it [5:10:43,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7656it [5:10:46,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7657it [5:10:49,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7658it [5:10:51,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7659it [5:10:53,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7660it [5:10:54,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7661it [5:10:57,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7662it [5:10:59,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:35 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:35 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7663it [5:11:01,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7664it [5:11:02,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7665it [5:11:05,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7666it [5:11:09,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7667it [5:11:12,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:49 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:49 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7668it [5:11:16,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7669it [5:11:19,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:56:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:56:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7670it [5:11:23,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:00 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:00 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7671it [5:11:25,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:02 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:02 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7672it [5:11:28,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7673it [5:11:29,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7674it [5:11:32,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7675it [5:11:36,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7676it [5:11:37,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7677it [5:11:39,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7678it [5:11:41,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7679it [5:11:43,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7680it [5:11:44,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:21 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:21 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7681it [5:11:47,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:24 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:24 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7682it [5:11:49,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7683it [5:11:51,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7684it [5:11:54,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7685it [5:11:56,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7686it [5:11:59,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:36 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:36 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7687it [5:12:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7688it [5:12:02,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7689it [5:12:04,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7690it [5:12:06,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7691it [5:12:08,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:45 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:45 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7692it [5:12:10,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:46 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7693it [5:12:13,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7694it [5:12:17,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:53 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:53 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7695it [5:12:19,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7696it [5:12:21,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:57:58 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:57:58 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7697it [5:12:24,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7698it [5:12:26,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:03 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:03 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7699it [5:12:28,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:05 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:05 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7700it [5:12:30,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:07 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:07 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7701it [5:12:31,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:08 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:08 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7702it [5:12:33,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7703it [5:12:37,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:13 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:13 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7704it [5:12:40,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:16 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:16 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7705it [5:12:42,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:18 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:18 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7706it [5:12:43,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:20 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:20 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7707it [5:12:47,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:23 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:23 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7708it [5:12:50,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:26 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:26 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7709it [5:12:52,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:28 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:28 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7710it [5:12:53,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:30 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:30 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7711it [5:12:55,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:32 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:32 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7712it [5:12:56,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7713it [5:13:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7714it [5:13:04,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:41 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:41 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7715it [5:13:06,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7716it [5:13:08,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:44 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:44 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7717it [5:13:12,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:48 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:48 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7718it [5:13:13,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:50 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:50 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7719it [5:13:15,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:52 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:52 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7720it [5:13:17,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:54 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:54 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7721it [5:13:19,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:58:56 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:58:56 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7722it [5:13:24,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:01 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:01 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7723it [5:13:28,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:04 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:04 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7724it [5:13:29,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:06 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:06 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7725it [5:13:33,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:10 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:10 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7726it [5:13:35,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:12 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:12 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7727it [5:13:37,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:14 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:14 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7728it [5:13:39,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:15 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:15 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7729it [5:13:42,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:19 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:19 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7730it [5:13:45,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:22 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:22 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7731it [5:13:49,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:25 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:25 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7732it [5:13:50,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:27 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:27 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7733it [5:13:54,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:31 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:31 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7734it [5:13:56,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:33 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:33 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7735it [5:13:57,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:34 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:34 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7736it [5:14:00,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:37 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:37 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7737it [5:14:02,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:39 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:39 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7738it [5:14:04,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:40 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:40 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7739it [5:14:05,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:42 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 11-05 10:59:42 chat_utils.py:574] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7740it [5:14:09,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-05 10:59:46 chat_utils.py:570] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    }
   ],
   "source": [
    "pair_text_train = pixtral_inference(\n",
    "    llm=model,\n",
    "    sampling_params=sampling_params,\n",
    "    image_url_1=pair_1_url_train,\n",
    "    image_url_2=pair_2_url_train,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Save into json based on fname\n",
    "export_generated_text_json(\n",
    "    generated_text=pair_text_train,\n",
    "    fname_list_pair_1=pair_1_fname_train, \n",
    "    fname_list_pair_2=pair_2_fname_train,\n",
    "    target_dir='formatted_dataset/face_recognition/text'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddcce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_text_test = pixtral_inference(\n",
    "    llm=model,\n",
    "    sampling_params=sampling_params,\n",
    "    image_url_1=pair_1_url_test\n",
    "    image_url_2=pair_2_url_test\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "export_generated_text_json(\n",
    "    generated_text=pair_text_test,\n",
    "    fname_list_pair_1=pair_1_fname_test,\n",
    "    fname_list_pair_2=pair_2_fname_test,\n",
    "    target_dir='formatted_dataset/face_recognition/text'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair_text[0][0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from module.load_data import *\n",
    "\n",
    "# idx = 1\n",
    "# base_image_path = 'formatted_dataset/face_recognition/images/'\n",
    "# image_1, image_2 = load_image_pair(base_image_path + pair_1_fname_train[idx], base_image_path + pair_2_fname_train[idx])\n",
    "# print(label_pairing_train[0], label_anchor_train[0], label_non_anchor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ecd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a51155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5abd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(image_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e5f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
